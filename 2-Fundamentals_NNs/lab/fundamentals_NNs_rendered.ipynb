{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "EPSILON = 1e-8 # small constant to avoid underflow or divide per 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time, the data will correspond to greyscale images. <br> Two different datasets can be used here:\n",
    "- The MNIST dataset, small 8*8 images, corresponding to handwritten digits &rightarrow; 10 classes\n",
    "- The Fashion MNIST dataset, medium 28*28 images, corresponding to clothes pictures &rightarrow; 10 classes\n",
    "\n",
    "#### Starting with the simple MNIST is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MNIST\"\n",
    "# dataset = \"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset='MNIST'):\n",
    "    if dataset == 'MNIST':\n",
    "        digits = load_digits()\n",
    "        X, Y = np.asarray(digits['data'], dtype='float32'), np.asarray(digits['target'], dtype='int32')\n",
    "        return X, Y\n",
    "    elif dataset == 'FASHION_MNIST':\n",
    "        import tensorflow as tf\n",
    "        fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "        (X, Y), (_, _) = fashion_mnist.load_data()\n",
    "        X = X.reshape((X.shape[0], X.shape[1] * X.shape[2]))\n",
    "        X, Y = np.asarray(X, dtype='float32'), np.asarray(Y, dtype='int32')\n",
    "        return X, Y\n",
    "X, Y = load_data(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1797\n",
      "Input dimension: 64\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: {:d}'.format(X.shape[0]))\n",
    "print('Input dimension: {:d}'.format(X.shape[1]))  # images 8x8 or 28*28 actually\n",
    "print('Number of classes: {:d}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range max-min of greyscale pixel values: (16.0, 0.0)\n",
      "First image sample:\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "First image label: 0\n",
      "Input design matrix shape: (1797, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Range max-min of greyscale pixel values: ({0:.1f}, {1:.1f})\".format(np.max(X), np.min(X)))\n",
    "print(\"First image sample:\\n{0}\".format(X[0]))\n",
    "print(\"First image label: {0}\".format(Y[0]))\n",
    "print(\"Input design matrix shape: {0}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n",
    "Each image in the dataset consists of a 8 x 8 (or 28 x 28) matrix, of greyscale pixels. For the MNIST dataset, the values are between 0 and 16 where 0 represents white, 16 represents black and there are many shades of grey in-between. For the Fashion MNIST dataset, the values are between 0 and 255.<br>Each image is assigned a corresponding numerical label, so the image in ```X[i]``` has its corresponding label stored in ```Y[i]```.\n",
    "\n",
    "The next cells below demonstrate how to visualise the input data. Make sure you understand what's happening, particularly how the indices correspond to individual items in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_sample(X, Y, nrows=2, ncols=2):\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    for row in ax:\n",
    "        for col in row:\n",
    "            index = random.randint(0, X.shape[0])\n",
    "            dim = np.sqrt(X.shape[1]).astype(int)\n",
    "            col.imshow(X[index].reshape((dim, dim)), cmap=plt.cm.gray_r)\n",
    "            col.set_title(\"image label: %d\" % Y[index])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_data_sample(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â II - Multiclass classification MLP with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II a) - Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/mlp_mnist.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here will be to implement \"from scratch\" a Multilayer Perceptron for classification.\n",
    "\n",
    "We will define the formal categorical cross entropy loss as follows:\n",
    "$$\n",
    "l(\\mathbf{\\Theta}, \\mathbf{X}, \\mathbf{Y}) = - \\frac{1}{n} \\sum_{i=1}^n \\log \\mathbf{f}(\\mathbf{x}_i ; \\mathbf{\\Theta})^\\top y_i\n",
    "$$\n",
    "<center>with $y_i$ being the one-hot encoded true label for the sample $i$, and $\\Theta = (\\mathbf{W}^h; \\mathbf{b}^h; \\mathbf{W}^o; \\mathbf{b}^o)$</center>\n",
    "<center>In addition, $\\mathbf{f}(\\mathbf{x}) = softmax(\\mathbf{z^o}(\\mathbf{x})) = softmax(\\mathbf{W}^o\\mathbf{h}(\\mathbf{x}) + \\mathbf{b}^o)$</center>\n",
    "<center>and $\\mathbf{h}(\\mathbf{x}) = g(\\mathbf{z^h}(\\mathbf{x})) = g(\\mathbf{W}^h\\mathbf{x} + \\mathbf{b}^h)$, $g$ being the activation function and could be implemented with $sigmoid$ or $relu$</center>\n",
    "\n",
    "## Objectives:\n",
    "- Write the categorical cross entropy loss function\n",
    "- Write the activation functions with their associated gradient\n",
    "- Write the softmax function that is going to be used to output the predicted probabilities\n",
    "- Implement the forward pass through the neural network\n",
    "- Implement the backpropagation according to the used loss: progagate the gradients using the chain rule and return $(\\mathbf{\\nabla_{W^h}}l ; \\mathbf{\\nabla_{b^h}}l ; \\mathbf{\\nabla_{W^o}}l ; \\mathbf{\\nabla_{b^o}}l)$\n",
    "- Implement dropout regularization in the forward pass: be careful to consider both training and prediction cases\n",
    "- Implement the SGD optimization algorithm, and improve it with simple momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple graph function to let you have a global overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/function_graph.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You may find numpy outer products useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html <br>\n",
    "We have: $outer(u, v) = u \\cdot v^T$, with $u, v$ two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = np.random.normal(size=(5,)), np.random.normal(size=(10,))\n",
    "assert np.array_equal(\n",
    "    np.outer(u, v),\n",
    "    np.dot(np.reshape(u, (u.size, 1)), np.reshape(v, (1, v.size)))\n",
    ")\n",
    "assert np.outer(u, v).shape == (5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You also may find numpy matmul function useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html <br>\n",
    "It can be used to perform matrix products along one fixed dimension (i.e. the batch size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = np.random.randint(0, 100, size=(64, 5, 10)), np.random.randint(0, 100, size=(64, 10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(\n",
    "    np.stack([np.dot(A_i, B_i) for A_i, B_i in zip(A, B)]),\n",
    "    np.matmul(A, B)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â II b) - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    \"\"\"MLP with one hidden layer having a hidden activation,\n",
    "    and one output layer having a softmax activation\"\"\"\n",
    "    def __init__(self, X, Y, hidden_size, activation='relu',\n",
    "                 initialization='uniform', dropout=False, dropout_rate=0):\n",
    "        # input, hidden, and output dimensions on the MLP based on X, Y\n",
    "        self.input_size, self.output_size = X.shape[1], len(np.unique(Y))\n",
    "        self.hidden_size = hidden_size\n",
    "        # initialization strategies: avoid a full-0 initialization of the weight matrices\n",
    "        if initialization == 'uniform':\n",
    "            self.W_h = np.random.uniform(size=(self.hidden_size, self.input_size), high=0.01, low=-0.01)\n",
    "            self.W_o = np.random.uniform(size=(self.output_size, self.hidden_size), high=0.01, low=-0.01)\n",
    "        elif initialization == 'normal':\n",
    "            self.W_h = np.random.normal(size=(self.hidden_size, self.input_size), loc=0, scale=0.01)\n",
    "            self.W_o = np.random.normal(size=(self.output_size, self.hidden_size), loc=0, scale=0.01)\n",
    "        # the bias could be initializated to 0 or a random low constant\n",
    "        self.b_h = np.zeros(self.hidden_size)\n",
    "        self.b_o = np.zeros(self.output_size)\n",
    "        # our namedtuple structure of gradients\n",
    "        self.Grads = namedtuple('Grads', ['W_h', 'b_h', 'W_o', 'b_o'])\n",
    "        # and the velocities associated which are going to be useful for the momentum\n",
    "        self.velocities = {'W_h': 0., 'b_h': 0., 'W_o': 0., 'b_o': 0.}\n",
    "        #Â the hidden activation function used\n",
    "        self.activation = activation\n",
    "        #Â arrays to track back the losses and accuracies evolution\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        self.validation_acc_history = []\n",
    "        #Â train val split and normalization of the features\n",
    "        self.X_tr, self.X_val, self.Y_tr, self.Y_val = self.split_train_validation(X, Y)\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "        self.X_tr = self.scaler.fit_transform(self.X_tr)\n",
    "        self.X_val = self.scaler.transform(self.X_val)\n",
    "        #Â dropout parameters\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # step used for the optimization algorithm and setted later\n",
    "        self.step = None\n",
    "    \n",
    "    # One-hot encoding of the target\n",
    "    # Transform the integer represensation to a sparse one\n",
    "    @staticmethod\n",
    "    def one_hot(n_classes, Y):\n",
    "        return np.eye(n_classes)[Y]\n",
    "    \n",
    "    # Reverse one-hot encoding of the target\n",
    "    # Recover the former integer representation\n",
    "    # ex: from (0,0,1,0) to 2\n",
    "    @staticmethod\n",
    "    def reverse_one_hot(Y_one_hot):\n",
    "        return np.asarray(np.where(Y_one_hot==1)[1], dtype='int32')\n",
    "    \n",
    "    \"\"\"\n",
    "    Activation functions and their gradient\n",
    "    \"\"\"\n",
    "    # In implementations below X is a matrix of shape (n_samples, p)\n",
    "    \n",
    "    #Â A max_value value is indicated for the relu and grad_relu functions\n",
    "    # Make sure to clip the output to it to prevent numerical overflow (exploding gradient)\n",
    "    # Make it so the max value reachable is max_value\n",
    "    @staticmethod\n",
    "    def relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        return np.minimum(np.maximum(X, 0), max_value)\n",
    "    \n",
    "    # Make it so the gradient becomes 0 when X becomes greater than max_value\n",
    "    @staticmethod\n",
    "    def grad_relu(X, max_value=20):\n",
    "        return ((X > 0) & (X < max_value)).astype('int32')\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    def grad_sigmoid(self, X):\n",
    "        return self.sigmoid(X) * (1 - self.sigmoid(X))\n",
    "    \n",
    "    # Softmax function to output probabilities\n",
    "    @staticmethod\n",
    "    def softmax(X):\n",
    "        exp = np.exp(X)\n",
    "        return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "    \n",
    "    #Â Loss function\n",
    "    #Â Consider using EPSILON to prevent numerical issues (log(0) is undefined)\n",
    "    # Y_true and Y_pred are of shape (n_samples,n_classes)\n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(Y_true, Y_pred):\n",
    "        loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n",
    "        result = -np.mean(loglikelihoods)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_train_validation(X, Y, test_size=0.25, seed=False):\n",
    "        random_state = 42 if seed else np.random.randint(1e3)\n",
    "        X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "        return X_tr, X_val, Y_tr, Y_val\n",
    "    \n",
    "    # Sample random batch in (X, Y) with a given batch_size for SGD\n",
    "    @staticmethod\n",
    "    def get_random_batch(X, Y, batch_size):\n",
    "        indexes = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
    "        return X[indexes], Y[indexes]\n",
    "        \n",
    "    #Â Forward pass: compute f(x) as y, and return optionally the hidden states h(x) and z_h(x) for compute_grads\n",
    "    def forward(self, X, return_activation=False, training=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_activation = self.relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_activation = self.sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "        z_h = np.dot(self.W_h, X.T).T + self.b_h\n",
    "        h = g_activation(z_h)\n",
    "        if self.dropout:\n",
    "            if training:\n",
    "                dropout_mask = np.random.binomial(1, 1-self.dropout_rate, size=h.shape)\n",
    "                h *= dropout_mask\n",
    "            else:\n",
    "                h *= (1-self.dropout_rate)\n",
    "        z_o = np.dot(self.W_o, h.T).T + self.b_o\n",
    "        y = self.softmax(z_o)\n",
    "            \n",
    "        if return_activation:\n",
    "            return y, h, z_h\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    #Â Backpropagation: return an instantiation of self.Grads that contains the average gradients for the given batch\n",
    "    def compute_grads(self, X, Y_true, vectorized=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_grad = self.grad_relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_grad = self.grad_sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape((1,) + X.shape)\n",
    "        \n",
    "        if not vectorized:\n",
    "            n = X.shape[0]\n",
    "            grad_W_h = np.zeros((self.hidden_size, self.input_size))\n",
    "            grad_b_h = np.zeros((self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((self.output_size, self.hidden_size))\n",
    "            grad_b_o = np.zeros((self.output_size, ))\n",
    "            for x, y_true in zip(X, Y_true):\n",
    "                y_pred, h, z_h = self.forward(x, return_activation=True, training=True)\n",
    "                \n",
    "                grad_z_o = y_pred - self.one_hot(self.output_size, y_true)\n",
    "                grad_W_o += np.outer(grad_z_o, h.T)\n",
    "                grad_b_o += grad_z_o\n",
    "                grad_h = np.dot(self.W_o.T, grad_z_o)\n",
    "                \n",
    "                grad_z_h = grad_h * g_grad(z_h)\n",
    "                grad_W_h += np.outer(grad_z_h, x.T)\n",
    "                grad_b_h += grad_z_h\n",
    "                \n",
    "            grads = self.Grads(grad_W_h/n, grad_b_h/n, grad_W_o/n, grad_b_o/n)\n",
    "            \n",
    "        else: \n",
    "            Y_pred, h, z_h = self.forward(X, return_activation=True, training=True)\n",
    "\n",
    "            grad_z_o = Y_pred - self.one_hot(self.output_size, Y_true)\n",
    "            grad_W_o = np.matmul(grad_z_o[:, :, np.newaxis], h[:, np.newaxis, :])\n",
    "            grad_b_o = grad_z_o\n",
    "            grad_h = np.dot(grad_z_o, self.W_o)\n",
    "\n",
    "            grad_z_h = grad_h * g_grad(z_h)\n",
    "            grad_W_h = np.matmul(grad_z_h[:, :, np.newaxis], X[:, np.newaxis, :])\n",
    "            grad_b_h = grad_z_h\n",
    "                    \n",
    "            grads = self.Grads(\n",
    "                np.mean(grad_W_h, axis=0),\n",
    "                np.mean(grad_b_h, axis=0),\n",
    "                np.mean(grad_W_o, axis=0),\n",
    "                np.mean(grad_b_o, axis=0)\n",
    "            )\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    # Perform the update of the parameters (W_h, b_h, W_o, b_o) based of their gradient\n",
    "    def optimizer_step(self, optimizer='gd', momentum=False, momentum_alpha=0.9, \n",
    "                       batch_size=None, vectorized=True):\n",
    "        if optimizer == 'gd':\n",
    "            grads = self.compute_grads(self.X_tr, self.Y_tr, vectorized=vectorized)\n",
    "        elif optimizer == 'sgd':\n",
    "            batch_X_tr, batch_Y_tr = self.get_random_batch(self.X_tr, self.Y_tr, batch_size)\n",
    "            grads = self.compute_grads(batch_X_tr, batch_Y_tr, vectorized=vectorized)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if not momentum:\n",
    "            self.W_h -= self.step * grads.W_h\n",
    "            self.b_h -= self.step * grads.b_h\n",
    "            self.W_o -= self.step * grads.W_o\n",
    "            self.b_o -= self.step * grads.b_o\n",
    "        else:\n",
    "            self.velocities['W_h'] = momentum_alpha * self.velocities['W_h'] - self.step * grads.W_h\n",
    "            self.W_h += self.velocities['W_h']\n",
    "            self.velocities['b_h'] = momentum_alpha * self.velocities['b_h'] - self.step * grads.b_h\n",
    "            self.b_h += self.velocities['b_h']\n",
    "            self.velocities['W_o'] = momentum_alpha * self.velocities['W_o'] - self.step * grads.W_o\n",
    "            self.W_o += self.velocities['W_o']\n",
    "            self.velocities['b_o'] = momentum_alpha * self.velocities['b_o'] - self.step * grads.b_o\n",
    "            self.b_o += self.velocities['b_o']\n",
    "    \n",
    "    # Loss wrapper\n",
    "    def loss(self, Y_true, Y_pred):\n",
    "        return self.categorical_cross_entropy(self.one_hot(self.output_size, Y_true), Y_pred)\n",
    "    \n",
    "    def loss_history_flush(self):\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        \n",
    "    # Main function that trains the MLP with a design matrix X and a target vector Y\n",
    "    def train(self, optimizer='sgd', momentum=False, min_iterations=500, max_iterations=5000, initial_step=1e-1,\n",
    "              batch_size=64, early_stopping=True, early_stopping_lookbehind=100, early_stopping_delta=1e-4, \n",
    "              vectorized=False, flush_history=True, verbose=True):\n",
    "        if flush_history:\n",
    "            self.loss_history_flush()\n",
    "        cpt_patience, best_validation_loss = 0, np.inf\n",
    "        iteration_number = 0\n",
    "        self.step = initial_step\n",
    "        while len(self.training_losses_history) < max_iterations:\n",
    "            iteration_number += 1\n",
    "            self.optimizer_step(\n",
    "                optimizer=optimizer, momentum=momentum, batch_size=batch_size, vectorized=vectorized\n",
    "            )\n",
    "            training_loss = self.loss(self.Y_tr, self.forward(self.X_tr))\n",
    "            self.training_losses_history.append(training_loss)\n",
    "            validation_loss = self.loss(self.Y_val, self.forward(self.X_val))\n",
    "            self.validation_losses_history.append(validation_loss)\n",
    "            validation_accuracy = self.accuracy_on_validation()\n",
    "            self.validation_acc_history.append(validation_accuracy)\n",
    "            if iteration_number > min_iterations and early_stopping:\n",
    "                if validation_loss + early_stopping_delta < best_validation_loss:\n",
    "                    best_validation_loss = validation_loss\n",
    "                    cpt_patience = 0\n",
    "                else:\n",
    "                    cpt_patience += 1\n",
    "            if verbose:\n",
    "                msg = \"iteration number: {0}\\t training loss: {1:.4f}\\t\" + \\\n",
    "                \"validation loss: {2:.4f}\\t validation accuracy: {3:.4f}\"\n",
    "                print(msg.format(iteration_number, \n",
    "                                 training_loss,\n",
    "                                 validation_loss,\n",
    "                                 validation_accuracy))\n",
    "            if cpt_patience >= early_stopping_lookbehind:\n",
    "                break\n",
    "    \n",
    "    # Return the predicted class once the MLP has been trained\n",
    "    def predict(self, X, normalize=True):\n",
    "        if normalize:\n",
    "            X = self.scaler.transform(X)\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "        \n",
    "    \"\"\"\n",
    "    Metrics and plots\n",
    "    \"\"\"\n",
    "    def accuracy_on_train(self):\n",
    "        return (self.predict(self.X_tr, normalize=False) == self.Y_tr).mean()\n",
    "\n",
    "    def accuracy_on_validation(self):\n",
    "        return (self.predict(self.X_val, normalize=False) == self.Y_val).mean()\n",
    "\n",
    "    def plot_loss_history(self, add_to_title=None):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(range(len(self.training_losses_history)), \n",
    "                 self.training_losses_history, label='Training loss evolution')\n",
    "        plt.plot(range(len(self.validation_losses_history)),\n",
    "                 self.validation_losses_history, label='Validation loss evolution')\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(\"iteration number\", fontsize=15)\n",
    "        plt.ylabel(\"Cross entropy loss\", fontsize=15)\n",
    "        base_title = \"Cross entropy loss evolution during training\"\n",
    "        if not self.dropout:\n",
    "            base_title += \", no dropout penalization\"\n",
    "        else:\n",
    "            base_title += \", {:.2f} dropout penalization\"\n",
    "            base_title = base_title.format(self.dropout_rate)\n",
    "        title = base_title + \", \" + add_to_title if add_to_title else base_title\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_validation_prediction(self, sample_id):\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "        classes = np.unique(self.Y_tr)\n",
    "        dim = np.sqrt(self.X_val.shape[1]).astype(int)\n",
    "        ax0.imshow(self.scaler.inverse_transform([self.X_val[sample_id]]).reshape(dim, dim), cmap=plt.cm.gray_r,\n",
    "                   interpolation='nearest')\n",
    "        ax0.set_title(\"True image label: %d\" % self.Y_val[sample_id]);\n",
    "\n",
    "        ax1.bar(classes, self.one_hot(len(classes), self.Y_val[sample_id]), label='true')\n",
    "        ax1.bar(classes, self.forward(self.X_val[sample_id]), label='prediction', color=\"red\")\n",
    "        ax1.set_xticks(classes)\n",
    "        prediction = self.predict(self.X_val[sample_id], normalize=False)\n",
    "        ax1.set_title('Output probabilities (prediction: %d)' % prediction)\n",
    "        ax1.set_xlabel('Digit class')\n",
    "        ax1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Standard MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\t training loss: 2.3026\tvalidation loss: 2.3026\t validation accuracy: 0.0933\n",
      "iteration number: 2\t training loss: 2.3026\tvalidation loss: 2.3026\t validation accuracy: 0.1111\n",
      "iteration number: 3\t training loss: 2.3025\tvalidation loss: 2.3025\t validation accuracy: 0.1622\n",
      "iteration number: 4\t training loss: 2.3025\tvalidation loss: 2.3025\t validation accuracy: 0.1667\n",
      "iteration number: 5\t training loss: 2.3025\tvalidation loss: 2.3024\t validation accuracy: 0.1689\n",
      "iteration number: 6\t training loss: 2.3025\tvalidation loss: 2.3024\t validation accuracy: 0.1111\n",
      "iteration number: 7\t training loss: 2.3024\tvalidation loss: 2.3023\t validation accuracy: 0.1111\n",
      "iteration number: 8\t training loss: 2.3023\tvalidation loss: 2.3024\t validation accuracy: 0.1133\n",
      "iteration number: 9\t training loss: 2.3023\tvalidation loss: 2.3022\t validation accuracy: 0.1111\n",
      "iteration number: 10\t training loss: 2.3022\tvalidation loss: 2.3023\t validation accuracy: 0.1689\n",
      "iteration number: 11\t training loss: 2.3022\tvalidation loss: 2.3023\t validation accuracy: 0.0844\n",
      "iteration number: 12\t training loss: 2.3022\tvalidation loss: 2.3023\t validation accuracy: 0.1022\n",
      "iteration number: 13\t training loss: 2.3021\tvalidation loss: 2.3021\t validation accuracy: 0.1622\n",
      "iteration number: 14\t training loss: 2.3021\tvalidation loss: 2.3023\t validation accuracy: 0.1800\n",
      "iteration number: 15\t training loss: 2.3020\tvalidation loss: 2.3024\t validation accuracy: 0.0867\n",
      "iteration number: 16\t training loss: 2.3020\tvalidation loss: 2.3023\t validation accuracy: 0.0844\n",
      "iteration number: 17\t training loss: 2.3020\tvalidation loss: 2.3023\t validation accuracy: 0.0867\n",
      "iteration number: 18\t training loss: 2.3019\tvalidation loss: 2.3023\t validation accuracy: 0.0978\n",
      "iteration number: 19\t training loss: 2.3019\tvalidation loss: 2.3023\t validation accuracy: 0.1022\n",
      "iteration number: 20\t training loss: 2.3019\tvalidation loss: 2.3022\t validation accuracy: 0.1956\n",
      "iteration number: 21\t training loss: 2.3018\tvalidation loss: 2.3023\t validation accuracy: 0.1111\n",
      "iteration number: 22\t training loss: 2.3018\tvalidation loss: 2.3022\t validation accuracy: 0.1956\n",
      "iteration number: 23\t training loss: 2.3018\tvalidation loss: 2.3021\t validation accuracy: 0.1311\n",
      "iteration number: 24\t training loss: 2.3018\tvalidation loss: 2.3020\t validation accuracy: 0.0978\n",
      "iteration number: 25\t training loss: 2.3017\tvalidation loss: 2.3022\t validation accuracy: 0.0978\n",
      "iteration number: 26\t training loss: 2.3017\tvalidation loss: 2.3021\t validation accuracy: 0.0978\n",
      "iteration number: 27\t training loss: 2.3017\tvalidation loss: 2.3020\t validation accuracy: 0.0978\n",
      "iteration number: 28\t training loss: 2.3016\tvalidation loss: 2.3019\t validation accuracy: 0.0978\n",
      "iteration number: 29\t training loss: 2.3016\tvalidation loss: 2.3019\t validation accuracy: 0.1489\n",
      "iteration number: 30\t training loss: 2.3015\tvalidation loss: 2.3018\t validation accuracy: 0.1511\n",
      "iteration number: 31\t training loss: 2.3015\tvalidation loss: 2.3017\t validation accuracy: 0.1689\n",
      "iteration number: 32\t training loss: 2.3014\tvalidation loss: 2.3016\t validation accuracy: 0.0867\n",
      "iteration number: 33\t training loss: 2.3013\tvalidation loss: 2.3015\t validation accuracy: 0.0867\n",
      "iteration number: 34\t training loss: 2.3013\tvalidation loss: 2.3015\t validation accuracy: 0.0867\n",
      "iteration number: 35\t training loss: 2.3012\tvalidation loss: 2.3014\t validation accuracy: 0.1111\n",
      "iteration number: 36\t training loss: 2.3011\tvalidation loss: 2.3015\t validation accuracy: 0.1067\n",
      "iteration number: 37\t training loss: 2.3010\tvalidation loss: 2.3016\t validation accuracy: 0.1067\n",
      "iteration number: 38\t training loss: 2.3009\tvalidation loss: 2.3016\t validation accuracy: 0.1067\n",
      "iteration number: 39\t training loss: 2.3008\tvalidation loss: 2.3015\t validation accuracy: 0.1067\n",
      "iteration number: 40\t training loss: 2.3007\tvalidation loss: 2.3016\t validation accuracy: 0.1067\n",
      "iteration number: 41\t training loss: 2.3007\tvalidation loss: 2.3015\t validation accuracy: 0.1067\n",
      "iteration number: 42\t training loss: 2.3006\tvalidation loss: 2.3017\t validation accuracy: 0.1067\n",
      "iteration number: 43\t training loss: 2.3005\tvalidation loss: 2.3016\t validation accuracy: 0.1067\n",
      "iteration number: 44\t training loss: 2.3004\tvalidation loss: 2.3017\t validation accuracy: 0.1067\n",
      "iteration number: 45\t training loss: 2.3003\tvalidation loss: 2.3017\t validation accuracy: 0.1156\n",
      "iteration number: 46\t training loss: 2.3003\tvalidation loss: 2.3015\t validation accuracy: 0.1067\n",
      "iteration number: 47\t training loss: 2.3002\tvalidation loss: 2.3014\t validation accuracy: 0.1800\n",
      "iteration number: 48\t training loss: 2.3001\tvalidation loss: 2.3012\t validation accuracy: 0.1067\n",
      "iteration number: 49\t training loss: 2.3000\tvalidation loss: 2.3012\t validation accuracy: 0.2111\n",
      "iteration number: 50\t training loss: 2.3000\tvalidation loss: 2.3012\t validation accuracy: 0.1089\n",
      "iteration number: 51\t training loss: 2.2998\tvalidation loss: 2.3011\t validation accuracy: 0.2667\n",
      "iteration number: 52\t training loss: 2.2997\tvalidation loss: 2.3013\t validation accuracy: 0.1200\n",
      "iteration number: 53\t training loss: 2.2996\tvalidation loss: 2.3010\t validation accuracy: 0.1778\n",
      "iteration number: 54\t training loss: 2.2995\tvalidation loss: 2.3009\t validation accuracy: 0.2733\n",
      "iteration number: 55\t training loss: 2.2993\tvalidation loss: 2.3007\t validation accuracy: 0.2133\n",
      "iteration number: 56\t training loss: 2.2993\tvalidation loss: 2.3006\t validation accuracy: 0.2044\n",
      "iteration number: 57\t training loss: 2.2991\tvalidation loss: 2.3004\t validation accuracy: 0.2067\n",
      "iteration number: 58\t training loss: 2.2990\tvalidation loss: 2.3003\t validation accuracy: 0.1044\n",
      "iteration number: 59\t training loss: 2.2989\tvalidation loss: 2.3002\t validation accuracy: 0.1822\n",
      "iteration number: 60\t training loss: 2.2988\tvalidation loss: 2.2999\t validation accuracy: 0.2067\n",
      "iteration number: 61\t training loss: 2.2987\tvalidation loss: 2.2999\t validation accuracy: 0.2067\n",
      "iteration number: 62\t training loss: 2.2985\tvalidation loss: 2.2999\t validation accuracy: 0.1956\n",
      "iteration number: 63\t training loss: 2.2984\tvalidation loss: 2.3000\t validation accuracy: 0.2022\n",
      "iteration number: 64\t training loss: 2.2982\tvalidation loss: 2.2997\t validation accuracy: 0.2533\n",
      "iteration number: 65\t training loss: 2.2981\tvalidation loss: 2.2995\t validation accuracy: 0.2822\n",
      "iteration number: 66\t training loss: 2.2980\tvalidation loss: 2.2996\t validation accuracy: 0.2622\n",
      "iteration number: 67\t training loss: 2.2978\tvalidation loss: 2.2994\t validation accuracy: 0.2133\n",
      "iteration number: 68\t training loss: 2.2977\tvalidation loss: 2.2990\t validation accuracy: 0.1711\n",
      "iteration number: 69\t training loss: 2.2975\tvalidation loss: 2.2990\t validation accuracy: 0.2067\n",
      "iteration number: 70\t training loss: 2.2972\tvalidation loss: 2.2988\t validation accuracy: 0.2000\n",
      "iteration number: 71\t training loss: 2.2969\tvalidation loss: 2.2984\t validation accuracy: 0.2000\n",
      "iteration number: 72\t training loss: 2.2967\tvalidation loss: 2.2980\t validation accuracy: 0.2022\n",
      "iteration number: 73\t training loss: 2.2964\tvalidation loss: 2.2977\t validation accuracy: 0.1933\n",
      "iteration number: 74\t training loss: 2.2962\tvalidation loss: 2.2976\t validation accuracy: 0.1022\n",
      "iteration number: 75\t training loss: 2.2959\tvalidation loss: 2.2974\t validation accuracy: 0.1022\n",
      "iteration number: 76\t training loss: 2.2957\tvalidation loss: 2.2971\t validation accuracy: 0.1022\n",
      "iteration number: 77\t training loss: 2.2955\tvalidation loss: 2.2967\t validation accuracy: 0.1044\n",
      "iteration number: 78\t training loss: 2.2952\tvalidation loss: 2.2965\t validation accuracy: 0.1022\n",
      "iteration number: 79\t training loss: 2.2949\tvalidation loss: 2.2964\t validation accuracy: 0.1200\n",
      "iteration number: 80\t training loss: 2.2946\tvalidation loss: 2.2957\t validation accuracy: 0.2089\n",
      "iteration number: 81\t training loss: 2.2944\tvalidation loss: 2.2955\t validation accuracy: 0.1489\n",
      "iteration number: 82\t training loss: 2.2942\tvalidation loss: 2.2951\t validation accuracy: 0.2000\n",
      "iteration number: 83\t training loss: 2.2938\tvalidation loss: 2.2948\t validation accuracy: 0.1844\n",
      "iteration number: 84\t training loss: 2.2934\tvalidation loss: 2.2945\t validation accuracy: 0.2044\n",
      "iteration number: 85\t training loss: 2.2931\tvalidation loss: 2.2942\t validation accuracy: 0.1978\n",
      "iteration number: 86\t training loss: 2.2927\tvalidation loss: 2.2942\t validation accuracy: 0.1467\n",
      "iteration number: 87\t training loss: 2.2923\tvalidation loss: 2.2941\t validation accuracy: 0.2133\n",
      "iteration number: 88\t training loss: 2.2918\tvalidation loss: 2.2940\t validation accuracy: 0.2289\n",
      "iteration number: 89\t training loss: 2.2914\tvalidation loss: 2.2933\t validation accuracy: 0.2578\n",
      "iteration number: 90\t training loss: 2.2911\tvalidation loss: 2.2929\t validation accuracy: 0.1511\n",
      "iteration number: 91\t training loss: 2.2907\tvalidation loss: 2.2924\t validation accuracy: 0.2044\n",
      "iteration number: 92\t training loss: 2.2902\tvalidation loss: 2.2923\t validation accuracy: 0.2422\n",
      "iteration number: 93\t training loss: 2.2898\tvalidation loss: 2.2919\t validation accuracy: 0.2778\n",
      "iteration number: 94\t training loss: 2.2893\tvalidation loss: 2.2913\t validation accuracy: 0.2956\n",
      "iteration number: 95\t training loss: 2.2887\tvalidation loss: 2.2907\t validation accuracy: 0.2689\n",
      "iteration number: 96\t training loss: 2.2883\tvalidation loss: 2.2901\t validation accuracy: 0.2489\n",
      "iteration number: 97\t training loss: 2.2878\tvalidation loss: 2.2893\t validation accuracy: 0.2578\n",
      "iteration number: 98\t training loss: 2.2872\tvalidation loss: 2.2888\t validation accuracy: 0.2533\n",
      "iteration number: 99\t training loss: 2.2866\tvalidation loss: 2.2881\t validation accuracy: 0.2622\n",
      "iteration number: 100\t training loss: 2.2859\tvalidation loss: 2.2873\t validation accuracy: 0.2556\n",
      "iteration number: 101\t training loss: 2.2854\tvalidation loss: 2.2867\t validation accuracy: 0.2689\n",
      "iteration number: 102\t training loss: 2.2848\tvalidation loss: 2.2862\t validation accuracy: 0.2422\n",
      "iteration number: 103\t training loss: 2.2842\tvalidation loss: 2.2855\t validation accuracy: 0.2111\n",
      "iteration number: 104\t training loss: 2.2834\tvalidation loss: 2.2848\t validation accuracy: 0.2667\n",
      "iteration number: 105\t training loss: 2.2827\tvalidation loss: 2.2842\t validation accuracy: 0.2756\n",
      "iteration number: 106\t training loss: 2.2820\tvalidation loss: 2.2832\t validation accuracy: 0.2800\n",
      "iteration number: 107\t training loss: 2.2810\tvalidation loss: 2.2824\t validation accuracy: 0.2867\n",
      "iteration number: 108\t training loss: 2.2802\tvalidation loss: 2.2815\t validation accuracy: 0.3067\n",
      "iteration number: 109\t training loss: 2.2793\tvalidation loss: 2.2805\t validation accuracy: 0.3289\n",
      "iteration number: 110\t training loss: 2.2786\tvalidation loss: 2.2798\t validation accuracy: 0.2756\n",
      "iteration number: 111\t training loss: 2.2778\tvalidation loss: 2.2787\t validation accuracy: 0.2711\n",
      "iteration number: 112\t training loss: 2.2767\tvalidation loss: 2.2777\t validation accuracy: 0.2733\n",
      "iteration number: 113\t training loss: 2.2758\tvalidation loss: 2.2771\t validation accuracy: 0.2800\n",
      "iteration number: 114\t training loss: 2.2749\tvalidation loss: 2.2763\t validation accuracy: 0.2778\n",
      "iteration number: 115\t training loss: 2.2739\tvalidation loss: 2.2754\t validation accuracy: 0.2844\n",
      "iteration number: 116\t training loss: 2.2728\tvalidation loss: 2.2744\t validation accuracy: 0.3644\n",
      "iteration number: 117\t training loss: 2.2718\tvalidation loss: 2.2733\t validation accuracy: 0.2911\n",
      "iteration number: 118\t training loss: 2.2707\tvalidation loss: 2.2718\t validation accuracy: 0.2800\n",
      "iteration number: 119\t training loss: 2.2696\tvalidation loss: 2.2704\t validation accuracy: 0.2244\n",
      "iteration number: 120\t training loss: 2.2683\tvalidation loss: 2.2691\t validation accuracy: 0.2356\n",
      "iteration number: 121\t training loss: 2.2670\tvalidation loss: 2.2680\t validation accuracy: 0.2844\n",
      "iteration number: 122\t training loss: 2.2656\tvalidation loss: 2.2663\t validation accuracy: 0.2822\n",
      "iteration number: 123\t training loss: 2.2642\tvalidation loss: 2.2645\t validation accuracy: 0.2756\n",
      "iteration number: 124\t training loss: 2.2628\tvalidation loss: 2.2631\t validation accuracy: 0.2378\n",
      "iteration number: 125\t training loss: 2.2614\tvalidation loss: 2.2618\t validation accuracy: 0.2400\n",
      "iteration number: 126\t training loss: 2.2597\tvalidation loss: 2.2601\t validation accuracy: 0.3000\n",
      "iteration number: 127\t training loss: 2.2580\tvalidation loss: 2.2582\t validation accuracy: 0.3200\n",
      "iteration number: 128\t training loss: 2.2564\tvalidation loss: 2.2570\t validation accuracy: 0.3489\n",
      "iteration number: 129\t training loss: 2.2548\tvalidation loss: 2.2551\t validation accuracy: 0.3444\n",
      "iteration number: 130\t training loss: 2.2529\tvalidation loss: 2.2533\t validation accuracy: 0.3111\n",
      "iteration number: 131\t training loss: 2.2510\tvalidation loss: 2.2518\t validation accuracy: 0.3667\n",
      "iteration number: 132\t training loss: 2.2493\tvalidation loss: 2.2496\t validation accuracy: 0.3511\n",
      "iteration number: 133\t training loss: 2.2476\tvalidation loss: 2.2475\t validation accuracy: 0.3222\n",
      "iteration number: 134\t training loss: 2.2461\tvalidation loss: 2.2459\t validation accuracy: 0.3600\n",
      "iteration number: 135\t training loss: 2.2440\tvalidation loss: 2.2444\t validation accuracy: 0.3556\n",
      "iteration number: 136\t training loss: 2.2415\tvalidation loss: 2.2419\t validation accuracy: 0.4467\n",
      "iteration number: 137\t training loss: 2.2390\tvalidation loss: 2.2393\t validation accuracy: 0.4333\n",
      "iteration number: 138\t training loss: 2.2367\tvalidation loss: 2.2372\t validation accuracy: 0.3467\n",
      "iteration number: 139\t training loss: 2.2339\tvalidation loss: 2.2338\t validation accuracy: 0.4511\n",
      "iteration number: 140\t training loss: 2.2316\tvalidation loss: 2.2316\t validation accuracy: 0.4267\n",
      "iteration number: 141\t training loss: 2.2288\tvalidation loss: 2.2285\t validation accuracy: 0.5089\n",
      "iteration number: 142\t training loss: 2.2262\tvalidation loss: 2.2260\t validation accuracy: 0.5244\n",
      "iteration number: 143\t training loss: 2.2234\tvalidation loss: 2.2231\t validation accuracy: 0.5311\n",
      "iteration number: 144\t training loss: 2.2206\tvalidation loss: 2.2207\t validation accuracy: 0.4778\n",
      "iteration number: 145\t training loss: 2.2181\tvalidation loss: 2.2181\t validation accuracy: 0.4778\n",
      "iteration number: 146\t training loss: 2.2150\tvalidation loss: 2.2144\t validation accuracy: 0.5467\n",
      "iteration number: 147\t training loss: 2.2119\tvalidation loss: 2.2111\t validation accuracy: 0.5578\n",
      "iteration number: 148\t training loss: 2.2087\tvalidation loss: 2.2078\t validation accuracy: 0.5733\n",
      "iteration number: 149\t training loss: 2.2058\tvalidation loss: 2.2054\t validation accuracy: 0.5378\n",
      "iteration number: 150\t training loss: 2.2024\tvalidation loss: 2.2024\t validation accuracy: 0.5400\n",
      "iteration number: 151\t training loss: 2.1980\tvalidation loss: 2.1975\t validation accuracy: 0.5467\n",
      "iteration number: 152\t training loss: 2.1947\tvalidation loss: 2.1941\t validation accuracy: 0.5556\n",
      "iteration number: 153\t training loss: 2.1910\tvalidation loss: 2.1905\t validation accuracy: 0.5067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 154\t training loss: 2.1865\tvalidation loss: 2.1852\t validation accuracy: 0.5600\n",
      "iteration number: 155\t training loss: 2.1820\tvalidation loss: 2.1809\t validation accuracy: 0.4933\n",
      "iteration number: 156\t training loss: 2.1782\tvalidation loss: 2.1767\t validation accuracy: 0.5511\n",
      "iteration number: 157\t training loss: 2.1736\tvalidation loss: 2.1730\t validation accuracy: 0.5156\n",
      "iteration number: 158\t training loss: 2.1697\tvalidation loss: 2.1688\t validation accuracy: 0.5200\n",
      "iteration number: 159\t training loss: 2.1655\tvalidation loss: 2.1656\t validation accuracy: 0.4800\n",
      "iteration number: 160\t training loss: 2.1613\tvalidation loss: 2.1620\t validation accuracy: 0.4267\n",
      "iteration number: 161\t training loss: 2.1569\tvalidation loss: 2.1564\t validation accuracy: 0.4689\n",
      "iteration number: 162\t training loss: 2.1519\tvalidation loss: 2.1513\t validation accuracy: 0.4778\n",
      "iteration number: 163\t training loss: 2.1466\tvalidation loss: 2.1456\t validation accuracy: 0.4711\n",
      "iteration number: 164\t training loss: 2.1422\tvalidation loss: 2.1416\t validation accuracy: 0.4822\n",
      "iteration number: 165\t training loss: 2.1369\tvalidation loss: 2.1370\t validation accuracy: 0.5000\n",
      "iteration number: 166\t training loss: 2.1307\tvalidation loss: 2.1308\t validation accuracy: 0.4622\n",
      "iteration number: 167\t training loss: 2.1247\tvalidation loss: 2.1249\t validation accuracy: 0.4333\n",
      "iteration number: 168\t training loss: 2.1182\tvalidation loss: 2.1177\t validation accuracy: 0.4533\n",
      "iteration number: 169\t training loss: 2.1122\tvalidation loss: 2.1120\t validation accuracy: 0.4689\n",
      "iteration number: 170\t training loss: 2.1060\tvalidation loss: 2.1058\t validation accuracy: 0.4822\n",
      "iteration number: 171\t training loss: 2.1006\tvalidation loss: 2.1016\t validation accuracy: 0.4178\n",
      "iteration number: 172\t training loss: 2.0947\tvalidation loss: 2.0957\t validation accuracy: 0.3978\n",
      "iteration number: 173\t training loss: 2.0868\tvalidation loss: 2.0875\t validation accuracy: 0.4156\n",
      "iteration number: 174\t training loss: 2.0804\tvalidation loss: 2.0804\t validation accuracy: 0.4044\n",
      "iteration number: 175\t training loss: 2.0732\tvalidation loss: 2.0724\t validation accuracy: 0.4289\n",
      "iteration number: 176\t training loss: 2.0648\tvalidation loss: 2.0633\t validation accuracy: 0.4311\n",
      "iteration number: 177\t training loss: 2.0565\tvalidation loss: 2.0544\t validation accuracy: 0.4378\n",
      "iteration number: 178\t training loss: 2.0497\tvalidation loss: 2.0479\t validation accuracy: 0.4356\n",
      "iteration number: 179\t training loss: 2.0431\tvalidation loss: 2.0403\t validation accuracy: 0.4622\n",
      "iteration number: 180\t training loss: 2.0340\tvalidation loss: 2.0308\t validation accuracy: 0.4600\n",
      "iteration number: 181\t training loss: 2.0250\tvalidation loss: 2.0220\t validation accuracy: 0.4622\n",
      "iteration number: 182\t training loss: 2.0168\tvalidation loss: 2.0141\t validation accuracy: 0.4400\n",
      "iteration number: 183\t training loss: 2.0082\tvalidation loss: 2.0056\t validation accuracy: 0.4511\n",
      "iteration number: 184\t training loss: 2.0001\tvalidation loss: 1.9991\t validation accuracy: 0.4311\n",
      "iteration number: 185\t training loss: 1.9897\tvalidation loss: 1.9873\t validation accuracy: 0.4467\n",
      "iteration number: 186\t training loss: 1.9802\tvalidation loss: 1.9778\t validation accuracy: 0.4556\n",
      "iteration number: 187\t training loss: 1.9709\tvalidation loss: 1.9670\t validation accuracy: 0.4733\n",
      "iteration number: 188\t training loss: 1.9614\tvalidation loss: 1.9564\t validation accuracy: 0.4911\n",
      "iteration number: 189\t training loss: 1.9523\tvalidation loss: 1.9479\t validation accuracy: 0.4844\n",
      "iteration number: 190\t training loss: 1.9436\tvalidation loss: 1.9400\t validation accuracy: 0.4822\n",
      "iteration number: 191\t training loss: 1.9333\tvalidation loss: 1.9294\t validation accuracy: 0.4956\n",
      "iteration number: 192\t training loss: 1.9235\tvalidation loss: 1.9198\t validation accuracy: 0.4756\n",
      "iteration number: 193\t training loss: 1.9134\tvalidation loss: 1.9101\t validation accuracy: 0.4778\n",
      "iteration number: 194\t training loss: 1.9010\tvalidation loss: 1.8948\t validation accuracy: 0.4978\n",
      "iteration number: 195\t training loss: 1.8901\tvalidation loss: 1.8819\t validation accuracy: 0.5044\n",
      "iteration number: 196\t training loss: 1.8797\tvalidation loss: 1.8720\t validation accuracy: 0.5178\n",
      "iteration number: 197\t training loss: 1.8695\tvalidation loss: 1.8606\t validation accuracy: 0.5222\n",
      "iteration number: 198\t training loss: 1.8597\tvalidation loss: 1.8496\t validation accuracy: 0.5556\n",
      "iteration number: 199\t training loss: 1.8495\tvalidation loss: 1.8388\t validation accuracy: 0.5578\n",
      "iteration number: 200\t training loss: 1.8380\tvalidation loss: 1.8253\t validation accuracy: 0.6178\n",
      "iteration number: 201\t training loss: 1.8267\tvalidation loss: 1.8134\t validation accuracy: 0.6289\n",
      "iteration number: 202\t training loss: 1.8157\tvalidation loss: 1.8012\t validation accuracy: 0.6911\n",
      "iteration number: 203\t training loss: 1.8044\tvalidation loss: 1.7887\t validation accuracy: 0.6600\n",
      "iteration number: 204\t training loss: 1.7941\tvalidation loss: 1.7785\t validation accuracy: 0.6467\n",
      "iteration number: 205\t training loss: 1.7810\tvalidation loss: 1.7647\t validation accuracy: 0.6378\n",
      "iteration number: 206\t training loss: 1.7696\tvalidation loss: 1.7521\t validation accuracy: 0.6156\n",
      "iteration number: 207\t training loss: 1.7566\tvalidation loss: 1.7380\t validation accuracy: 0.5956\n",
      "iteration number: 208\t training loss: 1.7454\tvalidation loss: 1.7249\t validation accuracy: 0.6156\n",
      "iteration number: 209\t training loss: 1.7337\tvalidation loss: 1.7146\t validation accuracy: 0.6200\n",
      "iteration number: 210\t training loss: 1.7213\tvalidation loss: 1.7038\t validation accuracy: 0.6044\n",
      "iteration number: 211\t training loss: 1.7076\tvalidation loss: 1.6861\t validation accuracy: 0.5889\n",
      "iteration number: 212\t training loss: 1.6954\tvalidation loss: 1.6722\t validation accuracy: 0.5711\n",
      "iteration number: 213\t training loss: 1.6830\tvalidation loss: 1.6584\t validation accuracy: 0.5844\n",
      "iteration number: 214\t training loss: 1.6709\tvalidation loss: 1.6450\t validation accuracy: 0.5667\n",
      "iteration number: 215\t training loss: 1.6581\tvalidation loss: 1.6318\t validation accuracy: 0.5822\n",
      "iteration number: 216\t training loss: 1.6449\tvalidation loss: 1.6201\t validation accuracy: 0.6178\n",
      "iteration number: 217\t training loss: 1.6325\tvalidation loss: 1.6061\t validation accuracy: 0.6333\n",
      "iteration number: 218\t training loss: 1.6211\tvalidation loss: 1.5914\t validation accuracy: 0.6556\n",
      "iteration number: 219\t training loss: 1.6092\tvalidation loss: 1.5812\t validation accuracy: 0.6178\n",
      "iteration number: 220\t training loss: 1.5965\tvalidation loss: 1.5704\t validation accuracy: 0.6289\n",
      "iteration number: 221\t training loss: 1.5841\tvalidation loss: 1.5594\t validation accuracy: 0.6222\n",
      "iteration number: 222\t training loss: 1.5719\tvalidation loss: 1.5438\t validation accuracy: 0.6578\n",
      "iteration number: 223\t training loss: 1.5603\tvalidation loss: 1.5303\t validation accuracy: 0.6489\n",
      "iteration number: 224\t training loss: 1.5487\tvalidation loss: 1.5171\t validation accuracy: 0.6978\n",
      "iteration number: 225\t training loss: 1.5355\tvalidation loss: 1.5036\t validation accuracy: 0.7267\n",
      "iteration number: 226\t training loss: 1.5228\tvalidation loss: 1.4899\t validation accuracy: 0.7244\n",
      "iteration number: 227\t training loss: 1.5104\tvalidation loss: 1.4785\t validation accuracy: 0.7244\n",
      "iteration number: 228\t training loss: 1.4978\tvalidation loss: 1.4674\t validation accuracy: 0.7200\n",
      "iteration number: 229\t training loss: 1.4844\tvalidation loss: 1.4519\t validation accuracy: 0.7533\n",
      "iteration number: 230\t training loss: 1.4719\tvalidation loss: 1.4385\t validation accuracy: 0.7533\n",
      "iteration number: 231\t training loss: 1.4600\tvalidation loss: 1.4280\t validation accuracy: 0.7533\n",
      "iteration number: 232\t training loss: 1.4510\tvalidation loss: 1.4196\t validation accuracy: 0.7333\n",
      "iteration number: 233\t training loss: 1.4380\tvalidation loss: 1.4056\t validation accuracy: 0.7378\n",
      "iteration number: 234\t training loss: 1.4234\tvalidation loss: 1.3881\t validation accuracy: 0.7644\n",
      "iteration number: 235\t training loss: 1.4174\tvalidation loss: 1.3825\t validation accuracy: 0.7689\n",
      "iteration number: 236\t training loss: 1.3989\tvalidation loss: 1.3640\t validation accuracy: 0.7444\n",
      "iteration number: 237\t training loss: 1.3868\tvalidation loss: 1.3498\t validation accuracy: 0.7489\n",
      "iteration number: 238\t training loss: 1.3744\tvalidation loss: 1.3360\t validation accuracy: 0.7756\n",
      "iteration number: 239\t training loss: 1.3644\tvalidation loss: 1.3256\t validation accuracy: 0.7511\n",
      "iteration number: 240\t training loss: 1.3530\tvalidation loss: 1.3142\t validation accuracy: 0.7333\n",
      "iteration number: 241\t training loss: 1.3426\tvalidation loss: 1.3030\t validation accuracy: 0.7400\n",
      "iteration number: 242\t training loss: 1.3333\tvalidation loss: 1.2943\t validation accuracy: 0.7533\n",
      "iteration number: 243\t training loss: 1.3210\tvalidation loss: 1.2821\t validation accuracy: 0.7444\n",
      "iteration number: 244\t training loss: 1.3081\tvalidation loss: 1.2702\t validation accuracy: 0.7511\n",
      "iteration number: 245\t training loss: 1.2981\tvalidation loss: 1.2565\t validation accuracy: 0.7622\n",
      "iteration number: 246\t training loss: 1.2881\tvalidation loss: 1.2477\t validation accuracy: 0.7267\n",
      "iteration number: 247\t training loss: 1.2778\tvalidation loss: 1.2391\t validation accuracy: 0.7556\n",
      "iteration number: 248\t training loss: 1.2675\tvalidation loss: 1.2301\t validation accuracy: 0.7622\n",
      "iteration number: 249\t training loss: 1.2562\tvalidation loss: 1.2153\t validation accuracy: 0.7889\n",
      "iteration number: 250\t training loss: 1.2468\tvalidation loss: 1.2071\t validation accuracy: 0.7733\n",
      "iteration number: 251\t training loss: 1.2349\tvalidation loss: 1.1943\t validation accuracy: 0.8089\n",
      "iteration number: 252\t training loss: 1.2275\tvalidation loss: 1.1808\t validation accuracy: 0.7733\n",
      "iteration number: 253\t training loss: 1.2194\tvalidation loss: 1.1707\t validation accuracy: 0.7711\n",
      "iteration number: 254\t training loss: 1.2082\tvalidation loss: 1.1593\t validation accuracy: 0.7911\n",
      "iteration number: 255\t training loss: 1.1967\tvalidation loss: 1.1514\t validation accuracy: 0.8089\n",
      "iteration number: 256\t training loss: 1.1874\tvalidation loss: 1.1426\t validation accuracy: 0.7911\n",
      "iteration number: 257\t training loss: 1.1771\tvalidation loss: 1.1309\t validation accuracy: 0.7889\n",
      "iteration number: 258\t training loss: 1.1685\tvalidation loss: 1.1227\t validation accuracy: 0.7889\n",
      "iteration number: 259\t training loss: 1.1560\tvalidation loss: 1.1103\t validation accuracy: 0.7867\n",
      "iteration number: 260\t training loss: 1.1491\tvalidation loss: 1.1018\t validation accuracy: 0.7933\n",
      "iteration number: 261\t training loss: 1.1384\tvalidation loss: 1.0908\t validation accuracy: 0.7822\n",
      "iteration number: 262\t training loss: 1.1287\tvalidation loss: 1.0797\t validation accuracy: 0.7889\n",
      "iteration number: 263\t training loss: 1.1192\tvalidation loss: 1.0695\t validation accuracy: 0.7889\n",
      "iteration number: 264\t training loss: 1.1116\tvalidation loss: 1.0634\t validation accuracy: 0.8311\n",
      "iteration number: 265\t training loss: 1.1029\tvalidation loss: 1.0560\t validation accuracy: 0.8156\n",
      "iteration number: 266\t training loss: 1.0965\tvalidation loss: 1.0462\t validation accuracy: 0.8378\n",
      "iteration number: 267\t training loss: 1.0889\tvalidation loss: 1.0413\t validation accuracy: 0.8356\n",
      "iteration number: 268\t training loss: 1.0758\tvalidation loss: 1.0280\t validation accuracy: 0.8356\n",
      "iteration number: 269\t training loss: 1.0663\tvalidation loss: 1.0196\t validation accuracy: 0.8556\n",
      "iteration number: 270\t training loss: 1.0589\tvalidation loss: 1.0102\t validation accuracy: 0.8378\n",
      "iteration number: 271\t training loss: 1.0494\tvalidation loss: 1.0029\t validation accuracy: 0.8178\n",
      "iteration number: 272\t training loss: 1.0422\tvalidation loss: 0.9923\t validation accuracy: 0.8289\n",
      "iteration number: 273\t training loss: 1.0341\tvalidation loss: 0.9836\t validation accuracy: 0.8378\n",
      "iteration number: 274\t training loss: 1.0274\tvalidation loss: 0.9788\t validation accuracy: 0.8267\n",
      "iteration number: 275\t training loss: 1.0214\tvalidation loss: 0.9712\t validation accuracy: 0.8133\n",
      "iteration number: 276\t training loss: 1.0121\tvalidation loss: 0.9592\t validation accuracy: 0.8178\n",
      "iteration number: 277\t training loss: 1.0009\tvalidation loss: 0.9506\t validation accuracy: 0.8333\n",
      "iteration number: 278\t training loss: 0.9940\tvalidation loss: 0.9437\t validation accuracy: 0.8311\n",
      "iteration number: 279\t training loss: 0.9872\tvalidation loss: 0.9386\t validation accuracy: 0.8422\n",
      "iteration number: 280\t training loss: 0.9784\tvalidation loss: 0.9309\t validation accuracy: 0.8400\n",
      "iteration number: 281\t training loss: 0.9747\tvalidation loss: 0.9280\t validation accuracy: 0.8156\n",
      "iteration number: 282\t training loss: 0.9684\tvalidation loss: 0.9203\t validation accuracy: 0.8200\n",
      "iteration number: 283\t training loss: 0.9594\tvalidation loss: 0.9134\t validation accuracy: 0.8444\n",
      "iteration number: 284\t training loss: 0.9496\tvalidation loss: 0.8997\t validation accuracy: 0.8378\n",
      "iteration number: 285\t training loss: 0.9431\tvalidation loss: 0.8952\t validation accuracy: 0.8467\n",
      "iteration number: 286\t training loss: 0.9387\tvalidation loss: 0.8902\t validation accuracy: 0.8356\n",
      "iteration number: 287\t training loss: 0.9313\tvalidation loss: 0.8832\t validation accuracy: 0.8444\n",
      "iteration number: 288\t training loss: 0.9211\tvalidation loss: 0.8717\t validation accuracy: 0.8533\n",
      "iteration number: 289\t training loss: 0.9151\tvalidation loss: 0.8644\t validation accuracy: 0.8378\n",
      "iteration number: 290\t training loss: 0.9118\tvalidation loss: 0.8570\t validation accuracy: 0.8200\n",
      "iteration number: 291\t training loss: 0.9040\tvalidation loss: 0.8494\t validation accuracy: 0.8267\n",
      "iteration number: 292\t training loss: 0.8979\tvalidation loss: 0.8453\t validation accuracy: 0.8133\n",
      "iteration number: 293\t training loss: 0.8891\tvalidation loss: 0.8356\t validation accuracy: 0.8289\n",
      "iteration number: 294\t training loss: 0.8830\tvalidation loss: 0.8324\t validation accuracy: 0.8533\n",
      "iteration number: 295\t training loss: 0.8761\tvalidation loss: 0.8266\t validation accuracy: 0.8756\n",
      "iteration number: 296\t training loss: 0.8690\tvalidation loss: 0.8171\t validation accuracy: 0.8844\n",
      "iteration number: 297\t training loss: 0.8646\tvalidation loss: 0.8125\t validation accuracy: 0.8644\n",
      "iteration number: 298\t training loss: 0.8565\tvalidation loss: 0.8056\t validation accuracy: 0.8667\n",
      "iteration number: 299\t training loss: 0.8564\tvalidation loss: 0.8084\t validation accuracy: 0.8733\n",
      "iteration number: 300\t training loss: 0.8465\tvalidation loss: 0.7948\t validation accuracy: 0.8756\n",
      "iteration number: 301\t training loss: 0.8401\tvalidation loss: 0.7912\t validation accuracy: 0.8667\n",
      "iteration number: 302\t training loss: 0.8331\tvalidation loss: 0.7826\t validation accuracy: 0.8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 303\t training loss: 0.8281\tvalidation loss: 0.7771\t validation accuracy: 0.8822\n",
      "iteration number: 304\t training loss: 0.8203\tvalidation loss: 0.7698\t validation accuracy: 0.8756\n",
      "iteration number: 305\t training loss: 0.8151\tvalidation loss: 0.7612\t validation accuracy: 0.8756\n",
      "iteration number: 306\t training loss: 0.8125\tvalidation loss: 0.7604\t validation accuracy: 0.8578\n",
      "iteration number: 307\t training loss: 0.8039\tvalidation loss: 0.7510\t validation accuracy: 0.8778\n",
      "iteration number: 308\t training loss: 0.8014\tvalidation loss: 0.7477\t validation accuracy: 0.8800\n",
      "iteration number: 309\t training loss: 0.7947\tvalidation loss: 0.7433\t validation accuracy: 0.8667\n",
      "iteration number: 310\t training loss: 0.7873\tvalidation loss: 0.7344\t validation accuracy: 0.8778\n",
      "iteration number: 311\t training loss: 0.7841\tvalidation loss: 0.7326\t validation accuracy: 0.8756\n",
      "iteration number: 312\t training loss: 0.7796\tvalidation loss: 0.7273\t validation accuracy: 0.8689\n",
      "iteration number: 313\t training loss: 0.7777\tvalidation loss: 0.7265\t validation accuracy: 0.8689\n",
      "iteration number: 314\t training loss: 0.7798\tvalidation loss: 0.7302\t validation accuracy: 0.8578\n",
      "iteration number: 315\t training loss: 0.7740\tvalidation loss: 0.7255\t validation accuracy: 0.8578\n",
      "iteration number: 316\t training loss: 0.7665\tvalidation loss: 0.7159\t validation accuracy: 0.8733\n",
      "iteration number: 317\t training loss: 0.7557\tvalidation loss: 0.7041\t validation accuracy: 0.8622\n",
      "iteration number: 318\t training loss: 0.7485\tvalidation loss: 0.6957\t validation accuracy: 0.8667\n",
      "iteration number: 319\t training loss: 0.7483\tvalidation loss: 0.6958\t validation accuracy: 0.8733\n",
      "iteration number: 320\t training loss: 0.7388\tvalidation loss: 0.6861\t validation accuracy: 0.8844\n",
      "iteration number: 321\t training loss: 0.7345\tvalidation loss: 0.6773\t validation accuracy: 0.8844\n",
      "iteration number: 322\t training loss: 0.7329\tvalidation loss: 0.6773\t validation accuracy: 0.8933\n",
      "iteration number: 323\t training loss: 0.7291\tvalidation loss: 0.6740\t validation accuracy: 0.8956\n",
      "iteration number: 324\t training loss: 0.7219\tvalidation loss: 0.6674\t validation accuracy: 0.8933\n",
      "iteration number: 325\t training loss: 0.7167\tvalidation loss: 0.6636\t validation accuracy: 0.8978\n",
      "iteration number: 326\t training loss: 0.7128\tvalidation loss: 0.6608\t validation accuracy: 0.8844\n",
      "iteration number: 327\t training loss: 0.7121\tvalidation loss: 0.6604\t validation accuracy: 0.8756\n",
      "iteration number: 328\t training loss: 0.7074\tvalidation loss: 0.6584\t validation accuracy: 0.8711\n",
      "iteration number: 329\t training loss: 0.7022\tvalidation loss: 0.6519\t validation accuracy: 0.8711\n",
      "iteration number: 330\t training loss: 0.6967\tvalidation loss: 0.6424\t validation accuracy: 0.8800\n",
      "iteration number: 331\t training loss: 0.6943\tvalidation loss: 0.6389\t validation accuracy: 0.8689\n",
      "iteration number: 332\t training loss: 0.6852\tvalidation loss: 0.6302\t validation accuracy: 0.8933\n",
      "iteration number: 333\t training loss: 0.6819\tvalidation loss: 0.6276\t validation accuracy: 0.9000\n",
      "iteration number: 334\t training loss: 0.6772\tvalidation loss: 0.6223\t validation accuracy: 0.8956\n",
      "iteration number: 335\t training loss: 0.6746\tvalidation loss: 0.6221\t validation accuracy: 0.8933\n",
      "iteration number: 336\t training loss: 0.6697\tvalidation loss: 0.6167\t validation accuracy: 0.8956\n",
      "iteration number: 337\t training loss: 0.6684\tvalidation loss: 0.6143\t validation accuracy: 0.8911\n",
      "iteration number: 338\t training loss: 0.6626\tvalidation loss: 0.6088\t validation accuracy: 0.8978\n",
      "iteration number: 339\t training loss: 0.6592\tvalidation loss: 0.6055\t validation accuracy: 0.8956\n",
      "iteration number: 340\t training loss: 0.6553\tvalidation loss: 0.6027\t validation accuracy: 0.8889\n",
      "iteration number: 341\t training loss: 0.6509\tvalidation loss: 0.5972\t validation accuracy: 0.8911\n",
      "iteration number: 342\t training loss: 0.6472\tvalidation loss: 0.5952\t validation accuracy: 0.8889\n",
      "iteration number: 343\t training loss: 0.6411\tvalidation loss: 0.5879\t validation accuracy: 0.8978\n",
      "iteration number: 344\t training loss: 0.6404\tvalidation loss: 0.5867\t validation accuracy: 0.8978\n",
      "iteration number: 345\t training loss: 0.6373\tvalidation loss: 0.5840\t validation accuracy: 0.9000\n",
      "iteration number: 346\t training loss: 0.6315\tvalidation loss: 0.5772\t validation accuracy: 0.8956\n",
      "iteration number: 347\t training loss: 0.6293\tvalidation loss: 0.5759\t validation accuracy: 0.8978\n",
      "iteration number: 348\t training loss: 0.6252\tvalidation loss: 0.5742\t validation accuracy: 0.8956\n",
      "iteration number: 349\t training loss: 0.6236\tvalidation loss: 0.5721\t validation accuracy: 0.9044\n",
      "iteration number: 350\t training loss: 0.6186\tvalidation loss: 0.5656\t validation accuracy: 0.9022\n",
      "iteration number: 351\t training loss: 0.6155\tvalidation loss: 0.5654\t validation accuracy: 0.9111\n",
      "iteration number: 352\t training loss: 0.6169\tvalidation loss: 0.5657\t validation accuracy: 0.9044\n",
      "iteration number: 353\t training loss: 0.6149\tvalidation loss: 0.5646\t validation accuracy: 0.9022\n",
      "iteration number: 354\t training loss: 0.6066\tvalidation loss: 0.5536\t validation accuracy: 0.9022\n",
      "iteration number: 355\t training loss: 0.6058\tvalidation loss: 0.5540\t validation accuracy: 0.9022\n",
      "iteration number: 356\t training loss: 0.6033\tvalidation loss: 0.5505\t validation accuracy: 0.9000\n",
      "iteration number: 357\t training loss: 0.6000\tvalidation loss: 0.5457\t validation accuracy: 0.9022\n",
      "iteration number: 358\t training loss: 0.5979\tvalidation loss: 0.5431\t validation accuracy: 0.9089\n",
      "iteration number: 359\t training loss: 0.5928\tvalidation loss: 0.5382\t validation accuracy: 0.9067\n",
      "iteration number: 360\t training loss: 0.5939\tvalidation loss: 0.5380\t validation accuracy: 0.8956\n",
      "iteration number: 361\t training loss: 0.5883\tvalidation loss: 0.5350\t validation accuracy: 0.8978\n",
      "iteration number: 362\t training loss: 0.5863\tvalidation loss: 0.5366\t validation accuracy: 0.8933\n",
      "iteration number: 363\t training loss: 0.5823\tvalidation loss: 0.5320\t validation accuracy: 0.8978\n",
      "iteration number: 364\t training loss: 0.5787\tvalidation loss: 0.5285\t validation accuracy: 0.9067\n",
      "iteration number: 365\t training loss: 0.5747\tvalidation loss: 0.5259\t validation accuracy: 0.9022\n",
      "iteration number: 366\t training loss: 0.5739\tvalidation loss: 0.5257\t validation accuracy: 0.9000\n",
      "iteration number: 367\t training loss: 0.5667\tvalidation loss: 0.5168\t validation accuracy: 0.9067\n",
      "iteration number: 368\t training loss: 0.5663\tvalidation loss: 0.5181\t validation accuracy: 0.9044\n",
      "iteration number: 369\t training loss: 0.5698\tvalidation loss: 0.5244\t validation accuracy: 0.8978\n",
      "iteration number: 370\t training loss: 0.5685\tvalidation loss: 0.5226\t validation accuracy: 0.8889\n",
      "iteration number: 371\t training loss: 0.5582\tvalidation loss: 0.5093\t validation accuracy: 0.9022\n",
      "iteration number: 372\t training loss: 0.5546\tvalidation loss: 0.5047\t validation accuracy: 0.9089\n",
      "iteration number: 373\t training loss: 0.5514\tvalidation loss: 0.5003\t validation accuracy: 0.9111\n",
      "iteration number: 374\t training loss: 0.5469\tvalidation loss: 0.4961\t validation accuracy: 0.9067\n",
      "iteration number: 375\t training loss: 0.5476\tvalidation loss: 0.4996\t validation accuracy: 0.9178\n",
      "iteration number: 376\t training loss: 0.5437\tvalidation loss: 0.4956\t validation accuracy: 0.9111\n",
      "iteration number: 377\t training loss: 0.5441\tvalidation loss: 0.4942\t validation accuracy: 0.9067\n",
      "iteration number: 378\t training loss: 0.5372\tvalidation loss: 0.4854\t validation accuracy: 0.9133\n",
      "iteration number: 379\t training loss: 0.5369\tvalidation loss: 0.4852\t validation accuracy: 0.9133\n",
      "iteration number: 380\t training loss: 0.5321\tvalidation loss: 0.4813\t validation accuracy: 0.9111\n",
      "iteration number: 381\t training loss: 0.5298\tvalidation loss: 0.4796\t validation accuracy: 0.9133\n",
      "iteration number: 382\t training loss: 0.5269\tvalidation loss: 0.4778\t validation accuracy: 0.9178\n",
      "iteration number: 383\t training loss: 0.5253\tvalidation loss: 0.4766\t validation accuracy: 0.9178\n",
      "iteration number: 384\t training loss: 0.5258\tvalidation loss: 0.4762\t validation accuracy: 0.9178\n",
      "iteration number: 385\t training loss: 0.5222\tvalidation loss: 0.4726\t validation accuracy: 0.9133\n",
      "iteration number: 386\t training loss: 0.5227\tvalidation loss: 0.4752\t validation accuracy: 0.9111\n",
      "iteration number: 387\t training loss: 0.5163\tvalidation loss: 0.4677\t validation accuracy: 0.9156\n",
      "iteration number: 388\t training loss: 0.5135\tvalidation loss: 0.4653\t validation accuracy: 0.9133\n",
      "iteration number: 389\t training loss: 0.5119\tvalidation loss: 0.4631\t validation accuracy: 0.9178\n",
      "iteration number: 390\t training loss: 0.5087\tvalidation loss: 0.4577\t validation accuracy: 0.9133\n",
      "iteration number: 391\t training loss: 0.5059\tvalidation loss: 0.4537\t validation accuracy: 0.9244\n",
      "iteration number: 392\t training loss: 0.5034\tvalidation loss: 0.4527\t validation accuracy: 0.9200\n",
      "iteration number: 393\t training loss: 0.5031\tvalidation loss: 0.4525\t validation accuracy: 0.9178\n",
      "iteration number: 394\t training loss: 0.4996\tvalidation loss: 0.4510\t validation accuracy: 0.9267\n",
      "iteration number: 395\t training loss: 0.5016\tvalidation loss: 0.4538\t validation accuracy: 0.9156\n",
      "iteration number: 396\t training loss: 0.4972\tvalidation loss: 0.4494\t validation accuracy: 0.9178\n",
      "iteration number: 397\t training loss: 0.4917\tvalidation loss: 0.4425\t validation accuracy: 0.9244\n",
      "iteration number: 398\t training loss: 0.4945\tvalidation loss: 0.4477\t validation accuracy: 0.9200\n",
      "iteration number: 399\t training loss: 0.4934\tvalidation loss: 0.4445\t validation accuracy: 0.9111\n",
      "iteration number: 400\t training loss: 0.4863\tvalidation loss: 0.4368\t validation accuracy: 0.9222\n",
      "iteration number: 401\t training loss: 0.4848\tvalidation loss: 0.4351\t validation accuracy: 0.9244\n",
      "iteration number: 402\t training loss: 0.4840\tvalidation loss: 0.4341\t validation accuracy: 0.9200\n",
      "iteration number: 403\t training loss: 0.4803\tvalidation loss: 0.4320\t validation accuracy: 0.9244\n",
      "iteration number: 404\t training loss: 0.4814\tvalidation loss: 0.4332\t validation accuracy: 0.9200\n",
      "iteration number: 405\t training loss: 0.4754\tvalidation loss: 0.4253\t validation accuracy: 0.9222\n",
      "iteration number: 406\t training loss: 0.4745\tvalidation loss: 0.4249\t validation accuracy: 0.9244\n",
      "iteration number: 407\t training loss: 0.4722\tvalidation loss: 0.4240\t validation accuracy: 0.9267\n",
      "iteration number: 408\t training loss: 0.4719\tvalidation loss: 0.4249\t validation accuracy: 0.9222\n",
      "iteration number: 409\t training loss: 0.4701\tvalidation loss: 0.4219\t validation accuracy: 0.9289\n",
      "iteration number: 410\t training loss: 0.4701\tvalidation loss: 0.4206\t validation accuracy: 0.9156\n",
      "iteration number: 411\t training loss: 0.4750\tvalidation loss: 0.4248\t validation accuracy: 0.9200\n",
      "iteration number: 412\t training loss: 0.4753\tvalidation loss: 0.4263\t validation accuracy: 0.9178\n",
      "iteration number: 413\t training loss: 0.4704\tvalidation loss: 0.4206\t validation accuracy: 0.9200\n",
      "iteration number: 414\t training loss: 0.4645\tvalidation loss: 0.4164\t validation accuracy: 0.9244\n",
      "iteration number: 415\t training loss: 0.4625\tvalidation loss: 0.4163\t validation accuracy: 0.9289\n",
      "iteration number: 416\t training loss: 0.4576\tvalidation loss: 0.4111\t validation accuracy: 0.9267\n",
      "iteration number: 417\t training loss: 0.4559\tvalidation loss: 0.4087\t validation accuracy: 0.9267\n",
      "iteration number: 418\t training loss: 0.4531\tvalidation loss: 0.4056\t validation accuracy: 0.9289\n",
      "iteration number: 419\t training loss: 0.4506\tvalidation loss: 0.4015\t validation accuracy: 0.9289\n",
      "iteration number: 420\t training loss: 0.4509\tvalidation loss: 0.4029\t validation accuracy: 0.9267\n",
      "iteration number: 421\t training loss: 0.4489\tvalidation loss: 0.4004\t validation accuracy: 0.9267\n",
      "iteration number: 422\t training loss: 0.4562\tvalidation loss: 0.4072\t validation accuracy: 0.9156\n",
      "iteration number: 423\t training loss: 0.4494\tvalidation loss: 0.4017\t validation accuracy: 0.9200\n",
      "iteration number: 424\t training loss: 0.4484\tvalidation loss: 0.4013\t validation accuracy: 0.9244\n",
      "iteration number: 425\t training loss: 0.4444\tvalidation loss: 0.3968\t validation accuracy: 0.9289\n",
      "iteration number: 426\t training loss: 0.4428\tvalidation loss: 0.3974\t validation accuracy: 0.9244\n",
      "iteration number: 427\t training loss: 0.4397\tvalidation loss: 0.3937\t validation accuracy: 0.9200\n",
      "iteration number: 428\t training loss: 0.4376\tvalidation loss: 0.3905\t validation accuracy: 0.9289\n",
      "iteration number: 429\t training loss: 0.4395\tvalidation loss: 0.3930\t validation accuracy: 0.9178\n",
      "iteration number: 430\t training loss: 0.4331\tvalidation loss: 0.3882\t validation accuracy: 0.9244\n",
      "iteration number: 431\t training loss: 0.4312\tvalidation loss: 0.3847\t validation accuracy: 0.9333\n",
      "iteration number: 432\t training loss: 0.4376\tvalidation loss: 0.3906\t validation accuracy: 0.9156\n",
      "iteration number: 433\t training loss: 0.4346\tvalidation loss: 0.3891\t validation accuracy: 0.9244\n",
      "iteration number: 434\t training loss: 0.4263\tvalidation loss: 0.3802\t validation accuracy: 0.9311\n",
      "iteration number: 435\t training loss: 0.4260\tvalidation loss: 0.3803\t validation accuracy: 0.9267\n",
      "iteration number: 436\t training loss: 0.4267\tvalidation loss: 0.3810\t validation accuracy: 0.9311\n",
      "iteration number: 437\t training loss: 0.4260\tvalidation loss: 0.3799\t validation accuracy: 0.9200\n",
      "iteration number: 438\t training loss: 0.4267\tvalidation loss: 0.3786\t validation accuracy: 0.9200\n",
      "iteration number: 439\t training loss: 0.4205\tvalidation loss: 0.3734\t validation accuracy: 0.9222\n",
      "iteration number: 440\t training loss: 0.4182\tvalidation loss: 0.3726\t validation accuracy: 0.9267\n",
      "iteration number: 441\t training loss: 0.4180\tvalidation loss: 0.3736\t validation accuracy: 0.9267\n",
      "iteration number: 442\t training loss: 0.4166\tvalidation loss: 0.3701\t validation accuracy: 0.9289\n",
      "iteration number: 443\t training loss: 0.4169\tvalidation loss: 0.3696\t validation accuracy: 0.9311\n",
      "iteration number: 444\t training loss: 0.4174\tvalidation loss: 0.3695\t validation accuracy: 0.9178\n",
      "iteration number: 445\t training loss: 0.4129\tvalidation loss: 0.3675\t validation accuracy: 0.9222\n",
      "iteration number: 446\t training loss: 0.4085\tvalidation loss: 0.3660\t validation accuracy: 0.9267\n",
      "iteration number: 447\t training loss: 0.4076\tvalidation loss: 0.3649\t validation accuracy: 0.9289\n",
      "iteration number: 448\t training loss: 0.4072\tvalidation loss: 0.3625\t validation accuracy: 0.9244\n",
      "iteration number: 449\t training loss: 0.4030\tvalidation loss: 0.3592\t validation accuracy: 0.9311\n",
      "iteration number: 450\t training loss: 0.4025\tvalidation loss: 0.3590\t validation accuracy: 0.9333\n",
      "iteration number: 451\t training loss: 0.4075\tvalidation loss: 0.3632\t validation accuracy: 0.9356\n",
      "iteration number: 452\t training loss: 0.4066\tvalidation loss: 0.3635\t validation accuracy: 0.9311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 453\t training loss: 0.4001\tvalidation loss: 0.3592\t validation accuracy: 0.9311\n",
      "iteration number: 454\t training loss: 0.3958\tvalidation loss: 0.3533\t validation accuracy: 0.9356\n",
      "iteration number: 455\t training loss: 0.3966\tvalidation loss: 0.3543\t validation accuracy: 0.9333\n",
      "iteration number: 456\t training loss: 0.3962\tvalidation loss: 0.3544\t validation accuracy: 0.9356\n",
      "iteration number: 457\t training loss: 0.3938\tvalidation loss: 0.3522\t validation accuracy: 0.9356\n",
      "iteration number: 458\t training loss: 0.3932\tvalidation loss: 0.3529\t validation accuracy: 0.9333\n",
      "iteration number: 459\t training loss: 0.3925\tvalidation loss: 0.3520\t validation accuracy: 0.9356\n",
      "iteration number: 460\t training loss: 0.3920\tvalidation loss: 0.3530\t validation accuracy: 0.9333\n",
      "iteration number: 461\t training loss: 0.3943\tvalidation loss: 0.3550\t validation accuracy: 0.9378\n",
      "iteration number: 462\t training loss: 0.3874\tvalidation loss: 0.3468\t validation accuracy: 0.9378\n",
      "iteration number: 463\t training loss: 0.3943\tvalidation loss: 0.3518\t validation accuracy: 0.9333\n",
      "iteration number: 464\t training loss: 0.3886\tvalidation loss: 0.3476\t validation accuracy: 0.9378\n",
      "iteration number: 465\t training loss: 0.3931\tvalidation loss: 0.3523\t validation accuracy: 0.9311\n",
      "iteration number: 466\t training loss: 0.3958\tvalidation loss: 0.3556\t validation accuracy: 0.9289\n",
      "iteration number: 467\t training loss: 0.3874\tvalidation loss: 0.3473\t validation accuracy: 0.9356\n",
      "iteration number: 468\t training loss: 0.3821\tvalidation loss: 0.3418\t validation accuracy: 0.9378\n",
      "iteration number: 469\t training loss: 0.3782\tvalidation loss: 0.3387\t validation accuracy: 0.9378\n",
      "iteration number: 470\t training loss: 0.3793\tvalidation loss: 0.3393\t validation accuracy: 0.9400\n",
      "iteration number: 471\t training loss: 0.3797\tvalidation loss: 0.3391\t validation accuracy: 0.9378\n",
      "iteration number: 472\t training loss: 0.3767\tvalidation loss: 0.3364\t validation accuracy: 0.9400\n",
      "iteration number: 473\t training loss: 0.3759\tvalidation loss: 0.3374\t validation accuracy: 0.9400\n",
      "iteration number: 474\t training loss: 0.3742\tvalidation loss: 0.3368\t validation accuracy: 0.9378\n",
      "iteration number: 475\t training loss: 0.3726\tvalidation loss: 0.3352\t validation accuracy: 0.9378\n",
      "iteration number: 476\t training loss: 0.3710\tvalidation loss: 0.3325\t validation accuracy: 0.9356\n",
      "iteration number: 477\t training loss: 0.3686\tvalidation loss: 0.3311\t validation accuracy: 0.9356\n",
      "iteration number: 478\t training loss: 0.3717\tvalidation loss: 0.3343\t validation accuracy: 0.9333\n",
      "iteration number: 479\t training loss: 0.3715\tvalidation loss: 0.3308\t validation accuracy: 0.9400\n",
      "iteration number: 480\t training loss: 0.3671\tvalidation loss: 0.3273\t validation accuracy: 0.9378\n",
      "iteration number: 481\t training loss: 0.3683\tvalidation loss: 0.3273\t validation accuracy: 0.9333\n",
      "iteration number: 482\t training loss: 0.3700\tvalidation loss: 0.3278\t validation accuracy: 0.9311\n",
      "iteration number: 483\t training loss: 0.3674\tvalidation loss: 0.3259\t validation accuracy: 0.9356\n",
      "iteration number: 484\t training loss: 0.3644\tvalidation loss: 0.3246\t validation accuracy: 0.9400\n",
      "iteration number: 485\t training loss: 0.3621\tvalidation loss: 0.3223\t validation accuracy: 0.9400\n",
      "iteration number: 486\t training loss: 0.3599\tvalidation loss: 0.3206\t validation accuracy: 0.9378\n",
      "iteration number: 487\t training loss: 0.3602\tvalidation loss: 0.3206\t validation accuracy: 0.9422\n",
      "iteration number: 488\t training loss: 0.3588\tvalidation loss: 0.3199\t validation accuracy: 0.9422\n",
      "iteration number: 489\t training loss: 0.3579\tvalidation loss: 0.3184\t validation accuracy: 0.9356\n",
      "iteration number: 490\t training loss: 0.3558\tvalidation loss: 0.3176\t validation accuracy: 0.9333\n",
      "iteration number: 491\t training loss: 0.3565\tvalidation loss: 0.3171\t validation accuracy: 0.9400\n",
      "iteration number: 492\t training loss: 0.3567\tvalidation loss: 0.3195\t validation accuracy: 0.9378\n",
      "iteration number: 493\t training loss: 0.3533\tvalidation loss: 0.3164\t validation accuracy: 0.9400\n",
      "iteration number: 494\t training loss: 0.3487\tvalidation loss: 0.3140\t validation accuracy: 0.9311\n",
      "iteration number: 495\t training loss: 0.3510\tvalidation loss: 0.3166\t validation accuracy: 0.9356\n",
      "iteration number: 496\t training loss: 0.3525\tvalidation loss: 0.3172\t validation accuracy: 0.9400\n",
      "iteration number: 497\t training loss: 0.3511\tvalidation loss: 0.3158\t validation accuracy: 0.9356\n",
      "iteration number: 498\t training loss: 0.3484\tvalidation loss: 0.3141\t validation accuracy: 0.9422\n",
      "iteration number: 499\t training loss: 0.3525\tvalidation loss: 0.3157\t validation accuracy: 0.9400\n",
      "iteration number: 500\t training loss: 0.3446\tvalidation loss: 0.3079\t validation accuracy: 0.9400\n",
      "iteration number: 501\t training loss: 0.3504\tvalidation loss: 0.3119\t validation accuracy: 0.9356\n",
      "iteration number: 502\t training loss: 0.3509\tvalidation loss: 0.3128\t validation accuracy: 0.9400\n",
      "iteration number: 503\t training loss: 0.3473\tvalidation loss: 0.3086\t validation accuracy: 0.9400\n",
      "iteration number: 504\t training loss: 0.3401\tvalidation loss: 0.3028\t validation accuracy: 0.9400\n",
      "iteration number: 505\t training loss: 0.3383\tvalidation loss: 0.3015\t validation accuracy: 0.9444\n",
      "iteration number: 506\t training loss: 0.3386\tvalidation loss: 0.3025\t validation accuracy: 0.9444\n",
      "iteration number: 507\t training loss: 0.3357\tvalidation loss: 0.3000\t validation accuracy: 0.9378\n",
      "iteration number: 508\t training loss: 0.3358\tvalidation loss: 0.3009\t validation accuracy: 0.9422\n",
      "iteration number: 509\t training loss: 0.3341\tvalidation loss: 0.2986\t validation accuracy: 0.9400\n",
      "iteration number: 510\t training loss: 0.3339\tvalidation loss: 0.2968\t validation accuracy: 0.9422\n",
      "iteration number: 511\t training loss: 0.3351\tvalidation loss: 0.2991\t validation accuracy: 0.9356\n",
      "iteration number: 512\t training loss: 0.3302\tvalidation loss: 0.2961\t validation accuracy: 0.9356\n",
      "iteration number: 513\t training loss: 0.3293\tvalidation loss: 0.2949\t validation accuracy: 0.9400\n",
      "iteration number: 514\t training loss: 0.3309\tvalidation loss: 0.2961\t validation accuracy: 0.9333\n",
      "iteration number: 515\t training loss: 0.3339\tvalidation loss: 0.2991\t validation accuracy: 0.9378\n",
      "iteration number: 516\t training loss: 0.3330\tvalidation loss: 0.3001\t validation accuracy: 0.9356\n",
      "iteration number: 517\t training loss: 0.3288\tvalidation loss: 0.2965\t validation accuracy: 0.9356\n",
      "iteration number: 518\t training loss: 0.3270\tvalidation loss: 0.2939\t validation accuracy: 0.9400\n",
      "iteration number: 519\t training loss: 0.3270\tvalidation loss: 0.2927\t validation accuracy: 0.9422\n",
      "iteration number: 520\t training loss: 0.3254\tvalidation loss: 0.2920\t validation accuracy: 0.9378\n",
      "iteration number: 521\t training loss: 0.3252\tvalidation loss: 0.2920\t validation accuracy: 0.9422\n",
      "iteration number: 522\t training loss: 0.3217\tvalidation loss: 0.2883\t validation accuracy: 0.9422\n",
      "iteration number: 523\t training loss: 0.3217\tvalidation loss: 0.2872\t validation accuracy: 0.9422\n",
      "iteration number: 524\t training loss: 0.3224\tvalidation loss: 0.2885\t validation accuracy: 0.9400\n",
      "iteration number: 525\t training loss: 0.3189\tvalidation loss: 0.2850\t validation accuracy: 0.9422\n",
      "iteration number: 526\t training loss: 0.3200\tvalidation loss: 0.2857\t validation accuracy: 0.9444\n",
      "iteration number: 527\t training loss: 0.3181\tvalidation loss: 0.2848\t validation accuracy: 0.9400\n",
      "iteration number: 528\t training loss: 0.3182\tvalidation loss: 0.2849\t validation accuracy: 0.9444\n",
      "iteration number: 529\t training loss: 0.3184\tvalidation loss: 0.2856\t validation accuracy: 0.9400\n",
      "iteration number: 530\t training loss: 0.3250\tvalidation loss: 0.2911\t validation accuracy: 0.9333\n",
      "iteration number: 531\t training loss: 0.3158\tvalidation loss: 0.2818\t validation accuracy: 0.9378\n",
      "iteration number: 532\t training loss: 0.3139\tvalidation loss: 0.2806\t validation accuracy: 0.9378\n",
      "iteration number: 533\t training loss: 0.3133\tvalidation loss: 0.2813\t validation accuracy: 0.9378\n",
      "iteration number: 534\t training loss: 0.3140\tvalidation loss: 0.2819\t validation accuracy: 0.9378\n",
      "iteration number: 535\t training loss: 0.3126\tvalidation loss: 0.2808\t validation accuracy: 0.9378\n",
      "iteration number: 536\t training loss: 0.3122\tvalidation loss: 0.2799\t validation accuracy: 0.9356\n",
      "iteration number: 537\t training loss: 0.3144\tvalidation loss: 0.2825\t validation accuracy: 0.9311\n",
      "iteration number: 538\t training loss: 0.3096\tvalidation loss: 0.2783\t validation accuracy: 0.9400\n",
      "iteration number: 539\t training loss: 0.3127\tvalidation loss: 0.2803\t validation accuracy: 0.9378\n",
      "iteration number: 540\t training loss: 0.3159\tvalidation loss: 0.2829\t validation accuracy: 0.9378\n",
      "iteration number: 541\t training loss: 0.3098\tvalidation loss: 0.2761\t validation accuracy: 0.9378\n",
      "iteration number: 542\t training loss: 0.3090\tvalidation loss: 0.2767\t validation accuracy: 0.9400\n",
      "iteration number: 543\t training loss: 0.3066\tvalidation loss: 0.2739\t validation accuracy: 0.9422\n",
      "iteration number: 544\t training loss: 0.3047\tvalidation loss: 0.2732\t validation accuracy: 0.9378\n",
      "iteration number: 545\t training loss: 0.3055\tvalidation loss: 0.2747\t validation accuracy: 0.9400\n",
      "iteration number: 546\t training loss: 0.3022\tvalidation loss: 0.2726\t validation accuracy: 0.9400\n",
      "iteration number: 547\t training loss: 0.3034\tvalidation loss: 0.2730\t validation accuracy: 0.9422\n",
      "iteration number: 548\t training loss: 0.3026\tvalidation loss: 0.2734\t validation accuracy: 0.9400\n",
      "iteration number: 549\t training loss: 0.3035\tvalidation loss: 0.2755\t validation accuracy: 0.9378\n",
      "iteration number: 550\t training loss: 0.3021\tvalidation loss: 0.2735\t validation accuracy: 0.9400\n",
      "iteration number: 551\t training loss: 0.3029\tvalidation loss: 0.2742\t validation accuracy: 0.9378\n",
      "iteration number: 552\t training loss: 0.3026\tvalidation loss: 0.2741\t validation accuracy: 0.9378\n",
      "iteration number: 553\t training loss: 0.2996\tvalidation loss: 0.2724\t validation accuracy: 0.9400\n",
      "iteration number: 554\t training loss: 0.2965\tvalidation loss: 0.2677\t validation accuracy: 0.9400\n",
      "iteration number: 555\t training loss: 0.2965\tvalidation loss: 0.2686\t validation accuracy: 0.9400\n",
      "iteration number: 556\t training loss: 0.2949\tvalidation loss: 0.2663\t validation accuracy: 0.9422\n",
      "iteration number: 557\t training loss: 0.2964\tvalidation loss: 0.2680\t validation accuracy: 0.9378\n",
      "iteration number: 558\t training loss: 0.2973\tvalidation loss: 0.2695\t validation accuracy: 0.9400\n",
      "iteration number: 559\t training loss: 0.2939\tvalidation loss: 0.2672\t validation accuracy: 0.9422\n",
      "iteration number: 560\t training loss: 0.2925\tvalidation loss: 0.2652\t validation accuracy: 0.9422\n",
      "iteration number: 561\t training loss: 0.2938\tvalidation loss: 0.2659\t validation accuracy: 0.9400\n",
      "iteration number: 562\t training loss: 0.2910\tvalidation loss: 0.2624\t validation accuracy: 0.9422\n",
      "iteration number: 563\t training loss: 0.2906\tvalidation loss: 0.2629\t validation accuracy: 0.9400\n",
      "iteration number: 564\t training loss: 0.2916\tvalidation loss: 0.2628\t validation accuracy: 0.9422\n",
      "iteration number: 565\t training loss: 0.2909\tvalidation loss: 0.2632\t validation accuracy: 0.9444\n",
      "iteration number: 566\t training loss: 0.2913\tvalidation loss: 0.2617\t validation accuracy: 0.9400\n",
      "iteration number: 567\t training loss: 0.2886\tvalidation loss: 0.2598\t validation accuracy: 0.9378\n",
      "iteration number: 568\t training loss: 0.2876\tvalidation loss: 0.2576\t validation accuracy: 0.9400\n",
      "iteration number: 569\t training loss: 0.2877\tvalidation loss: 0.2575\t validation accuracy: 0.9422\n",
      "iteration number: 570\t training loss: 0.2900\tvalidation loss: 0.2588\t validation accuracy: 0.9467\n",
      "iteration number: 571\t training loss: 0.2892\tvalidation loss: 0.2583\t validation accuracy: 0.9444\n",
      "iteration number: 572\t training loss: 0.2889\tvalidation loss: 0.2599\t validation accuracy: 0.9400\n",
      "iteration number: 573\t training loss: 0.2875\tvalidation loss: 0.2586\t validation accuracy: 0.9422\n",
      "iteration number: 574\t training loss: 0.2834\tvalidation loss: 0.2556\t validation accuracy: 0.9400\n",
      "iteration number: 575\t training loss: 0.2857\tvalidation loss: 0.2568\t validation accuracy: 0.9333\n",
      "iteration number: 576\t training loss: 0.2825\tvalidation loss: 0.2553\t validation accuracy: 0.9378\n",
      "iteration number: 577\t training loss: 0.2814\tvalidation loss: 0.2550\t validation accuracy: 0.9444\n",
      "iteration number: 578\t training loss: 0.2820\tvalidation loss: 0.2558\t validation accuracy: 0.9378\n",
      "iteration number: 579\t training loss: 0.2792\tvalidation loss: 0.2534\t validation accuracy: 0.9400\n",
      "iteration number: 580\t training loss: 0.2807\tvalidation loss: 0.2528\t validation accuracy: 0.9333\n",
      "iteration number: 581\t training loss: 0.2805\tvalidation loss: 0.2517\t validation accuracy: 0.9333\n",
      "iteration number: 582\t training loss: 0.2784\tvalidation loss: 0.2501\t validation accuracy: 0.9356\n",
      "iteration number: 583\t training loss: 0.2760\tvalidation loss: 0.2479\t validation accuracy: 0.9400\n",
      "iteration number: 584\t training loss: 0.2775\tvalidation loss: 0.2481\t validation accuracy: 0.9378\n",
      "iteration number: 585\t training loss: 0.2749\tvalidation loss: 0.2450\t validation accuracy: 0.9444\n",
      "iteration number: 586\t training loss: 0.2767\tvalidation loss: 0.2470\t validation accuracy: 0.9422\n",
      "iteration number: 587\t training loss: 0.2744\tvalidation loss: 0.2442\t validation accuracy: 0.9422\n",
      "iteration number: 588\t training loss: 0.2783\tvalidation loss: 0.2467\t validation accuracy: 0.9400\n",
      "iteration number: 589\t training loss: 0.2776\tvalidation loss: 0.2486\t validation accuracy: 0.9422\n",
      "iteration number: 590\t training loss: 0.2736\tvalidation loss: 0.2460\t validation accuracy: 0.9422\n",
      "iteration number: 591\t training loss: 0.2720\tvalidation loss: 0.2453\t validation accuracy: 0.9400\n",
      "iteration number: 592\t training loss: 0.2731\tvalidation loss: 0.2462\t validation accuracy: 0.9356\n",
      "iteration number: 593\t training loss: 0.2753\tvalidation loss: 0.2474\t validation accuracy: 0.9400\n",
      "iteration number: 594\t training loss: 0.2736\tvalidation loss: 0.2459\t validation accuracy: 0.9356\n",
      "iteration number: 595\t training loss: 0.2706\tvalidation loss: 0.2426\t validation accuracy: 0.9378\n",
      "iteration number: 596\t training loss: 0.2696\tvalidation loss: 0.2414\t validation accuracy: 0.9333\n",
      "iteration number: 597\t training loss: 0.2691\tvalidation loss: 0.2402\t validation accuracy: 0.9378\n",
      "iteration number: 598\t training loss: 0.2716\tvalidation loss: 0.2456\t validation accuracy: 0.9400\n",
      "iteration number: 599\t training loss: 0.2659\tvalidation loss: 0.2409\t validation accuracy: 0.9422\n",
      "iteration number: 600\t training loss: 0.2683\tvalidation loss: 0.2416\t validation accuracy: 0.9422\n",
      "iteration number: 601\t training loss: 0.2663\tvalidation loss: 0.2386\t validation accuracy: 0.9400\n",
      "iteration number: 602\t training loss: 0.2689\tvalidation loss: 0.2407\t validation accuracy: 0.9400\n",
      "iteration number: 603\t training loss: 0.2635\tvalidation loss: 0.2378\t validation accuracy: 0.9400\n",
      "iteration number: 604\t training loss: 0.2626\tvalidation loss: 0.2369\t validation accuracy: 0.9378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 605\t training loss: 0.2641\tvalidation loss: 0.2384\t validation accuracy: 0.9378\n",
      "iteration number: 606\t training loss: 0.2637\tvalidation loss: 0.2373\t validation accuracy: 0.9378\n",
      "iteration number: 607\t training loss: 0.2641\tvalidation loss: 0.2377\t validation accuracy: 0.9356\n",
      "iteration number: 608\t training loss: 0.2598\tvalidation loss: 0.2344\t validation accuracy: 0.9378\n",
      "iteration number: 609\t training loss: 0.2612\tvalidation loss: 0.2356\t validation accuracy: 0.9378\n",
      "iteration number: 610\t training loss: 0.2644\tvalidation loss: 0.2364\t validation accuracy: 0.9378\n",
      "iteration number: 611\t training loss: 0.2616\tvalidation loss: 0.2353\t validation accuracy: 0.9356\n",
      "iteration number: 612\t training loss: 0.2607\tvalidation loss: 0.2354\t validation accuracy: 0.9400\n",
      "iteration number: 613\t training loss: 0.2591\tvalidation loss: 0.2339\t validation accuracy: 0.9467\n",
      "iteration number: 614\t training loss: 0.2581\tvalidation loss: 0.2312\t validation accuracy: 0.9422\n",
      "iteration number: 615\t training loss: 0.2590\tvalidation loss: 0.2320\t validation accuracy: 0.9444\n",
      "iteration number: 616\t training loss: 0.2603\tvalidation loss: 0.2325\t validation accuracy: 0.9467\n",
      "iteration number: 617\t training loss: 0.2587\tvalidation loss: 0.2336\t validation accuracy: 0.9400\n",
      "iteration number: 618\t training loss: 0.2577\tvalidation loss: 0.2337\t validation accuracy: 0.9422\n",
      "iteration number: 619\t training loss: 0.2604\tvalidation loss: 0.2367\t validation accuracy: 0.9400\n",
      "iteration number: 620\t training loss: 0.2590\tvalidation loss: 0.2351\t validation accuracy: 0.9467\n",
      "iteration number: 621\t training loss: 0.2577\tvalidation loss: 0.2342\t validation accuracy: 0.9444\n",
      "iteration number: 622\t training loss: 0.2548\tvalidation loss: 0.2313\t validation accuracy: 0.9400\n",
      "iteration number: 623\t training loss: 0.2542\tvalidation loss: 0.2321\t validation accuracy: 0.9378\n",
      "iteration number: 624\t training loss: 0.2538\tvalidation loss: 0.2324\t validation accuracy: 0.9400\n",
      "iteration number: 625\t training loss: 0.2570\tvalidation loss: 0.2346\t validation accuracy: 0.9333\n",
      "iteration number: 626\t training loss: 0.2527\tvalidation loss: 0.2277\t validation accuracy: 0.9489\n",
      "iteration number: 627\t training loss: 0.2530\tvalidation loss: 0.2274\t validation accuracy: 0.9444\n",
      "iteration number: 628\t training loss: 0.2522\tvalidation loss: 0.2265\t validation accuracy: 0.9444\n",
      "iteration number: 629\t training loss: 0.2556\tvalidation loss: 0.2278\t validation accuracy: 0.9422\n",
      "iteration number: 630\t training loss: 0.2541\tvalidation loss: 0.2270\t validation accuracy: 0.9444\n",
      "iteration number: 631\t training loss: 0.2638\tvalidation loss: 0.2343\t validation accuracy: 0.9444\n",
      "iteration number: 632\t training loss: 0.2542\tvalidation loss: 0.2285\t validation accuracy: 0.9444\n",
      "iteration number: 633\t training loss: 0.2616\tvalidation loss: 0.2361\t validation accuracy: 0.9422\n",
      "iteration number: 634\t training loss: 0.2609\tvalidation loss: 0.2340\t validation accuracy: 0.9378\n",
      "iteration number: 635\t training loss: 0.2515\tvalidation loss: 0.2260\t validation accuracy: 0.9400\n",
      "iteration number: 636\t training loss: 0.2480\tvalidation loss: 0.2235\t validation accuracy: 0.9467\n",
      "iteration number: 637\t training loss: 0.2496\tvalidation loss: 0.2249\t validation accuracy: 0.9467\n",
      "iteration number: 638\t training loss: 0.2493\tvalidation loss: 0.2258\t validation accuracy: 0.9489\n",
      "iteration number: 639\t training loss: 0.2448\tvalidation loss: 0.2224\t validation accuracy: 0.9444\n",
      "iteration number: 640\t training loss: 0.2475\tvalidation loss: 0.2237\t validation accuracy: 0.9467\n",
      "iteration number: 641\t training loss: 0.2461\tvalidation loss: 0.2232\t validation accuracy: 0.9444\n",
      "iteration number: 642\t training loss: 0.2456\tvalidation loss: 0.2240\t validation accuracy: 0.9444\n",
      "iteration number: 643\t training loss: 0.2436\tvalidation loss: 0.2227\t validation accuracy: 0.9467\n",
      "iteration number: 644\t training loss: 0.2468\tvalidation loss: 0.2253\t validation accuracy: 0.9422\n",
      "iteration number: 645\t training loss: 0.2427\tvalidation loss: 0.2208\t validation accuracy: 0.9444\n",
      "iteration number: 646\t training loss: 0.2460\tvalidation loss: 0.2244\t validation accuracy: 0.9444\n",
      "iteration number: 647\t training loss: 0.2440\tvalidation loss: 0.2194\t validation accuracy: 0.9444\n",
      "iteration number: 648\t training loss: 0.2434\tvalidation loss: 0.2191\t validation accuracy: 0.9422\n",
      "iteration number: 649\t training loss: 0.2470\tvalidation loss: 0.2243\t validation accuracy: 0.9444\n",
      "iteration number: 650\t training loss: 0.2460\tvalidation loss: 0.2247\t validation accuracy: 0.9422\n",
      "iteration number: 651\t training loss: 0.2420\tvalidation loss: 0.2217\t validation accuracy: 0.9400\n",
      "iteration number: 652\t training loss: 0.2393\tvalidation loss: 0.2183\t validation accuracy: 0.9444\n",
      "iteration number: 653\t training loss: 0.2377\tvalidation loss: 0.2169\t validation accuracy: 0.9467\n",
      "iteration number: 654\t training loss: 0.2406\tvalidation loss: 0.2191\t validation accuracy: 0.9467\n",
      "iteration number: 655\t training loss: 0.2416\tvalidation loss: 0.2209\t validation accuracy: 0.9400\n",
      "iteration number: 656\t training loss: 0.2387\tvalidation loss: 0.2195\t validation accuracy: 0.9422\n",
      "iteration number: 657\t training loss: 0.2389\tvalidation loss: 0.2192\t validation accuracy: 0.9444\n",
      "iteration number: 658\t training loss: 0.2397\tvalidation loss: 0.2205\t validation accuracy: 0.9444\n",
      "iteration number: 659\t training loss: 0.2394\tvalidation loss: 0.2202\t validation accuracy: 0.9444\n",
      "iteration number: 660\t training loss: 0.2388\tvalidation loss: 0.2209\t validation accuracy: 0.9422\n",
      "iteration number: 661\t training loss: 0.2369\tvalidation loss: 0.2192\t validation accuracy: 0.9444\n",
      "iteration number: 662\t training loss: 0.2367\tvalidation loss: 0.2174\t validation accuracy: 0.9444\n",
      "iteration number: 663\t training loss: 0.2347\tvalidation loss: 0.2154\t validation accuracy: 0.9444\n",
      "iteration number: 664\t training loss: 0.2347\tvalidation loss: 0.2159\t validation accuracy: 0.9400\n",
      "iteration number: 665\t training loss: 0.2360\tvalidation loss: 0.2161\t validation accuracy: 0.9378\n",
      "iteration number: 666\t training loss: 0.2334\tvalidation loss: 0.2135\t validation accuracy: 0.9422\n",
      "iteration number: 667\t training loss: 0.2339\tvalidation loss: 0.2143\t validation accuracy: 0.9422\n",
      "iteration number: 668\t training loss: 0.2340\tvalidation loss: 0.2142\t validation accuracy: 0.9467\n",
      "iteration number: 669\t training loss: 0.2357\tvalidation loss: 0.2151\t validation accuracy: 0.9467\n",
      "iteration number: 670\t training loss: 0.2331\tvalidation loss: 0.2146\t validation accuracy: 0.9444\n",
      "iteration number: 671\t training loss: 0.2341\tvalidation loss: 0.2152\t validation accuracy: 0.9400\n",
      "iteration number: 672\t training loss: 0.2301\tvalidation loss: 0.2133\t validation accuracy: 0.9378\n",
      "iteration number: 673\t training loss: 0.2303\tvalidation loss: 0.2123\t validation accuracy: 0.9422\n",
      "iteration number: 674\t training loss: 0.2311\tvalidation loss: 0.2140\t validation accuracy: 0.9422\n",
      "iteration number: 675\t training loss: 0.2303\tvalidation loss: 0.2140\t validation accuracy: 0.9467\n",
      "iteration number: 676\t training loss: 0.2302\tvalidation loss: 0.2144\t validation accuracy: 0.9444\n",
      "iteration number: 677\t training loss: 0.2289\tvalidation loss: 0.2130\t validation accuracy: 0.9422\n",
      "iteration number: 678\t training loss: 0.2299\tvalidation loss: 0.2139\t validation accuracy: 0.9400\n",
      "iteration number: 679\t training loss: 0.2318\tvalidation loss: 0.2162\t validation accuracy: 0.9356\n",
      "iteration number: 680\t training loss: 0.2283\tvalidation loss: 0.2104\t validation accuracy: 0.9444\n",
      "iteration number: 681\t training loss: 0.2270\tvalidation loss: 0.2101\t validation accuracy: 0.9467\n",
      "iteration number: 682\t training loss: 0.2276\tvalidation loss: 0.2124\t validation accuracy: 0.9378\n",
      "iteration number: 683\t training loss: 0.2264\tvalidation loss: 0.2100\t validation accuracy: 0.9467\n",
      "iteration number: 684\t training loss: 0.2286\tvalidation loss: 0.2121\t validation accuracy: 0.9467\n",
      "iteration number: 685\t training loss: 0.2250\tvalidation loss: 0.2079\t validation accuracy: 0.9467\n",
      "iteration number: 686\t training loss: 0.2264\tvalidation loss: 0.2084\t validation accuracy: 0.9444\n",
      "iteration number: 687\t training loss: 0.2266\tvalidation loss: 0.2077\t validation accuracy: 0.9467\n",
      "iteration number: 688\t training loss: 0.2264\tvalidation loss: 0.2065\t validation accuracy: 0.9467\n",
      "iteration number: 689\t training loss: 0.2247\tvalidation loss: 0.2069\t validation accuracy: 0.9444\n",
      "iteration number: 690\t training loss: 0.2271\tvalidation loss: 0.2081\t validation accuracy: 0.9467\n",
      "iteration number: 691\t training loss: 0.2256\tvalidation loss: 0.2077\t validation accuracy: 0.9444\n",
      "iteration number: 692\t training loss: 0.2240\tvalidation loss: 0.2066\t validation accuracy: 0.9467\n",
      "iteration number: 693\t training loss: 0.2272\tvalidation loss: 0.2078\t validation accuracy: 0.9489\n",
      "iteration number: 694\t training loss: 0.2254\tvalidation loss: 0.2065\t validation accuracy: 0.9489\n",
      "iteration number: 695\t training loss: 0.2233\tvalidation loss: 0.2055\t validation accuracy: 0.9467\n",
      "iteration number: 696\t training loss: 0.2235\tvalidation loss: 0.2069\t validation accuracy: 0.9444\n",
      "iteration number: 697\t training loss: 0.2215\tvalidation loss: 0.2062\t validation accuracy: 0.9444\n",
      "iteration number: 698\t training loss: 0.2218\tvalidation loss: 0.2083\t validation accuracy: 0.9422\n",
      "iteration number: 699\t training loss: 0.2214\tvalidation loss: 0.2075\t validation accuracy: 0.9444\n",
      "iteration number: 700\t training loss: 0.2225\tvalidation loss: 0.2080\t validation accuracy: 0.9467\n",
      "iteration number: 701\t training loss: 0.2219\tvalidation loss: 0.2059\t validation accuracy: 0.9511\n",
      "iteration number: 702\t training loss: 0.2205\tvalidation loss: 0.2053\t validation accuracy: 0.9467\n",
      "iteration number: 703\t training loss: 0.2205\tvalidation loss: 0.2059\t validation accuracy: 0.9444\n",
      "iteration number: 704\t training loss: 0.2196\tvalidation loss: 0.2044\t validation accuracy: 0.9444\n",
      "iteration number: 705\t training loss: 0.2205\tvalidation loss: 0.2056\t validation accuracy: 0.9400\n",
      "iteration number: 706\t training loss: 0.2176\tvalidation loss: 0.2016\t validation accuracy: 0.9422\n",
      "iteration number: 707\t training loss: 0.2183\tvalidation loss: 0.2029\t validation accuracy: 0.9422\n",
      "iteration number: 708\t training loss: 0.2170\tvalidation loss: 0.2025\t validation accuracy: 0.9422\n",
      "iteration number: 709\t training loss: 0.2172\tvalidation loss: 0.2016\t validation accuracy: 0.9400\n",
      "iteration number: 710\t training loss: 0.2158\tvalidation loss: 0.2012\t validation accuracy: 0.9422\n",
      "iteration number: 711\t training loss: 0.2153\tvalidation loss: 0.2007\t validation accuracy: 0.9467\n",
      "iteration number: 712\t training loss: 0.2166\tvalidation loss: 0.2006\t validation accuracy: 0.9467\n",
      "iteration number: 713\t training loss: 0.2174\tvalidation loss: 0.2018\t validation accuracy: 0.9489\n",
      "iteration number: 714\t training loss: 0.2196\tvalidation loss: 0.2026\t validation accuracy: 0.9489\n",
      "iteration number: 715\t training loss: 0.2206\tvalidation loss: 0.2028\t validation accuracy: 0.9511\n",
      "iteration number: 716\t training loss: 0.2207\tvalidation loss: 0.2024\t validation accuracy: 0.9511\n",
      "iteration number: 717\t training loss: 0.2173\tvalidation loss: 0.1998\t validation accuracy: 0.9489\n",
      "iteration number: 718\t training loss: 0.2185\tvalidation loss: 0.2023\t validation accuracy: 0.9467\n",
      "iteration number: 719\t training loss: 0.2140\tvalidation loss: 0.1981\t validation accuracy: 0.9467\n",
      "iteration number: 720\t training loss: 0.2144\tvalidation loss: 0.1985\t validation accuracy: 0.9467\n",
      "iteration number: 721\t training loss: 0.2152\tvalidation loss: 0.2007\t validation accuracy: 0.9422\n",
      "iteration number: 722\t training loss: 0.2138\tvalidation loss: 0.1988\t validation accuracy: 0.9444\n",
      "iteration number: 723\t training loss: 0.2190\tvalidation loss: 0.2013\t validation accuracy: 0.9511\n",
      "iteration number: 724\t training loss: 0.2114\tvalidation loss: 0.1968\t validation accuracy: 0.9444\n",
      "iteration number: 725\t training loss: 0.2152\tvalidation loss: 0.2004\t validation accuracy: 0.9489\n",
      "iteration number: 726\t training loss: 0.2128\tvalidation loss: 0.1980\t validation accuracy: 0.9467\n",
      "iteration number: 727\t training loss: 0.2122\tvalidation loss: 0.1970\t validation accuracy: 0.9489\n",
      "iteration number: 728\t training loss: 0.2130\tvalidation loss: 0.1968\t validation accuracy: 0.9489\n",
      "iteration number: 729\t training loss: 0.2119\tvalidation loss: 0.1956\t validation accuracy: 0.9511\n",
      "iteration number: 730\t training loss: 0.2096\tvalidation loss: 0.1953\t validation accuracy: 0.9489\n",
      "iteration number: 731\t training loss: 0.2101\tvalidation loss: 0.1953\t validation accuracy: 0.9444\n",
      "iteration number: 732\t training loss: 0.2127\tvalidation loss: 0.1974\t validation accuracy: 0.9489\n",
      "iteration number: 733\t training loss: 0.2112\tvalidation loss: 0.1946\t validation accuracy: 0.9511\n",
      "iteration number: 734\t training loss: 0.2101\tvalidation loss: 0.1954\t validation accuracy: 0.9467\n",
      "iteration number: 735\t training loss: 0.2117\tvalidation loss: 0.1961\t validation accuracy: 0.9467\n",
      "iteration number: 736\t training loss: 0.2143\tvalidation loss: 0.1966\t validation accuracy: 0.9467\n",
      "iteration number: 737\t training loss: 0.2168\tvalidation loss: 0.1989\t validation accuracy: 0.9511\n",
      "iteration number: 738\t training loss: 0.2148\tvalidation loss: 0.1968\t validation accuracy: 0.9511\n",
      "iteration number: 739\t training loss: 0.2116\tvalidation loss: 0.1952\t validation accuracy: 0.9489\n",
      "iteration number: 740\t training loss: 0.2093\tvalidation loss: 0.1936\t validation accuracy: 0.9489\n",
      "iteration number: 741\t training loss: 0.2108\tvalidation loss: 0.1949\t validation accuracy: 0.9489\n",
      "iteration number: 742\t training loss: 0.2106\tvalidation loss: 0.1948\t validation accuracy: 0.9467\n",
      "iteration number: 743\t training loss: 0.2064\tvalidation loss: 0.1922\t validation accuracy: 0.9467\n",
      "iteration number: 744\t training loss: 0.2063\tvalidation loss: 0.1929\t validation accuracy: 0.9467\n",
      "iteration number: 745\t training loss: 0.2073\tvalidation loss: 0.1932\t validation accuracy: 0.9400\n",
      "iteration number: 746\t training loss: 0.2098\tvalidation loss: 0.1931\t validation accuracy: 0.9467\n",
      "iteration number: 747\t training loss: 0.2080\tvalidation loss: 0.1920\t validation accuracy: 0.9489\n",
      "iteration number: 748\t training loss: 0.2078\tvalidation loss: 0.1926\t validation accuracy: 0.9467\n",
      "iteration number: 749\t training loss: 0.2068\tvalidation loss: 0.1929\t validation accuracy: 0.9400\n",
      "iteration number: 750\t training loss: 0.2058\tvalidation loss: 0.1923\t validation accuracy: 0.9422\n",
      "iteration number: 751\t training loss: 0.2034\tvalidation loss: 0.1902\t validation accuracy: 0.9444\n",
      "iteration number: 752\t training loss: 0.2020\tvalidation loss: 0.1893\t validation accuracy: 0.9489\n",
      "iteration number: 753\t training loss: 0.2019\tvalidation loss: 0.1905\t validation accuracy: 0.9444\n",
      "iteration number: 754\t training loss: 0.2040\tvalidation loss: 0.1908\t validation accuracy: 0.9444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 755\t training loss: 0.2046\tvalidation loss: 0.1916\t validation accuracy: 0.9467\n",
      "iteration number: 756\t training loss: 0.2011\tvalidation loss: 0.1891\t validation accuracy: 0.9467\n",
      "iteration number: 757\t training loss: 0.2006\tvalidation loss: 0.1899\t validation accuracy: 0.9444\n",
      "iteration number: 758\t training loss: 0.2049\tvalidation loss: 0.1953\t validation accuracy: 0.9444\n",
      "iteration number: 759\t training loss: 0.2048\tvalidation loss: 0.1953\t validation accuracy: 0.9400\n",
      "iteration number: 760\t training loss: 0.2180\tvalidation loss: 0.2049\t validation accuracy: 0.9356\n",
      "iteration number: 761\t training loss: 0.2033\tvalidation loss: 0.1928\t validation accuracy: 0.9467\n",
      "iteration number: 762\t training loss: 0.2009\tvalidation loss: 0.1901\t validation accuracy: 0.9422\n",
      "iteration number: 763\t training loss: 0.1992\tvalidation loss: 0.1890\t validation accuracy: 0.9422\n",
      "iteration number: 764\t training loss: 0.1998\tvalidation loss: 0.1900\t validation accuracy: 0.9422\n",
      "iteration number: 765\t training loss: 0.2004\tvalidation loss: 0.1894\t validation accuracy: 0.9444\n",
      "iteration number: 766\t training loss: 0.2006\tvalidation loss: 0.1897\t validation accuracy: 0.9378\n",
      "iteration number: 767\t training loss: 0.2054\tvalidation loss: 0.1942\t validation accuracy: 0.9422\n",
      "iteration number: 768\t training loss: 0.2003\tvalidation loss: 0.1889\t validation accuracy: 0.9400\n",
      "iteration number: 769\t training loss: 0.2006\tvalidation loss: 0.1879\t validation accuracy: 0.9467\n",
      "iteration number: 770\t training loss: 0.2016\tvalidation loss: 0.1867\t validation accuracy: 0.9422\n",
      "iteration number: 771\t training loss: 0.2014\tvalidation loss: 0.1869\t validation accuracy: 0.9444\n",
      "iteration number: 772\t training loss: 0.1974\tvalidation loss: 0.1842\t validation accuracy: 0.9511\n",
      "iteration number: 773\t training loss: 0.1983\tvalidation loss: 0.1876\t validation accuracy: 0.9533\n",
      "iteration number: 774\t training loss: 0.1987\tvalidation loss: 0.1869\t validation accuracy: 0.9467\n",
      "iteration number: 775\t training loss: 0.1975\tvalidation loss: 0.1852\t validation accuracy: 0.9422\n",
      "iteration number: 776\t training loss: 0.1955\tvalidation loss: 0.1827\t validation accuracy: 0.9444\n",
      "iteration number: 777\t training loss: 0.1967\tvalidation loss: 0.1832\t validation accuracy: 0.9444\n",
      "iteration number: 778\t training loss: 0.1952\tvalidation loss: 0.1821\t validation accuracy: 0.9511\n",
      "iteration number: 779\t training loss: 0.1982\tvalidation loss: 0.1838\t validation accuracy: 0.9511\n",
      "iteration number: 780\t training loss: 0.1961\tvalidation loss: 0.1829\t validation accuracy: 0.9511\n",
      "iteration number: 781\t training loss: 0.1995\tvalidation loss: 0.1858\t validation accuracy: 0.9511\n",
      "iteration number: 782\t training loss: 0.2015\tvalidation loss: 0.1889\t validation accuracy: 0.9511\n",
      "iteration number: 783\t training loss: 0.1959\tvalidation loss: 0.1858\t validation accuracy: 0.9444\n",
      "iteration number: 784\t training loss: 0.1958\tvalidation loss: 0.1867\t validation accuracy: 0.9444\n",
      "iteration number: 785\t training loss: 0.1951\tvalidation loss: 0.1851\t validation accuracy: 0.9467\n",
      "iteration number: 786\t training loss: 0.1948\tvalidation loss: 0.1850\t validation accuracy: 0.9489\n",
      "iteration number: 787\t training loss: 0.1945\tvalidation loss: 0.1834\t validation accuracy: 0.9489\n",
      "iteration number: 788\t training loss: 0.1957\tvalidation loss: 0.1827\t validation accuracy: 0.9467\n",
      "iteration number: 789\t training loss: 0.1958\tvalidation loss: 0.1836\t validation accuracy: 0.9422\n",
      "iteration number: 790\t training loss: 0.1911\tvalidation loss: 0.1809\t validation accuracy: 0.9467\n",
      "iteration number: 791\t training loss: 0.1911\tvalidation loss: 0.1796\t validation accuracy: 0.9556\n",
      "iteration number: 792\t training loss: 0.1936\tvalidation loss: 0.1818\t validation accuracy: 0.9422\n",
      "iteration number: 793\t training loss: 0.1933\tvalidation loss: 0.1799\t validation accuracy: 0.9511\n",
      "iteration number: 794\t training loss: 0.1952\tvalidation loss: 0.1825\t validation accuracy: 0.9533\n",
      "iteration number: 795\t training loss: 0.1908\tvalidation loss: 0.1799\t validation accuracy: 0.9511\n",
      "iteration number: 796\t training loss: 0.1940\tvalidation loss: 0.1805\t validation accuracy: 0.9556\n",
      "iteration number: 797\t training loss: 0.1910\tvalidation loss: 0.1786\t validation accuracy: 0.9556\n",
      "iteration number: 798\t training loss: 0.1902\tvalidation loss: 0.1781\t validation accuracy: 0.9511\n",
      "iteration number: 799\t training loss: 0.1888\tvalidation loss: 0.1773\t validation accuracy: 0.9556\n",
      "iteration number: 800\t training loss: 0.1896\tvalidation loss: 0.1778\t validation accuracy: 0.9533\n",
      "iteration number: 801\t training loss: 0.1899\tvalidation loss: 0.1789\t validation accuracy: 0.9556\n",
      "iteration number: 802\t training loss: 0.1908\tvalidation loss: 0.1779\t validation accuracy: 0.9578\n",
      "iteration number: 803\t training loss: 0.1887\tvalidation loss: 0.1767\t validation accuracy: 0.9533\n",
      "iteration number: 804\t training loss: 0.1876\tvalidation loss: 0.1763\t validation accuracy: 0.9511\n",
      "iteration number: 805\t training loss: 0.1888\tvalidation loss: 0.1771\t validation accuracy: 0.9511\n",
      "iteration number: 806\t training loss: 0.1878\tvalidation loss: 0.1764\t validation accuracy: 0.9556\n",
      "iteration number: 807\t training loss: 0.1881\tvalidation loss: 0.1764\t validation accuracy: 0.9533\n",
      "iteration number: 808\t training loss: 0.1867\tvalidation loss: 0.1751\t validation accuracy: 0.9511\n",
      "iteration number: 809\t training loss: 0.1887\tvalidation loss: 0.1766\t validation accuracy: 0.9556\n",
      "iteration number: 810\t training loss: 0.1886\tvalidation loss: 0.1766\t validation accuracy: 0.9489\n",
      "iteration number: 811\t training loss: 0.1866\tvalidation loss: 0.1776\t validation accuracy: 0.9511\n",
      "iteration number: 812\t training loss: 0.1863\tvalidation loss: 0.1776\t validation accuracy: 0.9489\n",
      "iteration number: 813\t training loss: 0.1883\tvalidation loss: 0.1789\t validation accuracy: 0.9511\n",
      "iteration number: 814\t training loss: 0.1885\tvalidation loss: 0.1796\t validation accuracy: 0.9533\n",
      "iteration number: 815\t training loss: 0.1874\tvalidation loss: 0.1781\t validation accuracy: 0.9533\n",
      "iteration number: 816\t training loss: 0.1874\tvalidation loss: 0.1779\t validation accuracy: 0.9556\n",
      "iteration number: 817\t training loss: 0.1858\tvalidation loss: 0.1771\t validation accuracy: 0.9556\n",
      "iteration number: 818\t training loss: 0.1854\tvalidation loss: 0.1769\t validation accuracy: 0.9511\n",
      "iteration number: 819\t training loss: 0.1875\tvalidation loss: 0.1792\t validation accuracy: 0.9489\n",
      "iteration number: 820\t training loss: 0.1873\tvalidation loss: 0.1774\t validation accuracy: 0.9556\n",
      "iteration number: 821\t training loss: 0.1879\tvalidation loss: 0.1774\t validation accuracy: 0.9556\n",
      "iteration number: 822\t training loss: 0.1889\tvalidation loss: 0.1785\t validation accuracy: 0.9556\n",
      "iteration number: 823\t training loss: 0.1888\tvalidation loss: 0.1797\t validation accuracy: 0.9511\n",
      "iteration number: 824\t training loss: 0.1899\tvalidation loss: 0.1827\t validation accuracy: 0.9444\n",
      "iteration number: 825\t training loss: 0.1861\tvalidation loss: 0.1765\t validation accuracy: 0.9578\n",
      "iteration number: 826\t training loss: 0.1875\tvalidation loss: 0.1785\t validation accuracy: 0.9489\n",
      "iteration number: 827\t training loss: 0.1831\tvalidation loss: 0.1743\t validation accuracy: 0.9511\n",
      "iteration number: 828\t training loss: 0.1836\tvalidation loss: 0.1727\t validation accuracy: 0.9556\n",
      "iteration number: 829\t training loss: 0.1863\tvalidation loss: 0.1756\t validation accuracy: 0.9489\n",
      "iteration number: 830\t training loss: 0.1856\tvalidation loss: 0.1746\t validation accuracy: 0.9556\n",
      "iteration number: 831\t training loss: 0.1836\tvalidation loss: 0.1728\t validation accuracy: 0.9556\n",
      "iteration number: 832\t training loss: 0.1829\tvalidation loss: 0.1723\t validation accuracy: 0.9578\n",
      "iteration number: 833\t training loss: 0.1848\tvalidation loss: 0.1742\t validation accuracy: 0.9489\n",
      "iteration number: 834\t training loss: 0.1829\tvalidation loss: 0.1718\t validation accuracy: 0.9556\n",
      "iteration number: 835\t training loss: 0.1811\tvalidation loss: 0.1708\t validation accuracy: 0.9556\n",
      "iteration number: 836\t training loss: 0.1809\tvalidation loss: 0.1724\t validation accuracy: 0.9489\n",
      "iteration number: 837\t training loss: 0.1808\tvalidation loss: 0.1715\t validation accuracy: 0.9489\n",
      "iteration number: 838\t training loss: 0.1832\tvalidation loss: 0.1730\t validation accuracy: 0.9511\n",
      "iteration number: 839\t training loss: 0.1835\tvalidation loss: 0.1751\t validation accuracy: 0.9556\n",
      "iteration number: 840\t training loss: 0.1872\tvalidation loss: 0.1799\t validation accuracy: 0.9511\n",
      "iteration number: 841\t training loss: 0.1829\tvalidation loss: 0.1744\t validation accuracy: 0.9444\n",
      "iteration number: 842\t training loss: 0.1812\tvalidation loss: 0.1723\t validation accuracy: 0.9467\n",
      "iteration number: 843\t training loss: 0.1791\tvalidation loss: 0.1702\t validation accuracy: 0.9556\n",
      "iteration number: 844\t training loss: 0.1828\tvalidation loss: 0.1733\t validation accuracy: 0.9444\n",
      "iteration number: 845\t training loss: 0.1804\tvalidation loss: 0.1705\t validation accuracy: 0.9444\n",
      "iteration number: 846\t training loss: 0.1783\tvalidation loss: 0.1684\t validation accuracy: 0.9533\n",
      "iteration number: 847\t training loss: 0.1816\tvalidation loss: 0.1724\t validation accuracy: 0.9511\n",
      "iteration number: 848\t training loss: 0.1786\tvalidation loss: 0.1703\t validation accuracy: 0.9511\n",
      "iteration number: 849\t training loss: 0.1781\tvalidation loss: 0.1705\t validation accuracy: 0.9511\n",
      "iteration number: 850\t training loss: 0.1776\tvalidation loss: 0.1681\t validation accuracy: 0.9533\n",
      "iteration number: 851\t training loss: 0.1762\tvalidation loss: 0.1672\t validation accuracy: 0.9467\n",
      "iteration number: 852\t training loss: 0.1759\tvalidation loss: 0.1662\t validation accuracy: 0.9556\n",
      "iteration number: 853\t training loss: 0.1767\tvalidation loss: 0.1666\t validation accuracy: 0.9511\n",
      "iteration number: 854\t training loss: 0.1772\tvalidation loss: 0.1666\t validation accuracy: 0.9578\n",
      "iteration number: 855\t training loss: 0.1772\tvalidation loss: 0.1663\t validation accuracy: 0.9578\n",
      "iteration number: 856\t training loss: 0.1777\tvalidation loss: 0.1701\t validation accuracy: 0.9467\n",
      "iteration number: 857\t training loss: 0.1773\tvalidation loss: 0.1676\t validation accuracy: 0.9511\n",
      "iteration number: 858\t training loss: 0.1773\tvalidation loss: 0.1679\t validation accuracy: 0.9556\n",
      "iteration number: 859\t training loss: 0.1786\tvalidation loss: 0.1705\t validation accuracy: 0.9556\n",
      "iteration number: 860\t training loss: 0.1804\tvalidation loss: 0.1716\t validation accuracy: 0.9533\n",
      "iteration number: 861\t training loss: 0.1776\tvalidation loss: 0.1701\t validation accuracy: 0.9533\n",
      "iteration number: 862\t training loss: 0.1803\tvalidation loss: 0.1736\t validation accuracy: 0.9489\n",
      "iteration number: 863\t training loss: 0.1802\tvalidation loss: 0.1731\t validation accuracy: 0.9489\n",
      "iteration number: 864\t training loss: 0.1818\tvalidation loss: 0.1742\t validation accuracy: 0.9511\n",
      "iteration number: 865\t training loss: 0.1777\tvalidation loss: 0.1712\t validation accuracy: 0.9467\n",
      "iteration number: 866\t training loss: 0.1772\tvalidation loss: 0.1714\t validation accuracy: 0.9467\n",
      "iteration number: 867\t training loss: 0.1799\tvalidation loss: 0.1699\t validation accuracy: 0.9533\n",
      "iteration number: 868\t training loss: 0.1754\tvalidation loss: 0.1663\t validation accuracy: 0.9556\n",
      "iteration number: 869\t training loss: 0.1743\tvalidation loss: 0.1658\t validation accuracy: 0.9511\n",
      "iteration number: 870\t training loss: 0.1768\tvalidation loss: 0.1692\t validation accuracy: 0.9533\n",
      "iteration number: 871\t training loss: 0.1840\tvalidation loss: 0.1717\t validation accuracy: 0.9467\n",
      "iteration number: 872\t training loss: 0.1775\tvalidation loss: 0.1673\t validation accuracy: 0.9556\n",
      "iteration number: 873\t training loss: 0.1737\tvalidation loss: 0.1660\t validation accuracy: 0.9467\n",
      "iteration number: 874\t training loss: 0.1737\tvalidation loss: 0.1665\t validation accuracy: 0.9489\n",
      "iteration number: 875\t training loss: 0.1729\tvalidation loss: 0.1651\t validation accuracy: 0.9444\n",
      "iteration number: 876\t training loss: 0.1722\tvalidation loss: 0.1636\t validation accuracy: 0.9556\n",
      "iteration number: 877\t training loss: 0.1717\tvalidation loss: 0.1631\t validation accuracy: 0.9533\n",
      "iteration number: 878\t training loss: 0.1728\tvalidation loss: 0.1638\t validation accuracy: 0.9556\n",
      "iteration number: 879\t training loss: 0.1718\tvalidation loss: 0.1631\t validation accuracy: 0.9578\n",
      "iteration number: 880\t training loss: 0.1725\tvalidation loss: 0.1629\t validation accuracy: 0.9578\n",
      "iteration number: 881\t training loss: 0.1709\tvalidation loss: 0.1615\t validation accuracy: 0.9556\n",
      "iteration number: 882\t training loss: 0.1710\tvalidation loss: 0.1614\t validation accuracy: 0.9578\n",
      "iteration number: 883\t training loss: 0.1706\tvalidation loss: 0.1613\t validation accuracy: 0.9600\n",
      "iteration number: 884\t training loss: 0.1712\tvalidation loss: 0.1632\t validation accuracy: 0.9511\n",
      "iteration number: 885\t training loss: 0.1709\tvalidation loss: 0.1644\t validation accuracy: 0.9533\n",
      "iteration number: 886\t training loss: 0.1713\tvalidation loss: 0.1650\t validation accuracy: 0.9489\n",
      "iteration number: 887\t training loss: 0.1721\tvalidation loss: 0.1649\t validation accuracy: 0.9467\n",
      "iteration number: 888\t training loss: 0.1698\tvalidation loss: 0.1612\t validation accuracy: 0.9622\n",
      "iteration number: 889\t training loss: 0.1686\tvalidation loss: 0.1594\t validation accuracy: 0.9578\n",
      "iteration number: 890\t training loss: 0.1691\tvalidation loss: 0.1595\t validation accuracy: 0.9578\n",
      "iteration number: 891\t training loss: 0.1713\tvalidation loss: 0.1591\t validation accuracy: 0.9600\n",
      "iteration number: 892\t training loss: 0.1690\tvalidation loss: 0.1592\t validation accuracy: 0.9511\n",
      "iteration number: 893\t training loss: 0.1687\tvalidation loss: 0.1583\t validation accuracy: 0.9556\n",
      "iteration number: 894\t training loss: 0.1685\tvalidation loss: 0.1581\t validation accuracy: 0.9556\n",
      "iteration number: 895\t training loss: 0.1679\tvalidation loss: 0.1590\t validation accuracy: 0.9556\n",
      "iteration number: 896\t training loss: 0.1677\tvalidation loss: 0.1589\t validation accuracy: 0.9556\n",
      "iteration number: 897\t training loss: 0.1681\tvalidation loss: 0.1596\t validation accuracy: 0.9511\n",
      "iteration number: 898\t training loss: 0.1677\tvalidation loss: 0.1604\t validation accuracy: 0.9511\n",
      "iteration number: 899\t training loss: 0.1672\tvalidation loss: 0.1581\t validation accuracy: 0.9556\n",
      "iteration number: 900\t training loss: 0.1670\tvalidation loss: 0.1591\t validation accuracy: 0.9511\n",
      "iteration number: 901\t training loss: 0.1668\tvalidation loss: 0.1581\t validation accuracy: 0.9578\n",
      "iteration number: 902\t training loss: 0.1695\tvalidation loss: 0.1595\t validation accuracy: 0.9533\n",
      "iteration number: 903\t training loss: 0.1708\tvalidation loss: 0.1619\t validation accuracy: 0.9511\n",
      "iteration number: 904\t training loss: 0.1659\tvalidation loss: 0.1581\t validation accuracy: 0.9556\n",
      "iteration number: 905\t training loss: 0.1658\tvalidation loss: 0.1577\t validation accuracy: 0.9578\n",
      "iteration number: 906\t training loss: 0.1659\tvalidation loss: 0.1587\t validation accuracy: 0.9533\n",
      "iteration number: 907\t training loss: 0.1649\tvalidation loss: 0.1574\t validation accuracy: 0.9533\n",
      "iteration number: 908\t training loss: 0.1673\tvalidation loss: 0.1591\t validation accuracy: 0.9533\n",
      "iteration number: 909\t training loss: 0.1694\tvalidation loss: 0.1622\t validation accuracy: 0.9511\n",
      "iteration number: 910\t training loss: 0.1690\tvalidation loss: 0.1618\t validation accuracy: 0.9533\n",
      "iteration number: 911\t training loss: 0.1727\tvalidation loss: 0.1641\t validation accuracy: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 912\t training loss: 0.1710\tvalidation loss: 0.1642\t validation accuracy: 0.9600\n",
      "iteration number: 913\t training loss: 0.1666\tvalidation loss: 0.1622\t validation accuracy: 0.9511\n",
      "iteration number: 914\t training loss: 0.1669\tvalidation loss: 0.1625\t validation accuracy: 0.9511\n",
      "iteration number: 915\t training loss: 0.1657\tvalidation loss: 0.1605\t validation accuracy: 0.9578\n",
      "iteration number: 916\t training loss: 0.1644\tvalidation loss: 0.1598\t validation accuracy: 0.9533\n",
      "iteration number: 917\t training loss: 0.1674\tvalidation loss: 0.1596\t validation accuracy: 0.9556\n",
      "iteration number: 918\t training loss: 0.1667\tvalidation loss: 0.1595\t validation accuracy: 0.9533\n",
      "iteration number: 919\t training loss: 0.1680\tvalidation loss: 0.1606\t validation accuracy: 0.9556\n",
      "iteration number: 920\t training loss: 0.1658\tvalidation loss: 0.1575\t validation accuracy: 0.9533\n",
      "iteration number: 921\t training loss: 0.1653\tvalidation loss: 0.1574\t validation accuracy: 0.9556\n",
      "iteration number: 922\t training loss: 0.1648\tvalidation loss: 0.1555\t validation accuracy: 0.9578\n",
      "iteration number: 923\t training loss: 0.1647\tvalidation loss: 0.1562\t validation accuracy: 0.9533\n",
      "iteration number: 924\t training loss: 0.1647\tvalidation loss: 0.1582\t validation accuracy: 0.9511\n",
      "iteration number: 925\t training loss: 0.1629\tvalidation loss: 0.1551\t validation accuracy: 0.9533\n",
      "iteration number: 926\t training loss: 0.1629\tvalidation loss: 0.1559\t validation accuracy: 0.9556\n",
      "iteration number: 927\t training loss: 0.1654\tvalidation loss: 0.1590\t validation accuracy: 0.9533\n",
      "iteration number: 928\t training loss: 0.1625\tvalidation loss: 0.1569\t validation accuracy: 0.9578\n",
      "iteration number: 929\t training loss: 0.1636\tvalidation loss: 0.1569\t validation accuracy: 0.9556\n",
      "iteration number: 930\t training loss: 0.1621\tvalidation loss: 0.1569\t validation accuracy: 0.9578\n",
      "iteration number: 931\t training loss: 0.1644\tvalidation loss: 0.1569\t validation accuracy: 0.9533\n",
      "iteration number: 932\t training loss: 0.1670\tvalidation loss: 0.1608\t validation accuracy: 0.9467\n",
      "iteration number: 933\t training loss: 0.1652\tvalidation loss: 0.1605\t validation accuracy: 0.9533\n",
      "iteration number: 934\t training loss: 0.1628\tvalidation loss: 0.1566\t validation accuracy: 0.9600\n",
      "iteration number: 935\t training loss: 0.1626\tvalidation loss: 0.1559\t validation accuracy: 0.9556\n",
      "iteration number: 936\t training loss: 0.1652\tvalidation loss: 0.1566\t validation accuracy: 0.9556\n",
      "iteration number: 937\t training loss: 0.1652\tvalidation loss: 0.1568\t validation accuracy: 0.9600\n",
      "iteration number: 938\t training loss: 0.1677\tvalidation loss: 0.1587\t validation accuracy: 0.9533\n",
      "iteration number: 939\t training loss: 0.1654\tvalidation loss: 0.1578\t validation accuracy: 0.9556\n",
      "iteration number: 940\t training loss: 0.1607\tvalidation loss: 0.1537\t validation accuracy: 0.9600\n",
      "iteration number: 941\t training loss: 0.1590\tvalidation loss: 0.1538\t validation accuracy: 0.9644\n",
      "iteration number: 942\t training loss: 0.1597\tvalidation loss: 0.1527\t validation accuracy: 0.9667\n",
      "iteration number: 943\t training loss: 0.1599\tvalidation loss: 0.1534\t validation accuracy: 0.9600\n",
      "iteration number: 944\t training loss: 0.1595\tvalidation loss: 0.1527\t validation accuracy: 0.9622\n",
      "iteration number: 945\t training loss: 0.1587\tvalidation loss: 0.1533\t validation accuracy: 0.9667\n",
      "iteration number: 946\t training loss: 0.1600\tvalidation loss: 0.1538\t validation accuracy: 0.9667\n",
      "iteration number: 947\t training loss: 0.1599\tvalidation loss: 0.1522\t validation accuracy: 0.9667\n",
      "iteration number: 948\t training loss: 0.1588\tvalidation loss: 0.1510\t validation accuracy: 0.9667\n",
      "iteration number: 949\t training loss: 0.1600\tvalidation loss: 0.1512\t validation accuracy: 0.9644\n",
      "iteration number: 950\t training loss: 0.1608\tvalidation loss: 0.1519\t validation accuracy: 0.9644\n",
      "iteration number: 951\t training loss: 0.1627\tvalidation loss: 0.1537\t validation accuracy: 0.9556\n",
      "iteration number: 952\t training loss: 0.1605\tvalidation loss: 0.1525\t validation accuracy: 0.9644\n",
      "iteration number: 953\t training loss: 0.1586\tvalidation loss: 0.1510\t validation accuracy: 0.9556\n",
      "iteration number: 954\t training loss: 0.1576\tvalidation loss: 0.1503\t validation accuracy: 0.9578\n",
      "iteration number: 955\t training loss: 0.1599\tvalidation loss: 0.1503\t validation accuracy: 0.9600\n",
      "iteration number: 956\t training loss: 0.1607\tvalidation loss: 0.1508\t validation accuracy: 0.9600\n",
      "iteration number: 957\t training loss: 0.1573\tvalidation loss: 0.1498\t validation accuracy: 0.9578\n",
      "iteration number: 958\t training loss: 0.1570\tvalidation loss: 0.1523\t validation accuracy: 0.9600\n",
      "iteration number: 959\t training loss: 0.1573\tvalidation loss: 0.1521\t validation accuracy: 0.9600\n",
      "iteration number: 960\t training loss: 0.1625\tvalidation loss: 0.1562\t validation accuracy: 0.9533\n",
      "iteration number: 961\t training loss: 0.1552\tvalidation loss: 0.1497\t validation accuracy: 0.9667\n",
      "iteration number: 962\t training loss: 0.1556\tvalidation loss: 0.1493\t validation accuracy: 0.9622\n",
      "iteration number: 963\t training loss: 0.1546\tvalidation loss: 0.1494\t validation accuracy: 0.9622\n",
      "iteration number: 964\t training loss: 0.1568\tvalidation loss: 0.1526\t validation accuracy: 0.9556\n",
      "iteration number: 965\t training loss: 0.1557\tvalidation loss: 0.1508\t validation accuracy: 0.9578\n",
      "iteration number: 966\t training loss: 0.1570\tvalidation loss: 0.1521\t validation accuracy: 0.9600\n",
      "iteration number: 967\t training loss: 0.1593\tvalidation loss: 0.1535\t validation accuracy: 0.9533\n",
      "iteration number: 968\t training loss: 0.1568\tvalidation loss: 0.1514\t validation accuracy: 0.9533\n",
      "iteration number: 969\t training loss: 0.1549\tvalidation loss: 0.1490\t validation accuracy: 0.9578\n",
      "iteration number: 970\t training loss: 0.1550\tvalidation loss: 0.1506\t validation accuracy: 0.9578\n",
      "iteration number: 971\t training loss: 0.1539\tvalidation loss: 0.1494\t validation accuracy: 0.9578\n",
      "iteration number: 972\t training loss: 0.1549\tvalidation loss: 0.1507\t validation accuracy: 0.9556\n",
      "iteration number: 973\t training loss: 0.1542\tvalidation loss: 0.1513\t validation accuracy: 0.9556\n",
      "iteration number: 974\t training loss: 0.1543\tvalidation loss: 0.1503\t validation accuracy: 0.9600\n",
      "iteration number: 975\t training loss: 0.1565\tvalidation loss: 0.1533\t validation accuracy: 0.9533\n",
      "iteration number: 976\t training loss: 0.1544\tvalidation loss: 0.1501\t validation accuracy: 0.9533\n",
      "iteration number: 977\t training loss: 0.1544\tvalidation loss: 0.1484\t validation accuracy: 0.9600\n",
      "iteration number: 978\t training loss: 0.1570\tvalidation loss: 0.1496\t validation accuracy: 0.9600\n",
      "iteration number: 979\t training loss: 0.1541\tvalidation loss: 0.1482\t validation accuracy: 0.9600\n",
      "iteration number: 980\t training loss: 0.1522\tvalidation loss: 0.1460\t validation accuracy: 0.9622\n",
      "iteration number: 981\t training loss: 0.1531\tvalidation loss: 0.1476\t validation accuracy: 0.9578\n",
      "iteration number: 982\t training loss: 0.1529\tvalidation loss: 0.1466\t validation accuracy: 0.9556\n",
      "iteration number: 983\t training loss: 0.1527\tvalidation loss: 0.1459\t validation accuracy: 0.9644\n",
      "iteration number: 984\t training loss: 0.1519\tvalidation loss: 0.1468\t validation accuracy: 0.9578\n",
      "iteration number: 985\t training loss: 0.1522\tvalidation loss: 0.1480\t validation accuracy: 0.9556\n",
      "iteration number: 986\t training loss: 0.1514\tvalidation loss: 0.1461\t validation accuracy: 0.9622\n",
      "iteration number: 987\t training loss: 0.1534\tvalidation loss: 0.1485\t validation accuracy: 0.9622\n",
      "iteration number: 988\t training loss: 0.1524\tvalidation loss: 0.1460\t validation accuracy: 0.9622\n",
      "iteration number: 989\t training loss: 0.1520\tvalidation loss: 0.1459\t validation accuracy: 0.9600\n",
      "iteration number: 990\t training loss: 0.1528\tvalidation loss: 0.1442\t validation accuracy: 0.9711\n",
      "iteration number: 991\t training loss: 0.1531\tvalidation loss: 0.1449\t validation accuracy: 0.9689\n",
      "iteration number: 992\t training loss: 0.1544\tvalidation loss: 0.1473\t validation accuracy: 0.9622\n",
      "iteration number: 993\t training loss: 0.1524\tvalidation loss: 0.1453\t validation accuracy: 0.9667\n",
      "iteration number: 994\t training loss: 0.1532\tvalidation loss: 0.1452\t validation accuracy: 0.9644\n",
      "iteration number: 995\t training loss: 0.1523\tvalidation loss: 0.1456\t validation accuracy: 0.9600\n",
      "iteration number: 996\t training loss: 0.1523\tvalidation loss: 0.1463\t validation accuracy: 0.9600\n",
      "iteration number: 997\t training loss: 0.1515\tvalidation loss: 0.1465\t validation accuracy: 0.9578\n",
      "iteration number: 998\t training loss: 0.1511\tvalidation loss: 0.1467\t validation accuracy: 0.9533\n",
      "iteration number: 999\t training loss: 0.1494\tvalidation loss: 0.1461\t validation accuracy: 0.9600\n",
      "iteration number: 1000\t training loss: 0.1501\tvalidation loss: 0.1471\t validation accuracy: 0.9556\n",
      "iteration number: 1001\t training loss: 0.1491\tvalidation loss: 0.1458\t validation accuracy: 0.9600\n",
      "iteration number: 1002\t training loss: 0.1497\tvalidation loss: 0.1465\t validation accuracy: 0.9622\n",
      "iteration number: 1003\t training loss: 0.1504\tvalidation loss: 0.1461\t validation accuracy: 0.9622\n",
      "iteration number: 1004\t training loss: 0.1495\tvalidation loss: 0.1471\t validation accuracy: 0.9578\n",
      "iteration number: 1005\t training loss: 0.1546\tvalidation loss: 0.1506\t validation accuracy: 0.9511\n",
      "iteration number: 1006\t training loss: 0.1529\tvalidation loss: 0.1489\t validation accuracy: 0.9511\n",
      "iteration number: 1007\t training loss: 0.1523\tvalidation loss: 0.1511\t validation accuracy: 0.9578\n",
      "iteration number: 1008\t training loss: 0.1526\tvalidation loss: 0.1494\t validation accuracy: 0.9556\n",
      "iteration number: 1009\t training loss: 0.1521\tvalidation loss: 0.1500\t validation accuracy: 0.9533\n",
      "iteration number: 1010\t training loss: 0.1493\tvalidation loss: 0.1453\t validation accuracy: 0.9644\n",
      "iteration number: 1011\t training loss: 0.1495\tvalidation loss: 0.1456\t validation accuracy: 0.9667\n",
      "iteration number: 1012\t training loss: 0.1495\tvalidation loss: 0.1452\t validation accuracy: 0.9667\n",
      "iteration number: 1013\t training loss: 0.1499\tvalidation loss: 0.1459\t validation accuracy: 0.9622\n",
      "iteration number: 1014\t training loss: 0.1488\tvalidation loss: 0.1456\t validation accuracy: 0.9644\n",
      "iteration number: 1015\t training loss: 0.1486\tvalidation loss: 0.1455\t validation accuracy: 0.9600\n",
      "iteration number: 1016\t training loss: 0.1491\tvalidation loss: 0.1480\t validation accuracy: 0.9556\n",
      "iteration number: 1017\t training loss: 0.1486\tvalidation loss: 0.1450\t validation accuracy: 0.9600\n",
      "iteration number: 1018\t training loss: 0.1509\tvalidation loss: 0.1474\t validation accuracy: 0.9622\n",
      "iteration number: 1019\t training loss: 0.1498\tvalidation loss: 0.1458\t validation accuracy: 0.9600\n",
      "iteration number: 1020\t training loss: 0.1487\tvalidation loss: 0.1447\t validation accuracy: 0.9622\n",
      "iteration number: 1021\t training loss: 0.1470\tvalidation loss: 0.1434\t validation accuracy: 0.9622\n",
      "iteration number: 1022\t training loss: 0.1484\tvalidation loss: 0.1429\t validation accuracy: 0.9667\n",
      "iteration number: 1023\t training loss: 0.1500\tvalidation loss: 0.1433\t validation accuracy: 0.9644\n",
      "iteration number: 1024\t training loss: 0.1483\tvalidation loss: 0.1421\t validation accuracy: 0.9644\n",
      "iteration number: 1025\t training loss: 0.1477\tvalidation loss: 0.1423\t validation accuracy: 0.9578\n",
      "iteration number: 1026\t training loss: 0.1470\tvalidation loss: 0.1427\t validation accuracy: 0.9600\n",
      "iteration number: 1027\t training loss: 0.1456\tvalidation loss: 0.1421\t validation accuracy: 0.9556\n",
      "iteration number: 1028\t training loss: 0.1462\tvalidation loss: 0.1407\t validation accuracy: 0.9578\n",
      "iteration number: 1029\t training loss: 0.1464\tvalidation loss: 0.1412\t validation accuracy: 0.9578\n",
      "iteration number: 1030\t training loss: 0.1475\tvalidation loss: 0.1422\t validation accuracy: 0.9578\n",
      "iteration number: 1031\t training loss: 0.1467\tvalidation loss: 0.1408\t validation accuracy: 0.9622\n",
      "iteration number: 1032\t training loss: 0.1455\tvalidation loss: 0.1405\t validation accuracy: 0.9622\n",
      "iteration number: 1033\t training loss: 0.1497\tvalidation loss: 0.1431\t validation accuracy: 0.9556\n",
      "iteration number: 1034\t training loss: 0.1471\tvalidation loss: 0.1428\t validation accuracy: 0.9578\n",
      "iteration number: 1035\t training loss: 0.1449\tvalidation loss: 0.1418\t validation accuracy: 0.9644\n",
      "iteration number: 1036\t training loss: 0.1439\tvalidation loss: 0.1408\t validation accuracy: 0.9622\n",
      "iteration number: 1037\t training loss: 0.1444\tvalidation loss: 0.1406\t validation accuracy: 0.9622\n",
      "iteration number: 1038\t training loss: 0.1457\tvalidation loss: 0.1412\t validation accuracy: 0.9578\n",
      "iteration number: 1039\t training loss: 0.1446\tvalidation loss: 0.1417\t validation accuracy: 0.9556\n",
      "iteration number: 1040\t training loss: 0.1446\tvalidation loss: 0.1422\t validation accuracy: 0.9578\n",
      "iteration number: 1041\t training loss: 0.1437\tvalidation loss: 0.1425\t validation accuracy: 0.9578\n",
      "iteration number: 1042\t training loss: 0.1444\tvalidation loss: 0.1420\t validation accuracy: 0.9578\n",
      "iteration number: 1043\t training loss: 0.1434\tvalidation loss: 0.1411\t validation accuracy: 0.9622\n",
      "iteration number: 1044\t training loss: 0.1431\tvalidation loss: 0.1392\t validation accuracy: 0.9600\n",
      "iteration number: 1045\t training loss: 0.1428\tvalidation loss: 0.1399\t validation accuracy: 0.9578\n",
      "iteration number: 1046\t training loss: 0.1436\tvalidation loss: 0.1396\t validation accuracy: 0.9600\n",
      "iteration number: 1047\t training loss: 0.1435\tvalidation loss: 0.1407\t validation accuracy: 0.9533\n",
      "iteration number: 1048\t training loss: 0.1433\tvalidation loss: 0.1414\t validation accuracy: 0.9600\n",
      "iteration number: 1049\t training loss: 0.1448\tvalidation loss: 0.1443\t validation accuracy: 0.9533\n",
      "iteration number: 1050\t training loss: 0.1454\tvalidation loss: 0.1455\t validation accuracy: 0.9533\n",
      "iteration number: 1051\t training loss: 0.1455\tvalidation loss: 0.1457\t validation accuracy: 0.9533\n",
      "iteration number: 1052\t training loss: 0.1439\tvalidation loss: 0.1431\t validation accuracy: 0.9511\n",
      "iteration number: 1053\t training loss: 0.1428\tvalidation loss: 0.1422\t validation accuracy: 0.9578\n",
      "iteration number: 1054\t training loss: 0.1415\tvalidation loss: 0.1403\t validation accuracy: 0.9578\n",
      "iteration number: 1055\t training loss: 0.1419\tvalidation loss: 0.1408\t validation accuracy: 0.9600\n",
      "iteration number: 1056\t training loss: 0.1441\tvalidation loss: 0.1446\t validation accuracy: 0.9533\n",
      "iteration number: 1057\t training loss: 0.1418\tvalidation loss: 0.1424\t validation accuracy: 0.9511\n",
      "iteration number: 1058\t training loss: 0.1424\tvalidation loss: 0.1415\t validation accuracy: 0.9533\n",
      "iteration number: 1059\t training loss: 0.1406\tvalidation loss: 0.1391\t validation accuracy: 0.9578\n",
      "iteration number: 1060\t training loss: 0.1404\tvalidation loss: 0.1388\t validation accuracy: 0.9556\n",
      "iteration number: 1061\t training loss: 0.1411\tvalidation loss: 0.1408\t validation accuracy: 0.9556\n",
      "iteration number: 1062\t training loss: 0.1400\tvalidation loss: 0.1394\t validation accuracy: 0.9600\n",
      "iteration number: 1063\t training loss: 0.1408\tvalidation loss: 0.1391\t validation accuracy: 0.9556\n",
      "iteration number: 1064\t training loss: 0.1400\tvalidation loss: 0.1382\t validation accuracy: 0.9578\n",
      "iteration number: 1065\t training loss: 0.1399\tvalidation loss: 0.1369\t validation accuracy: 0.9644\n",
      "iteration number: 1066\t training loss: 0.1399\tvalidation loss: 0.1371\t validation accuracy: 0.9622\n",
      "iteration number: 1067\t training loss: 0.1451\tvalidation loss: 0.1421\t validation accuracy: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1068\t training loss: 0.1453\tvalidation loss: 0.1419\t validation accuracy: 0.9578\n",
      "iteration number: 1069\t training loss: 0.1421\tvalidation loss: 0.1402\t validation accuracy: 0.9622\n",
      "iteration number: 1070\t training loss: 0.1429\tvalidation loss: 0.1398\t validation accuracy: 0.9622\n",
      "iteration number: 1071\t training loss: 0.1399\tvalidation loss: 0.1377\t validation accuracy: 0.9578\n",
      "iteration number: 1072\t training loss: 0.1403\tvalidation loss: 0.1383\t validation accuracy: 0.9578\n",
      "iteration number: 1073\t training loss: 0.1390\tvalidation loss: 0.1360\t validation accuracy: 0.9600\n",
      "iteration number: 1074\t training loss: 0.1395\tvalidation loss: 0.1353\t validation accuracy: 0.9600\n",
      "iteration number: 1075\t training loss: 0.1399\tvalidation loss: 0.1363\t validation accuracy: 0.9578\n",
      "iteration number: 1076\t training loss: 0.1402\tvalidation loss: 0.1391\t validation accuracy: 0.9600\n",
      "iteration number: 1077\t training loss: 0.1415\tvalidation loss: 0.1377\t validation accuracy: 0.9556\n",
      "iteration number: 1078\t training loss: 0.1401\tvalidation loss: 0.1368\t validation accuracy: 0.9622\n",
      "iteration number: 1079\t training loss: 0.1412\tvalidation loss: 0.1363\t validation accuracy: 0.9622\n",
      "iteration number: 1080\t training loss: 0.1442\tvalidation loss: 0.1373\t validation accuracy: 0.9644\n",
      "iteration number: 1081\t training loss: 0.1434\tvalidation loss: 0.1372\t validation accuracy: 0.9667\n",
      "iteration number: 1082\t training loss: 0.1466\tvalidation loss: 0.1395\t validation accuracy: 0.9533\n",
      "iteration number: 1083\t training loss: 0.1473\tvalidation loss: 0.1395\t validation accuracy: 0.9533\n",
      "iteration number: 1084\t training loss: 0.1415\tvalidation loss: 0.1359\t validation accuracy: 0.9600\n",
      "iteration number: 1085\t training loss: 0.1458\tvalidation loss: 0.1390\t validation accuracy: 0.9556\n",
      "iteration number: 1086\t training loss: 0.1391\tvalidation loss: 0.1350\t validation accuracy: 0.9644\n",
      "iteration number: 1087\t training loss: 0.1382\tvalidation loss: 0.1356\t validation accuracy: 0.9622\n",
      "iteration number: 1088\t training loss: 0.1396\tvalidation loss: 0.1388\t validation accuracy: 0.9556\n",
      "iteration number: 1089\t training loss: 0.1456\tvalidation loss: 0.1424\t validation accuracy: 0.9511\n",
      "iteration number: 1090\t training loss: 0.1428\tvalidation loss: 0.1405\t validation accuracy: 0.9511\n",
      "iteration number: 1091\t training loss: 0.1410\tvalidation loss: 0.1381\t validation accuracy: 0.9511\n",
      "iteration number: 1092\t training loss: 0.1378\tvalidation loss: 0.1364\t validation accuracy: 0.9600\n",
      "iteration number: 1093\t training loss: 0.1373\tvalidation loss: 0.1352\t validation accuracy: 0.9578\n",
      "iteration number: 1094\t training loss: 0.1377\tvalidation loss: 0.1381\t validation accuracy: 0.9578\n",
      "iteration number: 1095\t training loss: 0.1373\tvalidation loss: 0.1374\t validation accuracy: 0.9578\n",
      "iteration number: 1096\t training loss: 0.1372\tvalidation loss: 0.1373\t validation accuracy: 0.9600\n",
      "iteration number: 1097\t training loss: 0.1373\tvalidation loss: 0.1373\t validation accuracy: 0.9578\n",
      "iteration number: 1098\t training loss: 0.1378\tvalidation loss: 0.1366\t validation accuracy: 0.9644\n",
      "iteration number: 1099\t training loss: 0.1382\tvalidation loss: 0.1366\t validation accuracy: 0.9622\n",
      "iteration number: 1100\t training loss: 0.1370\tvalidation loss: 0.1344\t validation accuracy: 0.9689\n",
      "iteration number: 1101\t training loss: 0.1373\tvalidation loss: 0.1349\t validation accuracy: 0.9689\n",
      "iteration number: 1102\t training loss: 0.1360\tvalidation loss: 0.1340\t validation accuracy: 0.9689\n",
      "iteration number: 1103\t training loss: 0.1362\tvalidation loss: 0.1346\t validation accuracy: 0.9667\n",
      "iteration number: 1104\t training loss: 0.1369\tvalidation loss: 0.1353\t validation accuracy: 0.9667\n",
      "iteration number: 1105\t training loss: 0.1388\tvalidation loss: 0.1353\t validation accuracy: 0.9711\n",
      "iteration number: 1106\t training loss: 0.1366\tvalidation loss: 0.1345\t validation accuracy: 0.9667\n",
      "iteration number: 1107\t training loss: 0.1364\tvalidation loss: 0.1327\t validation accuracy: 0.9689\n",
      "iteration number: 1108\t training loss: 0.1364\tvalidation loss: 0.1327\t validation accuracy: 0.9667\n",
      "iteration number: 1109\t training loss: 0.1375\tvalidation loss: 0.1332\t validation accuracy: 0.9689\n",
      "iteration number: 1110\t training loss: 0.1384\tvalidation loss: 0.1336\t validation accuracy: 0.9644\n",
      "iteration number: 1111\t training loss: 0.1400\tvalidation loss: 0.1342\t validation accuracy: 0.9667\n",
      "iteration number: 1112\t training loss: 0.1396\tvalidation loss: 0.1340\t validation accuracy: 0.9667\n",
      "iteration number: 1113\t training loss: 0.1375\tvalidation loss: 0.1331\t validation accuracy: 0.9644\n",
      "iteration number: 1114\t training loss: 0.1368\tvalidation loss: 0.1326\t validation accuracy: 0.9689\n",
      "iteration number: 1115\t training loss: 0.1355\tvalidation loss: 0.1322\t validation accuracy: 0.9644\n",
      "iteration number: 1116\t training loss: 0.1353\tvalidation loss: 0.1324\t validation accuracy: 0.9600\n",
      "iteration number: 1117\t training loss: 0.1365\tvalidation loss: 0.1333\t validation accuracy: 0.9644\n",
      "iteration number: 1118\t training loss: 0.1363\tvalidation loss: 0.1332\t validation accuracy: 0.9578\n",
      "iteration number: 1119\t training loss: 0.1370\tvalidation loss: 0.1360\t validation accuracy: 0.9578\n",
      "iteration number: 1120\t training loss: 0.1374\tvalidation loss: 0.1375\t validation accuracy: 0.9600\n",
      "iteration number: 1121\t training loss: 0.1366\tvalidation loss: 0.1362\t validation accuracy: 0.9600\n",
      "iteration number: 1122\t training loss: 0.1359\tvalidation loss: 0.1381\t validation accuracy: 0.9533\n",
      "iteration number: 1123\t training loss: 0.1353\tvalidation loss: 0.1354\t validation accuracy: 0.9578\n",
      "iteration number: 1124\t training loss: 0.1348\tvalidation loss: 0.1348\t validation accuracy: 0.9600\n",
      "iteration number: 1125\t training loss: 0.1336\tvalidation loss: 0.1341\t validation accuracy: 0.9600\n",
      "iteration number: 1126\t training loss: 0.1340\tvalidation loss: 0.1336\t validation accuracy: 0.9622\n",
      "iteration number: 1127\t training loss: 0.1340\tvalidation loss: 0.1321\t validation accuracy: 0.9578\n",
      "iteration number: 1128\t training loss: 0.1327\tvalidation loss: 0.1330\t validation accuracy: 0.9600\n",
      "iteration number: 1129\t training loss: 0.1333\tvalidation loss: 0.1350\t validation accuracy: 0.9578\n",
      "iteration number: 1130\t training loss: 0.1343\tvalidation loss: 0.1352\t validation accuracy: 0.9556\n",
      "iteration number: 1131\t training loss: 0.1333\tvalidation loss: 0.1341\t validation accuracy: 0.9556\n",
      "iteration number: 1132\t training loss: 0.1325\tvalidation loss: 0.1342\t validation accuracy: 0.9578\n",
      "iteration number: 1133\t training loss: 0.1338\tvalidation loss: 0.1355\t validation accuracy: 0.9578\n",
      "iteration number: 1134\t training loss: 0.1349\tvalidation loss: 0.1380\t validation accuracy: 0.9600\n",
      "iteration number: 1135\t training loss: 0.1400\tvalidation loss: 0.1415\t validation accuracy: 0.9533\n",
      "iteration number: 1136\t training loss: 0.1344\tvalidation loss: 0.1369\t validation accuracy: 0.9578\n",
      "iteration number: 1137\t training loss: 0.1339\tvalidation loss: 0.1360\t validation accuracy: 0.9533\n",
      "iteration number: 1138\t training loss: 0.1327\tvalidation loss: 0.1343\t validation accuracy: 0.9556\n",
      "iteration number: 1139\t training loss: 0.1325\tvalidation loss: 0.1358\t validation accuracy: 0.9578\n",
      "iteration number: 1140\t training loss: 0.1327\tvalidation loss: 0.1356\t validation accuracy: 0.9533\n",
      "iteration number: 1141\t training loss: 0.1321\tvalidation loss: 0.1351\t validation accuracy: 0.9600\n",
      "iteration number: 1142\t training loss: 0.1321\tvalidation loss: 0.1338\t validation accuracy: 0.9556\n",
      "iteration number: 1143\t training loss: 0.1341\tvalidation loss: 0.1379\t validation accuracy: 0.9533\n",
      "iteration number: 1144\t training loss: 0.1342\tvalidation loss: 0.1374\t validation accuracy: 0.9489\n",
      "iteration number: 1145\t training loss: 0.1318\tvalidation loss: 0.1326\t validation accuracy: 0.9533\n",
      "iteration number: 1146\t training loss: 0.1312\tvalidation loss: 0.1313\t validation accuracy: 0.9600\n",
      "iteration number: 1147\t training loss: 0.1303\tvalidation loss: 0.1310\t validation accuracy: 0.9600\n",
      "iteration number: 1148\t training loss: 0.1298\tvalidation loss: 0.1305\t validation accuracy: 0.9578\n",
      "iteration number: 1149\t training loss: 0.1321\tvalidation loss: 0.1341\t validation accuracy: 0.9578\n",
      "iteration number: 1150\t training loss: 0.1299\tvalidation loss: 0.1306\t validation accuracy: 0.9600\n",
      "iteration number: 1151\t training loss: 0.1324\tvalidation loss: 0.1322\t validation accuracy: 0.9644\n",
      "iteration number: 1152\t training loss: 0.1317\tvalidation loss: 0.1325\t validation accuracy: 0.9622\n",
      "iteration number: 1153\t training loss: 0.1302\tvalidation loss: 0.1304\t validation accuracy: 0.9667\n",
      "iteration number: 1154\t training loss: 0.1314\tvalidation loss: 0.1330\t validation accuracy: 0.9578\n",
      "iteration number: 1155\t training loss: 0.1339\tvalidation loss: 0.1360\t validation accuracy: 0.9533\n",
      "iteration number: 1156\t training loss: 0.1332\tvalidation loss: 0.1348\t validation accuracy: 0.9556\n",
      "iteration number: 1157\t training loss: 0.1334\tvalidation loss: 0.1348\t validation accuracy: 0.9556\n",
      "iteration number: 1158\t training loss: 0.1323\tvalidation loss: 0.1348\t validation accuracy: 0.9578\n",
      "iteration number: 1159\t training loss: 0.1345\tvalidation loss: 0.1375\t validation accuracy: 0.9578\n",
      "iteration number: 1160\t training loss: 0.1351\tvalidation loss: 0.1375\t validation accuracy: 0.9556\n",
      "iteration number: 1161\t training loss: 0.1339\tvalidation loss: 0.1364\t validation accuracy: 0.9600\n",
      "iteration number: 1162\t training loss: 0.1312\tvalidation loss: 0.1329\t validation accuracy: 0.9622\n",
      "iteration number: 1163\t training loss: 0.1304\tvalidation loss: 0.1320\t validation accuracy: 0.9667\n",
      "iteration number: 1164\t training loss: 0.1317\tvalidation loss: 0.1344\t validation accuracy: 0.9622\n",
      "iteration number: 1165\t training loss: 0.1306\tvalidation loss: 0.1330\t validation accuracy: 0.9667\n",
      "iteration number: 1166\t training loss: 0.1312\tvalidation loss: 0.1325\t validation accuracy: 0.9622\n",
      "iteration number: 1167\t training loss: 0.1314\tvalidation loss: 0.1338\t validation accuracy: 0.9556\n",
      "iteration number: 1168\t training loss: 0.1299\tvalidation loss: 0.1330\t validation accuracy: 0.9600\n",
      "iteration number: 1169\t training loss: 0.1309\tvalidation loss: 0.1329\t validation accuracy: 0.9600\n",
      "iteration number: 1170\t training loss: 0.1288\tvalidation loss: 0.1307\t validation accuracy: 0.9667\n",
      "iteration number: 1171\t training loss: 0.1284\tvalidation loss: 0.1305\t validation accuracy: 0.9644\n",
      "iteration number: 1172\t training loss: 0.1282\tvalidation loss: 0.1306\t validation accuracy: 0.9622\n",
      "iteration number: 1173\t training loss: 0.1285\tvalidation loss: 0.1302\t validation accuracy: 0.9667\n",
      "iteration number: 1174\t training loss: 0.1280\tvalidation loss: 0.1301\t validation accuracy: 0.9600\n",
      "iteration number: 1175\t training loss: 0.1283\tvalidation loss: 0.1293\t validation accuracy: 0.9600\n",
      "iteration number: 1176\t training loss: 0.1296\tvalidation loss: 0.1318\t validation accuracy: 0.9556\n",
      "iteration number: 1177\t training loss: 0.1296\tvalidation loss: 0.1311\t validation accuracy: 0.9578\n",
      "iteration number: 1178\t training loss: 0.1345\tvalidation loss: 0.1330\t validation accuracy: 0.9600\n",
      "iteration number: 1179\t training loss: 0.1296\tvalidation loss: 0.1299\t validation accuracy: 0.9667\n",
      "iteration number: 1180\t training loss: 0.1284\tvalidation loss: 0.1293\t validation accuracy: 0.9689\n",
      "iteration number: 1181\t training loss: 0.1287\tvalidation loss: 0.1296\t validation accuracy: 0.9689\n",
      "iteration number: 1182\t training loss: 0.1281\tvalidation loss: 0.1305\t validation accuracy: 0.9600\n",
      "iteration number: 1183\t training loss: 0.1292\tvalidation loss: 0.1326\t validation accuracy: 0.9578\n",
      "iteration number: 1184\t training loss: 0.1293\tvalidation loss: 0.1309\t validation accuracy: 0.9667\n",
      "iteration number: 1185\t training loss: 0.1285\tvalidation loss: 0.1306\t validation accuracy: 0.9667\n",
      "iteration number: 1186\t training loss: 0.1292\tvalidation loss: 0.1313\t validation accuracy: 0.9711\n",
      "iteration number: 1187\t training loss: 0.1305\tvalidation loss: 0.1303\t validation accuracy: 0.9667\n",
      "iteration number: 1188\t training loss: 0.1277\tvalidation loss: 0.1303\t validation accuracy: 0.9622\n",
      "iteration number: 1189\t training loss: 0.1277\tvalidation loss: 0.1314\t validation accuracy: 0.9622\n",
      "iteration number: 1190\t training loss: 0.1266\tvalidation loss: 0.1298\t validation accuracy: 0.9644\n",
      "iteration number: 1191\t training loss: 0.1263\tvalidation loss: 0.1302\t validation accuracy: 0.9622\n",
      "iteration number: 1192\t training loss: 0.1266\tvalidation loss: 0.1306\t validation accuracy: 0.9622\n",
      "iteration number: 1193\t training loss: 0.1266\tvalidation loss: 0.1305\t validation accuracy: 0.9644\n",
      "iteration number: 1194\t training loss: 0.1262\tvalidation loss: 0.1280\t validation accuracy: 0.9600\n",
      "iteration number: 1195\t training loss: 0.1261\tvalidation loss: 0.1271\t validation accuracy: 0.9644\n",
      "iteration number: 1196\t training loss: 0.1257\tvalidation loss: 0.1288\t validation accuracy: 0.9644\n",
      "iteration number: 1197\t training loss: 0.1279\tvalidation loss: 0.1310\t validation accuracy: 0.9578\n",
      "iteration number: 1198\t training loss: 0.1256\tvalidation loss: 0.1289\t validation accuracy: 0.9644\n",
      "iteration number: 1199\t training loss: 0.1251\tvalidation loss: 0.1270\t validation accuracy: 0.9644\n",
      "iteration number: 1200\t training loss: 0.1278\tvalidation loss: 0.1289\t validation accuracy: 0.9644\n",
      "iteration number: 1201\t training loss: 0.1255\tvalidation loss: 0.1279\t validation accuracy: 0.9644\n",
      "iteration number: 1202\t training loss: 0.1246\tvalidation loss: 0.1255\t validation accuracy: 0.9689\n",
      "iteration number: 1203\t training loss: 0.1254\tvalidation loss: 0.1253\t validation accuracy: 0.9644\n",
      "iteration number: 1204\t training loss: 0.1246\tvalidation loss: 0.1261\t validation accuracy: 0.9644\n",
      "iteration number: 1205\t training loss: 0.1253\tvalidation loss: 0.1251\t validation accuracy: 0.9667\n",
      "iteration number: 1206\t training loss: 0.1269\tvalidation loss: 0.1271\t validation accuracy: 0.9644\n",
      "iteration number: 1207\t training loss: 0.1290\tvalidation loss: 0.1273\t validation accuracy: 0.9644\n",
      "iteration number: 1208\t training loss: 0.1284\tvalidation loss: 0.1280\t validation accuracy: 0.9644\n",
      "iteration number: 1209\t training loss: 0.1263\tvalidation loss: 0.1264\t validation accuracy: 0.9622\n",
      "iteration number: 1210\t training loss: 0.1254\tvalidation loss: 0.1274\t validation accuracy: 0.9622\n",
      "iteration number: 1211\t training loss: 0.1258\tvalidation loss: 0.1298\t validation accuracy: 0.9622\n",
      "iteration number: 1212\t training loss: 0.1255\tvalidation loss: 0.1277\t validation accuracy: 0.9600\n",
      "iteration number: 1213\t training loss: 0.1244\tvalidation loss: 0.1259\t validation accuracy: 0.9667\n",
      "iteration number: 1214\t training loss: 0.1247\tvalidation loss: 0.1269\t validation accuracy: 0.9689\n",
      "iteration number: 1215\t training loss: 0.1242\tvalidation loss: 0.1262\t validation accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1216\t training loss: 0.1238\tvalidation loss: 0.1258\t validation accuracy: 0.9644\n",
      "iteration number: 1217\t training loss: 0.1238\tvalidation loss: 0.1253\t validation accuracy: 0.9622\n",
      "iteration number: 1218\t training loss: 0.1239\tvalidation loss: 0.1246\t validation accuracy: 0.9667\n",
      "iteration number: 1219\t training loss: 0.1240\tvalidation loss: 0.1259\t validation accuracy: 0.9622\n",
      "iteration number: 1220\t training loss: 0.1235\tvalidation loss: 0.1249\t validation accuracy: 0.9667\n",
      "iteration number: 1221\t training loss: 0.1236\tvalidation loss: 0.1239\t validation accuracy: 0.9667\n",
      "iteration number: 1222\t training loss: 0.1230\tvalidation loss: 0.1234\t validation accuracy: 0.9667\n",
      "iteration number: 1223\t training loss: 0.1224\tvalidation loss: 0.1243\t validation accuracy: 0.9689\n",
      "iteration number: 1224\t training loss: 0.1235\tvalidation loss: 0.1261\t validation accuracy: 0.9667\n",
      "iteration number: 1225\t training loss: 0.1232\tvalidation loss: 0.1265\t validation accuracy: 0.9667\n",
      "iteration number: 1226\t training loss: 0.1237\tvalidation loss: 0.1257\t validation accuracy: 0.9622\n",
      "iteration number: 1227\t training loss: 0.1238\tvalidation loss: 0.1270\t validation accuracy: 0.9689\n",
      "iteration number: 1228\t training loss: 0.1243\tvalidation loss: 0.1304\t validation accuracy: 0.9600\n",
      "iteration number: 1229\t training loss: 0.1234\tvalidation loss: 0.1284\t validation accuracy: 0.9644\n",
      "iteration number: 1230\t training loss: 0.1248\tvalidation loss: 0.1309\t validation accuracy: 0.9578\n",
      "iteration number: 1231\t training loss: 0.1236\tvalidation loss: 0.1291\t validation accuracy: 0.9667\n",
      "iteration number: 1232\t training loss: 0.1232\tvalidation loss: 0.1257\t validation accuracy: 0.9644\n",
      "iteration number: 1233\t training loss: 0.1226\tvalidation loss: 0.1265\t validation accuracy: 0.9667\n",
      "iteration number: 1234\t training loss: 0.1222\tvalidation loss: 0.1257\t validation accuracy: 0.9667\n",
      "iteration number: 1235\t training loss: 0.1232\tvalidation loss: 0.1269\t validation accuracy: 0.9622\n",
      "iteration number: 1236\t training loss: 0.1238\tvalidation loss: 0.1272\t validation accuracy: 0.9622\n",
      "iteration number: 1237\t training loss: 0.1229\tvalidation loss: 0.1271\t validation accuracy: 0.9644\n",
      "iteration number: 1238\t training loss: 0.1226\tvalidation loss: 0.1264\t validation accuracy: 0.9644\n",
      "iteration number: 1239\t training loss: 0.1225\tvalidation loss: 0.1277\t validation accuracy: 0.9600\n",
      "iteration number: 1240\t training loss: 0.1222\tvalidation loss: 0.1270\t validation accuracy: 0.9667\n",
      "iteration number: 1241\t training loss: 0.1226\tvalidation loss: 0.1270\t validation accuracy: 0.9644\n",
      "iteration number: 1242\t training loss: 0.1230\tvalidation loss: 0.1283\t validation accuracy: 0.9667\n",
      "iteration number: 1243\t training loss: 0.1240\tvalidation loss: 0.1293\t validation accuracy: 0.9600\n",
      "iteration number: 1244\t training loss: 0.1214\tvalidation loss: 0.1260\t validation accuracy: 0.9644\n",
      "iteration number: 1245\t training loss: 0.1217\tvalidation loss: 0.1258\t validation accuracy: 0.9644\n",
      "iteration number: 1246\t training loss: 0.1223\tvalidation loss: 0.1263\t validation accuracy: 0.9600\n",
      "iteration number: 1247\t training loss: 0.1235\tvalidation loss: 0.1308\t validation accuracy: 0.9578\n",
      "iteration number: 1248\t training loss: 0.1234\tvalidation loss: 0.1306\t validation accuracy: 0.9578\n",
      "iteration number: 1249\t training loss: 0.1236\tvalidation loss: 0.1298\t validation accuracy: 0.9556\n",
      "iteration number: 1250\t training loss: 0.1224\tvalidation loss: 0.1279\t validation accuracy: 0.9600\n",
      "iteration number: 1251\t training loss: 0.1220\tvalidation loss: 0.1263\t validation accuracy: 0.9667\n",
      "iteration number: 1252\t training loss: 0.1207\tvalidation loss: 0.1237\t validation accuracy: 0.9644\n",
      "iteration number: 1253\t training loss: 0.1230\tvalidation loss: 0.1255\t validation accuracy: 0.9667\n",
      "iteration number: 1254\t training loss: 0.1225\tvalidation loss: 0.1265\t validation accuracy: 0.9622\n",
      "iteration number: 1255\t training loss: 0.1235\tvalidation loss: 0.1273\t validation accuracy: 0.9622\n",
      "iteration number: 1256\t training loss: 0.1212\tvalidation loss: 0.1265\t validation accuracy: 0.9622\n",
      "iteration number: 1257\t training loss: 0.1200\tvalidation loss: 0.1240\t validation accuracy: 0.9667\n",
      "iteration number: 1258\t training loss: 0.1207\tvalidation loss: 0.1261\t validation accuracy: 0.9667\n",
      "iteration number: 1259\t training loss: 0.1213\tvalidation loss: 0.1263\t validation accuracy: 0.9600\n",
      "iteration number: 1260\t training loss: 0.1190\tvalidation loss: 0.1233\t validation accuracy: 0.9667\n",
      "iteration number: 1261\t training loss: 0.1205\tvalidation loss: 0.1268\t validation accuracy: 0.9622\n",
      "iteration number: 1262\t training loss: 0.1205\tvalidation loss: 0.1275\t validation accuracy: 0.9622\n",
      "iteration number: 1263\t training loss: 0.1212\tvalidation loss: 0.1275\t validation accuracy: 0.9578\n",
      "iteration number: 1264\t training loss: 0.1191\tvalidation loss: 0.1234\t validation accuracy: 0.9689\n",
      "iteration number: 1265\t training loss: 0.1193\tvalidation loss: 0.1248\t validation accuracy: 0.9667\n",
      "iteration number: 1266\t training loss: 0.1193\tvalidation loss: 0.1254\t validation accuracy: 0.9667\n",
      "iteration number: 1267\t training loss: 0.1197\tvalidation loss: 0.1258\t validation accuracy: 0.9689\n",
      "iteration number: 1268\t training loss: 0.1197\tvalidation loss: 0.1265\t validation accuracy: 0.9644\n",
      "iteration number: 1269\t training loss: 0.1192\tvalidation loss: 0.1243\t validation accuracy: 0.9667\n",
      "iteration number: 1270\t training loss: 0.1189\tvalidation loss: 0.1245\t validation accuracy: 0.9644\n",
      "iteration number: 1271\t training loss: 0.1193\tvalidation loss: 0.1249\t validation accuracy: 0.9622\n",
      "iteration number: 1272\t training loss: 0.1194\tvalidation loss: 0.1265\t validation accuracy: 0.9622\n",
      "iteration number: 1273\t training loss: 0.1186\tvalidation loss: 0.1243\t validation accuracy: 0.9689\n",
      "iteration number: 1274\t training loss: 0.1206\tvalidation loss: 0.1230\t validation accuracy: 0.9578\n",
      "iteration number: 1275\t training loss: 0.1201\tvalidation loss: 0.1224\t validation accuracy: 0.9622\n",
      "iteration number: 1276\t training loss: 0.1206\tvalidation loss: 0.1222\t validation accuracy: 0.9622\n",
      "iteration number: 1277\t training loss: 0.1210\tvalidation loss: 0.1227\t validation accuracy: 0.9667\n",
      "iteration number: 1278\t training loss: 0.1207\tvalidation loss: 0.1225\t validation accuracy: 0.9711\n",
      "iteration number: 1279\t training loss: 0.1196\tvalidation loss: 0.1220\t validation accuracy: 0.9711\n",
      "iteration number: 1280\t training loss: 0.1186\tvalidation loss: 0.1211\t validation accuracy: 0.9667\n",
      "iteration number: 1281\t training loss: 0.1187\tvalidation loss: 0.1228\t validation accuracy: 0.9644\n",
      "iteration number: 1282\t training loss: 0.1176\tvalidation loss: 0.1233\t validation accuracy: 0.9644\n",
      "iteration number: 1283\t training loss: 0.1171\tvalidation loss: 0.1214\t validation accuracy: 0.9711\n",
      "iteration number: 1284\t training loss: 0.1174\tvalidation loss: 0.1204\t validation accuracy: 0.9711\n",
      "iteration number: 1285\t training loss: 0.1169\tvalidation loss: 0.1202\t validation accuracy: 0.9711\n",
      "iteration number: 1286\t training loss: 0.1171\tvalidation loss: 0.1217\t validation accuracy: 0.9733\n",
      "iteration number: 1287\t training loss: 0.1169\tvalidation loss: 0.1218\t validation accuracy: 0.9733\n",
      "iteration number: 1288\t training loss: 0.1166\tvalidation loss: 0.1207\t validation accuracy: 0.9756\n",
      "iteration number: 1289\t training loss: 0.1163\tvalidation loss: 0.1208\t validation accuracy: 0.9689\n",
      "iteration number: 1290\t training loss: 0.1175\tvalidation loss: 0.1199\t validation accuracy: 0.9689\n",
      "iteration number: 1291\t training loss: 0.1184\tvalidation loss: 0.1193\t validation accuracy: 0.9711\n",
      "iteration number: 1292\t training loss: 0.1164\tvalidation loss: 0.1193\t validation accuracy: 0.9689\n",
      "iteration number: 1293\t training loss: 0.1165\tvalidation loss: 0.1204\t validation accuracy: 0.9689\n",
      "iteration number: 1294\t training loss: 0.1202\tvalidation loss: 0.1222\t validation accuracy: 0.9667\n",
      "iteration number: 1295\t training loss: 0.1173\tvalidation loss: 0.1224\t validation accuracy: 0.9644\n",
      "iteration number: 1296\t training loss: 0.1179\tvalidation loss: 0.1223\t validation accuracy: 0.9622\n",
      "iteration number: 1297\t training loss: 0.1168\tvalidation loss: 0.1203\t validation accuracy: 0.9644\n",
      "iteration number: 1298\t training loss: 0.1168\tvalidation loss: 0.1209\t validation accuracy: 0.9622\n",
      "iteration number: 1299\t training loss: 0.1170\tvalidation loss: 0.1200\t validation accuracy: 0.9622\n",
      "iteration number: 1300\t training loss: 0.1161\tvalidation loss: 0.1195\t validation accuracy: 0.9689\n",
      "iteration number: 1301\t training loss: 0.1173\tvalidation loss: 0.1201\t validation accuracy: 0.9622\n",
      "iteration number: 1302\t training loss: 0.1172\tvalidation loss: 0.1215\t validation accuracy: 0.9689\n",
      "iteration number: 1303\t training loss: 0.1178\tvalidation loss: 0.1226\t validation accuracy: 0.9644\n",
      "iteration number: 1304\t training loss: 0.1163\tvalidation loss: 0.1225\t validation accuracy: 0.9711\n",
      "iteration number: 1305\t training loss: 0.1164\tvalidation loss: 0.1229\t validation accuracy: 0.9733\n",
      "iteration number: 1306\t training loss: 0.1174\tvalidation loss: 0.1243\t validation accuracy: 0.9711\n",
      "iteration number: 1307\t training loss: 0.1162\tvalidation loss: 0.1230\t validation accuracy: 0.9667\n",
      "iteration number: 1308\t training loss: 0.1155\tvalidation loss: 0.1220\t validation accuracy: 0.9644\n",
      "iteration number: 1309\t training loss: 0.1170\tvalidation loss: 0.1222\t validation accuracy: 0.9600\n",
      "iteration number: 1310\t training loss: 0.1185\tvalidation loss: 0.1249\t validation accuracy: 0.9600\n",
      "iteration number: 1311\t training loss: 0.1157\tvalidation loss: 0.1205\t validation accuracy: 0.9689\n",
      "iteration number: 1312\t training loss: 0.1157\tvalidation loss: 0.1197\t validation accuracy: 0.9689\n",
      "iteration number: 1313\t training loss: 0.1161\tvalidation loss: 0.1197\t validation accuracy: 0.9667\n",
      "iteration number: 1314\t training loss: 0.1173\tvalidation loss: 0.1236\t validation accuracy: 0.9644\n",
      "iteration number: 1315\t training loss: 0.1158\tvalidation loss: 0.1201\t validation accuracy: 0.9644\n",
      "iteration number: 1316\t training loss: 0.1173\tvalidation loss: 0.1210\t validation accuracy: 0.9644\n",
      "iteration number: 1317\t training loss: 0.1163\tvalidation loss: 0.1211\t validation accuracy: 0.9644\n",
      "iteration number: 1318\t training loss: 0.1190\tvalidation loss: 0.1246\t validation accuracy: 0.9689\n",
      "iteration number: 1319\t training loss: 0.1182\tvalidation loss: 0.1245\t validation accuracy: 0.9689\n",
      "iteration number: 1320\t training loss: 0.1153\tvalidation loss: 0.1203\t validation accuracy: 0.9711\n",
      "iteration number: 1321\t training loss: 0.1148\tvalidation loss: 0.1185\t validation accuracy: 0.9689\n",
      "iteration number: 1322\t training loss: 0.1143\tvalidation loss: 0.1185\t validation accuracy: 0.9667\n",
      "iteration number: 1323\t training loss: 0.1139\tvalidation loss: 0.1193\t validation accuracy: 0.9689\n",
      "iteration number: 1324\t training loss: 0.1140\tvalidation loss: 0.1187\t validation accuracy: 0.9711\n",
      "iteration number: 1325\t training loss: 0.1147\tvalidation loss: 0.1198\t validation accuracy: 0.9689\n",
      "iteration number: 1326\t training loss: 0.1167\tvalidation loss: 0.1201\t validation accuracy: 0.9622\n",
      "iteration number: 1327\t training loss: 0.1158\tvalidation loss: 0.1186\t validation accuracy: 0.9711\n",
      "iteration number: 1328\t training loss: 0.1147\tvalidation loss: 0.1171\t validation accuracy: 0.9733\n",
      "iteration number: 1329\t training loss: 0.1158\tvalidation loss: 0.1184\t validation accuracy: 0.9711\n",
      "iteration number: 1330\t training loss: 0.1144\tvalidation loss: 0.1175\t validation accuracy: 0.9733\n",
      "iteration number: 1331\t training loss: 0.1132\tvalidation loss: 0.1171\t validation accuracy: 0.9689\n",
      "iteration number: 1332\t training loss: 0.1136\tvalidation loss: 0.1174\t validation accuracy: 0.9689\n",
      "iteration number: 1333\t training loss: 0.1131\tvalidation loss: 0.1175\t validation accuracy: 0.9667\n",
      "iteration number: 1334\t training loss: 0.1133\tvalidation loss: 0.1188\t validation accuracy: 0.9689\n",
      "iteration number: 1335\t training loss: 0.1168\tvalidation loss: 0.1200\t validation accuracy: 0.9667\n",
      "iteration number: 1336\t training loss: 0.1177\tvalidation loss: 0.1196\t validation accuracy: 0.9689\n",
      "iteration number: 1337\t training loss: 0.1174\tvalidation loss: 0.1186\t validation accuracy: 0.9689\n",
      "iteration number: 1338\t training loss: 0.1160\tvalidation loss: 0.1181\t validation accuracy: 0.9711\n",
      "iteration number: 1339\t training loss: 0.1137\tvalidation loss: 0.1156\t validation accuracy: 0.9733\n",
      "iteration number: 1340\t training loss: 0.1136\tvalidation loss: 0.1158\t validation accuracy: 0.9733\n",
      "iteration number: 1341\t training loss: 0.1136\tvalidation loss: 0.1153\t validation accuracy: 0.9711\n",
      "iteration number: 1342\t training loss: 0.1140\tvalidation loss: 0.1141\t validation accuracy: 0.9733\n",
      "iteration number: 1343\t training loss: 0.1145\tvalidation loss: 0.1137\t validation accuracy: 0.9733\n",
      "iteration number: 1344\t training loss: 0.1140\tvalidation loss: 0.1134\t validation accuracy: 0.9711\n",
      "iteration number: 1345\t training loss: 0.1127\tvalidation loss: 0.1129\t validation accuracy: 0.9711\n",
      "iteration number: 1346\t training loss: 0.1134\tvalidation loss: 0.1127\t validation accuracy: 0.9733\n",
      "iteration number: 1347\t training loss: 0.1141\tvalidation loss: 0.1130\t validation accuracy: 0.9733\n",
      "iteration number: 1348\t training loss: 0.1145\tvalidation loss: 0.1140\t validation accuracy: 0.9689\n",
      "iteration number: 1349\t training loss: 0.1143\tvalidation loss: 0.1148\t validation accuracy: 0.9711\n",
      "iteration number: 1350\t training loss: 0.1128\tvalidation loss: 0.1141\t validation accuracy: 0.9711\n",
      "iteration number: 1351\t training loss: 0.1123\tvalidation loss: 0.1135\t validation accuracy: 0.9711\n",
      "iteration number: 1352\t training loss: 0.1123\tvalidation loss: 0.1152\t validation accuracy: 0.9711\n",
      "iteration number: 1353\t training loss: 0.1161\tvalidation loss: 0.1193\t validation accuracy: 0.9644\n",
      "iteration number: 1354\t training loss: 0.1127\tvalidation loss: 0.1154\t validation accuracy: 0.9667\n",
      "iteration number: 1355\t training loss: 0.1133\tvalidation loss: 0.1155\t validation accuracy: 0.9667\n",
      "iteration number: 1356\t training loss: 0.1121\tvalidation loss: 0.1144\t validation accuracy: 0.9733\n",
      "iteration number: 1357\t training loss: 0.1119\tvalidation loss: 0.1131\t validation accuracy: 0.9711\n",
      "iteration number: 1358\t training loss: 0.1126\tvalidation loss: 0.1137\t validation accuracy: 0.9733\n",
      "iteration number: 1359\t training loss: 0.1126\tvalidation loss: 0.1135\t validation accuracy: 0.9711\n",
      "iteration number: 1360\t training loss: 0.1123\tvalidation loss: 0.1135\t validation accuracy: 0.9711\n",
      "iteration number: 1361\t training loss: 0.1124\tvalidation loss: 0.1139\t validation accuracy: 0.9733\n",
      "iteration number: 1362\t training loss: 0.1133\tvalidation loss: 0.1157\t validation accuracy: 0.9733\n",
      "iteration number: 1363\t training loss: 0.1130\tvalidation loss: 0.1173\t validation accuracy: 0.9689\n",
      "iteration number: 1364\t training loss: 0.1109\tvalidation loss: 0.1131\t validation accuracy: 0.9689\n",
      "iteration number: 1365\t training loss: 0.1121\tvalidation loss: 0.1127\t validation accuracy: 0.9689\n",
      "iteration number: 1366\t training loss: 0.1110\tvalidation loss: 0.1118\t validation accuracy: 0.9733\n",
      "iteration number: 1367\t training loss: 0.1113\tvalidation loss: 0.1125\t validation accuracy: 0.9711\n",
      "iteration number: 1368\t training loss: 0.1134\tvalidation loss: 0.1131\t validation accuracy: 0.9711\n",
      "iteration number: 1369\t training loss: 0.1129\tvalidation loss: 0.1127\t validation accuracy: 0.9711\n",
      "iteration number: 1370\t training loss: 0.1112\tvalidation loss: 0.1111\t validation accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1371\t training loss: 0.1110\tvalidation loss: 0.1111\t validation accuracy: 0.9756\n",
      "iteration number: 1372\t training loss: 0.1106\tvalidation loss: 0.1119\t validation accuracy: 0.9756\n",
      "iteration number: 1373\t training loss: 0.1100\tvalidation loss: 0.1114\t validation accuracy: 0.9733\n",
      "iteration number: 1374\t training loss: 0.1117\tvalidation loss: 0.1113\t validation accuracy: 0.9711\n",
      "iteration number: 1375\t training loss: 0.1119\tvalidation loss: 0.1127\t validation accuracy: 0.9667\n",
      "iteration number: 1376\t training loss: 0.1114\tvalidation loss: 0.1117\t validation accuracy: 0.9689\n",
      "iteration number: 1377\t training loss: 0.1115\tvalidation loss: 0.1109\t validation accuracy: 0.9733\n",
      "iteration number: 1378\t training loss: 0.1120\tvalidation loss: 0.1124\t validation accuracy: 0.9733\n",
      "iteration number: 1379\t training loss: 0.1102\tvalidation loss: 0.1114\t validation accuracy: 0.9756\n",
      "iteration number: 1380\t training loss: 0.1112\tvalidation loss: 0.1116\t validation accuracy: 0.9733\n",
      "iteration number: 1381\t training loss: 0.1105\tvalidation loss: 0.1123\t validation accuracy: 0.9756\n",
      "iteration number: 1382\t training loss: 0.1136\tvalidation loss: 0.1129\t validation accuracy: 0.9711\n",
      "iteration number: 1383\t training loss: 0.1090\tvalidation loss: 0.1105\t validation accuracy: 0.9756\n",
      "iteration number: 1384\t training loss: 0.1088\tvalidation loss: 0.1101\t validation accuracy: 0.9711\n",
      "iteration number: 1385\t training loss: 0.1099\tvalidation loss: 0.1109\t validation accuracy: 0.9689\n",
      "iteration number: 1386\t training loss: 0.1093\tvalidation loss: 0.1100\t validation accuracy: 0.9689\n",
      "iteration number: 1387\t training loss: 0.1112\tvalidation loss: 0.1109\t validation accuracy: 0.9689\n",
      "iteration number: 1388\t training loss: 0.1091\tvalidation loss: 0.1112\t validation accuracy: 0.9689\n",
      "iteration number: 1389\t training loss: 0.1098\tvalidation loss: 0.1112\t validation accuracy: 0.9689\n",
      "iteration number: 1390\t training loss: 0.1105\tvalidation loss: 0.1110\t validation accuracy: 0.9689\n",
      "iteration number: 1391\t training loss: 0.1087\tvalidation loss: 0.1099\t validation accuracy: 0.9711\n",
      "iteration number: 1392\t training loss: 0.1098\tvalidation loss: 0.1094\t validation accuracy: 0.9733\n",
      "iteration number: 1393\t training loss: 0.1097\tvalidation loss: 0.1100\t validation accuracy: 0.9733\n",
      "iteration number: 1394\t training loss: 0.1082\tvalidation loss: 0.1098\t validation accuracy: 0.9756\n",
      "iteration number: 1395\t training loss: 0.1082\tvalidation loss: 0.1097\t validation accuracy: 0.9733\n",
      "iteration number: 1396\t training loss: 0.1089\tvalidation loss: 0.1105\t validation accuracy: 0.9689\n",
      "iteration number: 1397\t training loss: 0.1081\tvalidation loss: 0.1110\t validation accuracy: 0.9711\n",
      "iteration number: 1398\t training loss: 0.1076\tvalidation loss: 0.1109\t validation accuracy: 0.9733\n",
      "iteration number: 1399\t training loss: 0.1077\tvalidation loss: 0.1112\t validation accuracy: 0.9756\n",
      "iteration number: 1400\t training loss: 0.1094\tvalidation loss: 0.1126\t validation accuracy: 0.9711\n",
      "iteration number: 1401\t training loss: 0.1079\tvalidation loss: 0.1104\t validation accuracy: 0.9733\n",
      "iteration number: 1402\t training loss: 0.1084\tvalidation loss: 0.1111\t validation accuracy: 0.9711\n",
      "iteration number: 1403\t training loss: 0.1087\tvalidation loss: 0.1106\t validation accuracy: 0.9733\n",
      "iteration number: 1404\t training loss: 0.1099\tvalidation loss: 0.1101\t validation accuracy: 0.9756\n",
      "iteration number: 1405\t training loss: 0.1108\tvalidation loss: 0.1109\t validation accuracy: 0.9756\n",
      "iteration number: 1406\t training loss: 0.1116\tvalidation loss: 0.1105\t validation accuracy: 0.9778\n",
      "iteration number: 1407\t training loss: 0.1097\tvalidation loss: 0.1087\t validation accuracy: 0.9756\n",
      "iteration number: 1408\t training loss: 0.1106\tvalidation loss: 0.1095\t validation accuracy: 0.9733\n",
      "iteration number: 1409\t training loss: 0.1111\tvalidation loss: 0.1106\t validation accuracy: 0.9711\n",
      "iteration number: 1410\t training loss: 0.1123\tvalidation loss: 0.1102\t validation accuracy: 0.9711\n",
      "iteration number: 1411\t training loss: 0.1141\tvalidation loss: 0.1107\t validation accuracy: 0.9733\n",
      "iteration number: 1412\t training loss: 0.1098\tvalidation loss: 0.1098\t validation accuracy: 0.9711\n",
      "iteration number: 1413\t training loss: 0.1107\tvalidation loss: 0.1106\t validation accuracy: 0.9733\n",
      "iteration number: 1414\t training loss: 0.1081\tvalidation loss: 0.1080\t validation accuracy: 0.9711\n",
      "iteration number: 1415\t training loss: 0.1080\tvalidation loss: 0.1085\t validation accuracy: 0.9711\n",
      "iteration number: 1416\t training loss: 0.1078\tvalidation loss: 0.1082\t validation accuracy: 0.9711\n",
      "iteration number: 1417\t training loss: 0.1086\tvalidation loss: 0.1087\t validation accuracy: 0.9733\n",
      "iteration number: 1418\t training loss: 0.1078\tvalidation loss: 0.1095\t validation accuracy: 0.9711\n",
      "iteration number: 1419\t training loss: 0.1079\tvalidation loss: 0.1098\t validation accuracy: 0.9689\n",
      "iteration number: 1420\t training loss: 0.1075\tvalidation loss: 0.1095\t validation accuracy: 0.9689\n",
      "iteration number: 1421\t training loss: 0.1073\tvalidation loss: 0.1101\t validation accuracy: 0.9689\n",
      "iteration number: 1422\t training loss: 0.1073\tvalidation loss: 0.1096\t validation accuracy: 0.9711\n",
      "iteration number: 1423\t training loss: 0.1067\tvalidation loss: 0.1097\t validation accuracy: 0.9711\n",
      "iteration number: 1424\t training loss: 0.1081\tvalidation loss: 0.1106\t validation accuracy: 0.9689\n",
      "iteration number: 1425\t training loss: 0.1070\tvalidation loss: 0.1100\t validation accuracy: 0.9711\n",
      "iteration number: 1426\t training loss: 0.1074\tvalidation loss: 0.1107\t validation accuracy: 0.9711\n",
      "iteration number: 1427\t training loss: 0.1081\tvalidation loss: 0.1108\t validation accuracy: 0.9711\n",
      "iteration number: 1428\t training loss: 0.1068\tvalidation loss: 0.1100\t validation accuracy: 0.9733\n",
      "iteration number: 1429\t training loss: 0.1078\tvalidation loss: 0.1127\t validation accuracy: 0.9689\n",
      "iteration number: 1430\t training loss: 0.1078\tvalidation loss: 0.1096\t validation accuracy: 0.9733\n",
      "iteration number: 1431\t training loss: 0.1069\tvalidation loss: 0.1095\t validation accuracy: 0.9711\n",
      "iteration number: 1432\t training loss: 0.1077\tvalidation loss: 0.1103\t validation accuracy: 0.9756\n",
      "iteration number: 1433\t training loss: 0.1074\tvalidation loss: 0.1109\t validation accuracy: 0.9756\n",
      "iteration number: 1434\t training loss: 0.1075\tvalidation loss: 0.1124\t validation accuracy: 0.9733\n",
      "iteration number: 1435\t training loss: 0.1087\tvalidation loss: 0.1129\t validation accuracy: 0.9733\n",
      "iteration number: 1436\t training loss: 0.1062\tvalidation loss: 0.1087\t validation accuracy: 0.9733\n",
      "iteration number: 1437\t training loss: 0.1070\tvalidation loss: 0.1086\t validation accuracy: 0.9733\n",
      "iteration number: 1438\t training loss: 0.1085\tvalidation loss: 0.1100\t validation accuracy: 0.9756\n",
      "iteration number: 1439\t training loss: 0.1090\tvalidation loss: 0.1116\t validation accuracy: 0.9733\n",
      "iteration number: 1440\t training loss: 0.1078\tvalidation loss: 0.1116\t validation accuracy: 0.9689\n",
      "iteration number: 1441\t training loss: 0.1085\tvalidation loss: 0.1114\t validation accuracy: 0.9711\n",
      "iteration number: 1442\t training loss: 0.1082\tvalidation loss: 0.1112\t validation accuracy: 0.9711\n",
      "iteration number: 1443\t training loss: 0.1086\tvalidation loss: 0.1117\t validation accuracy: 0.9711\n",
      "iteration number: 1444\t training loss: 0.1070\tvalidation loss: 0.1119\t validation accuracy: 0.9711\n",
      "iteration number: 1445\t training loss: 0.1057\tvalidation loss: 0.1108\t validation accuracy: 0.9733\n",
      "iteration number: 1446\t training loss: 0.1061\tvalidation loss: 0.1121\t validation accuracy: 0.9689\n",
      "iteration number: 1447\t training loss: 0.1046\tvalidation loss: 0.1096\t validation accuracy: 0.9733\n",
      "iteration number: 1448\t training loss: 0.1052\tvalidation loss: 0.1100\t validation accuracy: 0.9689\n",
      "iteration number: 1449\t training loss: 0.1052\tvalidation loss: 0.1096\t validation accuracy: 0.9711\n",
      "iteration number: 1450\t training loss: 0.1045\tvalidation loss: 0.1088\t validation accuracy: 0.9711\n",
      "iteration number: 1451\t training loss: 0.1050\tvalidation loss: 0.1095\t validation accuracy: 0.9711\n",
      "iteration number: 1452\t training loss: 0.1054\tvalidation loss: 0.1092\t validation accuracy: 0.9711\n",
      "iteration number: 1453\t training loss: 0.1047\tvalidation loss: 0.1095\t validation accuracy: 0.9756\n",
      "iteration number: 1454\t training loss: 0.1045\tvalidation loss: 0.1089\t validation accuracy: 0.9756\n",
      "iteration number: 1455\t training loss: 0.1041\tvalidation loss: 0.1084\t validation accuracy: 0.9733\n",
      "iteration number: 1456\t training loss: 0.1041\tvalidation loss: 0.1086\t validation accuracy: 0.9711\n",
      "iteration number: 1457\t training loss: 0.1039\tvalidation loss: 0.1093\t validation accuracy: 0.9711\n",
      "iteration number: 1458\t training loss: 0.1051\tvalidation loss: 0.1107\t validation accuracy: 0.9733\n",
      "iteration number: 1459\t training loss: 0.1047\tvalidation loss: 0.1106\t validation accuracy: 0.9711\n",
      "iteration number: 1460\t training loss: 0.1032\tvalidation loss: 0.1083\t validation accuracy: 0.9711\n",
      "iteration number: 1461\t training loss: 0.1042\tvalidation loss: 0.1072\t validation accuracy: 0.9711\n",
      "iteration number: 1462\t training loss: 0.1046\tvalidation loss: 0.1075\t validation accuracy: 0.9711\n",
      "iteration number: 1463\t training loss: 0.1056\tvalidation loss: 0.1091\t validation accuracy: 0.9689\n",
      "iteration number: 1464\t training loss: 0.1040\tvalidation loss: 0.1092\t validation accuracy: 0.9711\n",
      "iteration number: 1465\t training loss: 0.1033\tvalidation loss: 0.1080\t validation accuracy: 0.9756\n",
      "iteration number: 1466\t training loss: 0.1044\tvalidation loss: 0.1088\t validation accuracy: 0.9689\n",
      "iteration number: 1467\t training loss: 0.1038\tvalidation loss: 0.1070\t validation accuracy: 0.9756\n",
      "iteration number: 1468\t training loss: 0.1032\tvalidation loss: 0.1068\t validation accuracy: 0.9733\n",
      "iteration number: 1469\t training loss: 0.1031\tvalidation loss: 0.1069\t validation accuracy: 0.9711\n",
      "iteration number: 1470\t training loss: 0.1036\tvalidation loss: 0.1071\t validation accuracy: 0.9711\n",
      "iteration number: 1471\t training loss: 0.1062\tvalidation loss: 0.1088\t validation accuracy: 0.9711\n",
      "iteration number: 1472\t training loss: 0.1047\tvalidation loss: 0.1073\t validation accuracy: 0.9711\n",
      "iteration number: 1473\t training loss: 0.1041\tvalidation loss: 0.1078\t validation accuracy: 0.9689\n",
      "iteration number: 1474\t training loss: 0.1032\tvalidation loss: 0.1081\t validation accuracy: 0.9733\n",
      "iteration number: 1475\t training loss: 0.1031\tvalidation loss: 0.1089\t validation accuracy: 0.9711\n",
      "iteration number: 1476\t training loss: 0.1034\tvalidation loss: 0.1086\t validation accuracy: 0.9711\n",
      "iteration number: 1477\t training loss: 0.1054\tvalidation loss: 0.1100\t validation accuracy: 0.9667\n",
      "iteration number: 1478\t training loss: 0.1043\tvalidation loss: 0.1088\t validation accuracy: 0.9733\n",
      "iteration number: 1479\t training loss: 0.1046\tvalidation loss: 0.1090\t validation accuracy: 0.9733\n",
      "iteration number: 1480\t training loss: 0.1059\tvalidation loss: 0.1096\t validation accuracy: 0.9667\n",
      "iteration number: 1481\t training loss: 0.1045\tvalidation loss: 0.1089\t validation accuracy: 0.9711\n",
      "iteration number: 1482\t training loss: 0.1046\tvalidation loss: 0.1091\t validation accuracy: 0.9689\n",
      "iteration number: 1483\t training loss: 0.1042\tvalidation loss: 0.1098\t validation accuracy: 0.9689\n",
      "iteration number: 1484\t training loss: 0.1031\tvalidation loss: 0.1077\t validation accuracy: 0.9733\n",
      "iteration number: 1485\t training loss: 0.1034\tvalidation loss: 0.1083\t validation accuracy: 0.9711\n",
      "iteration number: 1486\t training loss: 0.1036\tvalidation loss: 0.1081\t validation accuracy: 0.9733\n",
      "iteration number: 1487\t training loss: 0.1034\tvalidation loss: 0.1092\t validation accuracy: 0.9689\n",
      "iteration number: 1488\t training loss: 0.1018\tvalidation loss: 0.1071\t validation accuracy: 0.9756\n",
      "iteration number: 1489\t training loss: 0.1043\tvalidation loss: 0.1094\t validation accuracy: 0.9711\n",
      "iteration number: 1490\t training loss: 0.1023\tvalidation loss: 0.1078\t validation accuracy: 0.9756\n",
      "iteration number: 1491\t training loss: 0.1049\tvalidation loss: 0.1108\t validation accuracy: 0.9711\n",
      "iteration number: 1492\t training loss: 0.1040\tvalidation loss: 0.1098\t validation accuracy: 0.9711\n",
      "iteration number: 1493\t training loss: 0.1058\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 1494\t training loss: 0.1049\tvalidation loss: 0.1113\t validation accuracy: 0.9711\n",
      "iteration number: 1495\t training loss: 0.1066\tvalidation loss: 0.1122\t validation accuracy: 0.9689\n",
      "iteration number: 1496\t training loss: 0.1055\tvalidation loss: 0.1104\t validation accuracy: 0.9711\n",
      "iteration number: 1497\t training loss: 0.1057\tvalidation loss: 0.1107\t validation accuracy: 0.9689\n",
      "iteration number: 1498\t training loss: 0.1054\tvalidation loss: 0.1108\t validation accuracy: 0.9689\n",
      "iteration number: 1499\t training loss: 0.1040\tvalidation loss: 0.1085\t validation accuracy: 0.9711\n",
      "iteration number: 1500\t training loss: 0.1019\tvalidation loss: 0.1078\t validation accuracy: 0.9756\n",
      "iteration number: 1501\t training loss: 0.1030\tvalidation loss: 0.1108\t validation accuracy: 0.9689\n",
      "iteration number: 1502\t training loss: 0.1033\tvalidation loss: 0.1111\t validation accuracy: 0.9689\n",
      "iteration number: 1503\t training loss: 0.1027\tvalidation loss: 0.1102\t validation accuracy: 0.9711\n",
      "iteration number: 1504\t training loss: 0.1023\tvalidation loss: 0.1099\t validation accuracy: 0.9711\n",
      "iteration number: 1505\t training loss: 0.1051\tvalidation loss: 0.1123\t validation accuracy: 0.9689\n",
      "iteration number: 1506\t training loss: 0.1048\tvalidation loss: 0.1128\t validation accuracy: 0.9689\n",
      "iteration number: 1507\t training loss: 0.1050\tvalidation loss: 0.1140\t validation accuracy: 0.9689\n",
      "iteration number: 1508\t training loss: 0.1037\tvalidation loss: 0.1122\t validation accuracy: 0.9689\n",
      "iteration number: 1509\t training loss: 0.1036\tvalidation loss: 0.1126\t validation accuracy: 0.9689\n",
      "iteration number: 1510\t training loss: 0.1011\tvalidation loss: 0.1086\t validation accuracy: 0.9711\n",
      "iteration number: 1511\t training loss: 0.1030\tvalidation loss: 0.1110\t validation accuracy: 0.9689\n",
      "iteration number: 1512\t training loss: 0.1028\tvalidation loss: 0.1114\t validation accuracy: 0.9711\n",
      "iteration number: 1513\t training loss: 0.1025\tvalidation loss: 0.1108\t validation accuracy: 0.9689\n",
      "iteration number: 1514\t training loss: 0.1028\tvalidation loss: 0.1105\t validation accuracy: 0.9689\n",
      "iteration number: 1515\t training loss: 0.1050\tvalidation loss: 0.1133\t validation accuracy: 0.9689\n",
      "iteration number: 1516\t training loss: 0.1024\tvalidation loss: 0.1098\t validation accuracy: 0.9689\n",
      "iteration number: 1517\t training loss: 0.1011\tvalidation loss: 0.1081\t validation accuracy: 0.9733\n",
      "iteration number: 1518\t training loss: 0.1008\tvalidation loss: 0.1078\t validation accuracy: 0.9711\n",
      "iteration number: 1519\t training loss: 0.1008\tvalidation loss: 0.1084\t validation accuracy: 0.9689\n",
      "iteration number: 1520\t training loss: 0.1013\tvalidation loss: 0.1078\t validation accuracy: 0.9711\n",
      "iteration number: 1521\t training loss: 0.1007\tvalidation loss: 0.1079\t validation accuracy: 0.9689\n",
      "iteration number: 1522\t training loss: 0.1018\tvalidation loss: 0.1082\t validation accuracy: 0.9711\n",
      "iteration number: 1523\t training loss: 0.1009\tvalidation loss: 0.1073\t validation accuracy: 0.9711\n",
      "iteration number: 1524\t training loss: 0.1003\tvalidation loss: 0.1064\t validation accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1525\t training loss: 0.1008\tvalidation loss: 0.1082\t validation accuracy: 0.9689\n",
      "iteration number: 1526\t training loss: 0.1010\tvalidation loss: 0.1060\t validation accuracy: 0.9711\n",
      "iteration number: 1527\t training loss: 0.0998\tvalidation loss: 0.1066\t validation accuracy: 0.9689\n",
      "iteration number: 1528\t training loss: 0.0997\tvalidation loss: 0.1060\t validation accuracy: 0.9689\n",
      "iteration number: 1529\t training loss: 0.1010\tvalidation loss: 0.1083\t validation accuracy: 0.9689\n",
      "iteration number: 1530\t training loss: 0.1018\tvalidation loss: 0.1095\t validation accuracy: 0.9667\n",
      "iteration number: 1531\t training loss: 0.1008\tvalidation loss: 0.1074\t validation accuracy: 0.9689\n",
      "iteration number: 1532\t training loss: 0.1007\tvalidation loss: 0.1066\t validation accuracy: 0.9711\n",
      "iteration number: 1533\t training loss: 0.1009\tvalidation loss: 0.1075\t validation accuracy: 0.9733\n",
      "iteration number: 1534\t training loss: 0.1014\tvalidation loss: 0.1078\t validation accuracy: 0.9733\n",
      "iteration number: 1535\t training loss: 0.1010\tvalidation loss: 0.1075\t validation accuracy: 0.9733\n",
      "iteration number: 1536\t training loss: 0.1016\tvalidation loss: 0.1067\t validation accuracy: 0.9733\n",
      "iteration number: 1537\t training loss: 0.1008\tvalidation loss: 0.1056\t validation accuracy: 0.9756\n",
      "iteration number: 1538\t training loss: 0.0996\tvalidation loss: 0.1051\t validation accuracy: 0.9778\n",
      "iteration number: 1539\t training loss: 0.0993\tvalidation loss: 0.1043\t validation accuracy: 0.9711\n",
      "iteration number: 1540\t training loss: 0.0996\tvalidation loss: 0.1039\t validation accuracy: 0.9711\n",
      "iteration number: 1541\t training loss: 0.1003\tvalidation loss: 0.1041\t validation accuracy: 0.9756\n",
      "iteration number: 1542\t training loss: 0.1003\tvalidation loss: 0.1056\t validation accuracy: 0.9711\n",
      "iteration number: 1543\t training loss: 0.1001\tvalidation loss: 0.1058\t validation accuracy: 0.9711\n",
      "iteration number: 1544\t training loss: 0.1010\tvalidation loss: 0.1058\t validation accuracy: 0.9667\n",
      "iteration number: 1545\t training loss: 0.1007\tvalidation loss: 0.1054\t validation accuracy: 0.9711\n",
      "iteration number: 1546\t training loss: 0.1002\tvalidation loss: 0.1047\t validation accuracy: 0.9733\n",
      "iteration number: 1547\t training loss: 0.0990\tvalidation loss: 0.1043\t validation accuracy: 0.9711\n",
      "iteration number: 1548\t training loss: 0.0995\tvalidation loss: 0.1038\t validation accuracy: 0.9711\n",
      "iteration number: 1549\t training loss: 0.0993\tvalidation loss: 0.1042\t validation accuracy: 0.9711\n",
      "iteration number: 1550\t training loss: 0.0993\tvalidation loss: 0.1036\t validation accuracy: 0.9733\n",
      "iteration number: 1551\t training loss: 0.1001\tvalidation loss: 0.1037\t validation accuracy: 0.9667\n",
      "iteration number: 1552\t training loss: 0.1005\tvalidation loss: 0.1038\t validation accuracy: 0.9667\n",
      "iteration number: 1553\t training loss: 0.0996\tvalidation loss: 0.1025\t validation accuracy: 0.9689\n",
      "iteration number: 1554\t training loss: 0.1000\tvalidation loss: 0.1030\t validation accuracy: 0.9667\n",
      "iteration number: 1555\t training loss: 0.1005\tvalidation loss: 0.1029\t validation accuracy: 0.9733\n",
      "iteration number: 1556\t training loss: 0.0999\tvalidation loss: 0.1025\t validation accuracy: 0.9733\n",
      "iteration number: 1557\t training loss: 0.1001\tvalidation loss: 0.1036\t validation accuracy: 0.9711\n",
      "iteration number: 1558\t training loss: 0.0992\tvalidation loss: 0.1031\t validation accuracy: 0.9756\n",
      "iteration number: 1559\t training loss: 0.1015\tvalidation loss: 0.1056\t validation accuracy: 0.9711\n",
      "iteration number: 1560\t training loss: 0.0994\tvalidation loss: 0.1042\t validation accuracy: 0.9733\n",
      "iteration number: 1561\t training loss: 0.1002\tvalidation loss: 0.1047\t validation accuracy: 0.9689\n",
      "iteration number: 1562\t training loss: 0.0996\tvalidation loss: 0.1031\t validation accuracy: 0.9733\n",
      "iteration number: 1563\t training loss: 0.0983\tvalidation loss: 0.1040\t validation accuracy: 0.9711\n",
      "iteration number: 1564\t training loss: 0.0982\tvalidation loss: 0.1030\t validation accuracy: 0.9711\n",
      "iteration number: 1565\t training loss: 0.0986\tvalidation loss: 0.1041\t validation accuracy: 0.9711\n",
      "iteration number: 1566\t training loss: 0.0979\tvalidation loss: 0.1027\t validation accuracy: 0.9756\n",
      "iteration number: 1567\t training loss: 0.0983\tvalidation loss: 0.1038\t validation accuracy: 0.9733\n",
      "iteration number: 1568\t training loss: 0.0980\tvalidation loss: 0.1035\t validation accuracy: 0.9689\n",
      "iteration number: 1569\t training loss: 0.0984\tvalidation loss: 0.1040\t validation accuracy: 0.9689\n",
      "iteration number: 1570\t training loss: 0.0987\tvalidation loss: 0.1053\t validation accuracy: 0.9711\n",
      "iteration number: 1571\t training loss: 0.0981\tvalidation loss: 0.1040\t validation accuracy: 0.9689\n",
      "iteration number: 1572\t training loss: 0.0982\tvalidation loss: 0.1053\t validation accuracy: 0.9689\n",
      "iteration number: 1573\t training loss: 0.0978\tvalidation loss: 0.1038\t validation accuracy: 0.9667\n",
      "iteration number: 1574\t training loss: 0.0982\tvalidation loss: 0.1033\t validation accuracy: 0.9689\n",
      "iteration number: 1575\t training loss: 0.0976\tvalidation loss: 0.1029\t validation accuracy: 0.9689\n",
      "iteration number: 1576\t training loss: 0.0970\tvalidation loss: 0.1031\t validation accuracy: 0.9689\n",
      "iteration number: 1577\t training loss: 0.0986\tvalidation loss: 0.1066\t validation accuracy: 0.9689\n",
      "iteration number: 1578\t training loss: 0.1003\tvalidation loss: 0.1097\t validation accuracy: 0.9667\n",
      "iteration number: 1579\t training loss: 0.1008\tvalidation loss: 0.1104\t validation accuracy: 0.9667\n",
      "iteration number: 1580\t training loss: 0.0975\tvalidation loss: 0.1052\t validation accuracy: 0.9711\n",
      "iteration number: 1581\t training loss: 0.0970\tvalidation loss: 0.1031\t validation accuracy: 0.9711\n",
      "iteration number: 1582\t training loss: 0.0975\tvalidation loss: 0.1037\t validation accuracy: 0.9689\n",
      "iteration number: 1583\t training loss: 0.0964\tvalidation loss: 0.1018\t validation accuracy: 0.9733\n",
      "iteration number: 1584\t training loss: 0.0964\tvalidation loss: 0.1031\t validation accuracy: 0.9711\n",
      "iteration number: 1585\t training loss: 0.0988\tvalidation loss: 0.1078\t validation accuracy: 0.9689\n",
      "iteration number: 1586\t training loss: 0.0997\tvalidation loss: 0.1075\t validation accuracy: 0.9644\n",
      "iteration number: 1587\t training loss: 0.0992\tvalidation loss: 0.1065\t validation accuracy: 0.9667\n",
      "iteration number: 1588\t training loss: 0.0980\tvalidation loss: 0.1064\t validation accuracy: 0.9689\n",
      "iteration number: 1589\t training loss: 0.0971\tvalidation loss: 0.1053\t validation accuracy: 0.9689\n",
      "iteration number: 1590\t training loss: 0.0970\tvalidation loss: 0.1037\t validation accuracy: 0.9711\n",
      "iteration number: 1591\t training loss: 0.0965\tvalidation loss: 0.1037\t validation accuracy: 0.9667\n",
      "iteration number: 1592\t training loss: 0.0969\tvalidation loss: 0.1045\t validation accuracy: 0.9689\n",
      "iteration number: 1593\t training loss: 0.0989\tvalidation loss: 0.1081\t validation accuracy: 0.9667\n",
      "iteration number: 1594\t training loss: 0.1001\tvalidation loss: 0.1103\t validation accuracy: 0.9667\n",
      "iteration number: 1595\t training loss: 0.0982\tvalidation loss: 0.1067\t validation accuracy: 0.9689\n",
      "iteration number: 1596\t training loss: 0.0977\tvalidation loss: 0.1055\t validation accuracy: 0.9689\n",
      "iteration number: 1597\t training loss: 0.0992\tvalidation loss: 0.1072\t validation accuracy: 0.9622\n",
      "iteration number: 1598\t training loss: 0.1000\tvalidation loss: 0.1074\t validation accuracy: 0.9644\n",
      "iteration number: 1599\t training loss: 0.0991\tvalidation loss: 0.1059\t validation accuracy: 0.9644\n",
      "iteration number: 1600\t training loss: 0.0988\tvalidation loss: 0.1055\t validation accuracy: 0.9644\n",
      "iteration number: 1601\t training loss: 0.0981\tvalidation loss: 0.1056\t validation accuracy: 0.9733\n",
      "iteration number: 1602\t training loss: 0.0974\tvalidation loss: 0.1053\t validation accuracy: 0.9711\n",
      "iteration number: 1603\t training loss: 0.0979\tvalidation loss: 0.1060\t validation accuracy: 0.9711\n",
      "iteration number: 1604\t training loss: 0.0967\tvalidation loss: 0.1036\t validation accuracy: 0.9711\n",
      "iteration number: 1605\t training loss: 0.0960\tvalidation loss: 0.1019\t validation accuracy: 0.9711\n",
      "iteration number: 1606\t training loss: 0.0960\tvalidation loss: 0.1019\t validation accuracy: 0.9689\n",
      "iteration number: 1607\t training loss: 0.0968\tvalidation loss: 0.1003\t validation accuracy: 0.9733\n",
      "iteration number: 1608\t training loss: 0.0955\tvalidation loss: 0.1025\t validation accuracy: 0.9711\n",
      "iteration number: 1609\t training loss: 0.0955\tvalidation loss: 0.1016\t validation accuracy: 0.9711\n",
      "iteration number: 1610\t training loss: 0.0951\tvalidation loss: 0.1014\t validation accuracy: 0.9711\n",
      "iteration number: 1611\t training loss: 0.0960\tvalidation loss: 0.1026\t validation accuracy: 0.9711\n",
      "iteration number: 1612\t training loss: 0.0957\tvalidation loss: 0.1027\t validation accuracy: 0.9689\n",
      "iteration number: 1613\t training loss: 0.0961\tvalidation loss: 0.1028\t validation accuracy: 0.9689\n",
      "iteration number: 1614\t training loss: 0.0956\tvalidation loss: 0.1023\t validation accuracy: 0.9689\n",
      "iteration number: 1615\t training loss: 0.0950\tvalidation loss: 0.1016\t validation accuracy: 0.9689\n",
      "iteration number: 1616\t training loss: 0.0948\tvalidation loss: 0.1015\t validation accuracy: 0.9711\n",
      "iteration number: 1617\t training loss: 0.0946\tvalidation loss: 0.1008\t validation accuracy: 0.9711\n",
      "iteration number: 1618\t training loss: 0.0952\tvalidation loss: 0.1016\t validation accuracy: 0.9733\n",
      "iteration number: 1619\t training loss: 0.0963\tvalidation loss: 0.1025\t validation accuracy: 0.9756\n",
      "iteration number: 1620\t training loss: 0.0967\tvalidation loss: 0.1014\t validation accuracy: 0.9778\n",
      "iteration number: 1621\t training loss: 0.0964\tvalidation loss: 0.1004\t validation accuracy: 0.9778\n",
      "iteration number: 1622\t training loss: 0.0969\tvalidation loss: 0.1008\t validation accuracy: 0.9778\n",
      "iteration number: 1623\t training loss: 0.0949\tvalidation loss: 0.1003\t validation accuracy: 0.9778\n",
      "iteration number: 1624\t training loss: 0.0946\tvalidation loss: 0.0998\t validation accuracy: 0.9756\n",
      "iteration number: 1625\t training loss: 0.0956\tvalidation loss: 0.1003\t validation accuracy: 0.9778\n",
      "iteration number: 1626\t training loss: 0.0949\tvalidation loss: 0.0998\t validation accuracy: 0.9778\n",
      "iteration number: 1627\t training loss: 0.0950\tvalidation loss: 0.0992\t validation accuracy: 0.9756\n",
      "iteration number: 1628\t training loss: 0.0953\tvalidation loss: 0.1000\t validation accuracy: 0.9800\n",
      "iteration number: 1629\t training loss: 0.0965\tvalidation loss: 0.1002\t validation accuracy: 0.9778\n",
      "iteration number: 1630\t training loss: 0.0957\tvalidation loss: 0.0995\t validation accuracy: 0.9800\n",
      "iteration number: 1631\t training loss: 0.0959\tvalidation loss: 0.0999\t validation accuracy: 0.9778\n",
      "iteration number: 1632\t training loss: 0.0959\tvalidation loss: 0.0997\t validation accuracy: 0.9711\n",
      "iteration number: 1633\t training loss: 0.0959\tvalidation loss: 0.1003\t validation accuracy: 0.9733\n",
      "iteration number: 1634\t training loss: 0.0952\tvalidation loss: 0.0997\t validation accuracy: 0.9756\n",
      "iteration number: 1635\t training loss: 0.0951\tvalidation loss: 0.0991\t validation accuracy: 0.9778\n",
      "iteration number: 1636\t training loss: 0.0953\tvalidation loss: 0.0986\t validation accuracy: 0.9733\n",
      "iteration number: 1637\t training loss: 0.0963\tvalidation loss: 0.0997\t validation accuracy: 0.9733\n",
      "iteration number: 1638\t training loss: 0.0968\tvalidation loss: 0.1002\t validation accuracy: 0.9711\n",
      "iteration number: 1639\t training loss: 0.0952\tvalidation loss: 0.0992\t validation accuracy: 0.9733\n",
      "iteration number: 1640\t training loss: 0.0976\tvalidation loss: 0.1002\t validation accuracy: 0.9689\n",
      "iteration number: 1641\t training loss: 0.0965\tvalidation loss: 0.0984\t validation accuracy: 0.9733\n",
      "iteration number: 1642\t training loss: 0.0970\tvalidation loss: 0.1001\t validation accuracy: 0.9667\n",
      "iteration number: 1643\t training loss: 0.0956\tvalidation loss: 0.0989\t validation accuracy: 0.9711\n",
      "iteration number: 1644\t training loss: 0.0951\tvalidation loss: 0.0990\t validation accuracy: 0.9711\n",
      "iteration number: 1645\t training loss: 0.0954\tvalidation loss: 0.0994\t validation accuracy: 0.9711\n",
      "iteration number: 1646\t training loss: 0.0951\tvalidation loss: 0.1008\t validation accuracy: 0.9733\n",
      "iteration number: 1647\t training loss: 0.0958\tvalidation loss: 0.1004\t validation accuracy: 0.9733\n",
      "iteration number: 1648\t training loss: 0.0963\tvalidation loss: 0.1010\t validation accuracy: 0.9733\n",
      "iteration number: 1649\t training loss: 0.0968\tvalidation loss: 0.1015\t validation accuracy: 0.9711\n",
      "iteration number: 1650\t training loss: 0.0974\tvalidation loss: 0.1024\t validation accuracy: 0.9689\n",
      "iteration number: 1651\t training loss: 0.0993\tvalidation loss: 0.1032\t validation accuracy: 0.9689\n",
      "iteration number: 1652\t training loss: 0.1003\tvalidation loss: 0.1031\t validation accuracy: 0.9711\n",
      "iteration number: 1653\t training loss: 0.0932\tvalidation loss: 0.0982\t validation accuracy: 0.9711\n",
      "iteration number: 1654\t training loss: 0.0936\tvalidation loss: 0.1000\t validation accuracy: 0.9689\n",
      "iteration number: 1655\t training loss: 0.0936\tvalidation loss: 0.0998\t validation accuracy: 0.9689\n",
      "iteration number: 1656\t training loss: 0.0933\tvalidation loss: 0.0996\t validation accuracy: 0.9689\n",
      "iteration number: 1657\t training loss: 0.0940\tvalidation loss: 0.1009\t validation accuracy: 0.9689\n",
      "iteration number: 1658\t training loss: 0.0934\tvalidation loss: 0.1001\t validation accuracy: 0.9689\n",
      "iteration number: 1659\t training loss: 0.0939\tvalidation loss: 0.0990\t validation accuracy: 0.9778\n",
      "iteration number: 1660\t training loss: 0.0929\tvalidation loss: 0.0991\t validation accuracy: 0.9733\n",
      "iteration number: 1661\t training loss: 0.0928\tvalidation loss: 0.0997\t validation accuracy: 0.9711\n",
      "iteration number: 1662\t training loss: 0.0940\tvalidation loss: 0.1004\t validation accuracy: 0.9711\n",
      "iteration number: 1663\t training loss: 0.0925\tvalidation loss: 0.0985\t validation accuracy: 0.9756\n",
      "iteration number: 1664\t training loss: 0.0924\tvalidation loss: 0.0986\t validation accuracy: 0.9778\n",
      "iteration number: 1665\t training loss: 0.0926\tvalidation loss: 0.0986\t validation accuracy: 0.9778\n",
      "iteration number: 1666\t training loss: 0.0925\tvalidation loss: 0.0982\t validation accuracy: 0.9800\n",
      "iteration number: 1667\t training loss: 0.0933\tvalidation loss: 0.0982\t validation accuracy: 0.9822\n",
      "iteration number: 1668\t training loss: 0.0935\tvalidation loss: 0.0995\t validation accuracy: 0.9822\n",
      "iteration number: 1669\t training loss: 0.0949\tvalidation loss: 0.1004\t validation accuracy: 0.9800\n",
      "iteration number: 1670\t training loss: 0.0939\tvalidation loss: 0.0978\t validation accuracy: 0.9800\n",
      "iteration number: 1671\t training loss: 0.0964\tvalidation loss: 0.1020\t validation accuracy: 0.9756\n",
      "iteration number: 1672\t training loss: 0.0961\tvalidation loss: 0.1032\t validation accuracy: 0.9711\n",
      "iteration number: 1673\t training loss: 0.1011\tvalidation loss: 0.1085\t validation accuracy: 0.9711\n",
      "iteration number: 1674\t training loss: 0.0990\tvalidation loss: 0.1057\t validation accuracy: 0.9733\n",
      "iteration number: 1675\t training loss: 0.1027\tvalidation loss: 0.1085\t validation accuracy: 0.9733\n",
      "iteration number: 1676\t training loss: 0.0974\tvalidation loss: 0.1019\t validation accuracy: 0.9733\n",
      "iteration number: 1677\t training loss: 0.0948\tvalidation loss: 0.1012\t validation accuracy: 0.9733\n",
      "iteration number: 1678\t training loss: 0.0956\tvalidation loss: 0.1019\t validation accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1679\t training loss: 0.0952\tvalidation loss: 0.1026\t validation accuracy: 0.9711\n",
      "iteration number: 1680\t training loss: 0.0948\tvalidation loss: 0.1011\t validation accuracy: 0.9711\n",
      "iteration number: 1681\t training loss: 0.0941\tvalidation loss: 0.0988\t validation accuracy: 0.9778\n",
      "iteration number: 1682\t training loss: 0.0937\tvalidation loss: 0.0978\t validation accuracy: 0.9756\n",
      "iteration number: 1683\t training loss: 0.0932\tvalidation loss: 0.0968\t validation accuracy: 0.9778\n",
      "iteration number: 1684\t training loss: 0.0925\tvalidation loss: 0.0971\t validation accuracy: 0.9756\n",
      "iteration number: 1685\t training loss: 0.0923\tvalidation loss: 0.0968\t validation accuracy: 0.9733\n",
      "iteration number: 1686\t training loss: 0.0926\tvalidation loss: 0.0983\t validation accuracy: 0.9756\n",
      "iteration number: 1687\t training loss: 0.0948\tvalidation loss: 0.1014\t validation accuracy: 0.9711\n",
      "iteration number: 1688\t training loss: 0.0933\tvalidation loss: 0.0999\t validation accuracy: 0.9733\n",
      "iteration number: 1689\t training loss: 0.0928\tvalidation loss: 0.0992\t validation accuracy: 0.9756\n",
      "iteration number: 1690\t training loss: 0.0939\tvalidation loss: 0.1014\t validation accuracy: 0.9667\n",
      "iteration number: 1691\t training loss: 0.0937\tvalidation loss: 0.1009\t validation accuracy: 0.9689\n",
      "iteration number: 1692\t training loss: 0.0929\tvalidation loss: 0.0993\t validation accuracy: 0.9733\n",
      "iteration number: 1693\t training loss: 0.0934\tvalidation loss: 0.1006\t validation accuracy: 0.9711\n",
      "iteration number: 1694\t training loss: 0.0956\tvalidation loss: 0.1002\t validation accuracy: 0.9711\n",
      "iteration number: 1695\t training loss: 0.0951\tvalidation loss: 0.0998\t validation accuracy: 0.9756\n",
      "iteration number: 1696\t training loss: 0.0936\tvalidation loss: 0.0989\t validation accuracy: 0.9756\n",
      "iteration number: 1697\t training loss: 0.0922\tvalidation loss: 0.0979\t validation accuracy: 0.9756\n",
      "iteration number: 1698\t training loss: 0.0919\tvalidation loss: 0.0979\t validation accuracy: 0.9756\n",
      "iteration number: 1699\t training loss: 0.0927\tvalidation loss: 0.0984\t validation accuracy: 0.9778\n",
      "iteration number: 1700\t training loss: 0.0921\tvalidation loss: 0.0983\t validation accuracy: 0.9778\n",
      "iteration number: 1701\t training loss: 0.0912\tvalidation loss: 0.0971\t validation accuracy: 0.9778\n",
      "iteration number: 1702\t training loss: 0.0918\tvalidation loss: 0.0985\t validation accuracy: 0.9756\n",
      "iteration number: 1703\t training loss: 0.0911\tvalidation loss: 0.0981\t validation accuracy: 0.9711\n",
      "iteration number: 1704\t training loss: 0.0918\tvalidation loss: 0.0994\t validation accuracy: 0.9667\n",
      "iteration number: 1705\t training loss: 0.0917\tvalidation loss: 0.0993\t validation accuracy: 0.9689\n",
      "iteration number: 1706\t training loss: 0.0904\tvalidation loss: 0.0967\t validation accuracy: 0.9778\n",
      "iteration number: 1707\t training loss: 0.0928\tvalidation loss: 0.1003\t validation accuracy: 0.9689\n",
      "iteration number: 1708\t training loss: 0.0969\tvalidation loss: 0.1031\t validation accuracy: 0.9667\n",
      "iteration number: 1709\t training loss: 0.0941\tvalidation loss: 0.0996\t validation accuracy: 0.9667\n",
      "iteration number: 1710\t training loss: 0.0907\tvalidation loss: 0.0963\t validation accuracy: 0.9756\n",
      "iteration number: 1711\t training loss: 0.0903\tvalidation loss: 0.0966\t validation accuracy: 0.9733\n",
      "iteration number: 1712\t training loss: 0.0901\tvalidation loss: 0.0956\t validation accuracy: 0.9756\n",
      "iteration number: 1713\t training loss: 0.0907\tvalidation loss: 0.0963\t validation accuracy: 0.9778\n",
      "iteration number: 1714\t training loss: 0.0917\tvalidation loss: 0.0970\t validation accuracy: 0.9822\n",
      "iteration number: 1715\t training loss: 0.0919\tvalidation loss: 0.0971\t validation accuracy: 0.9733\n",
      "iteration number: 1716\t training loss: 0.0910\tvalidation loss: 0.0965\t validation accuracy: 0.9778\n",
      "iteration number: 1717\t training loss: 0.0902\tvalidation loss: 0.0962\t validation accuracy: 0.9756\n",
      "iteration number: 1718\t training loss: 0.0914\tvalidation loss: 0.0965\t validation accuracy: 0.9778\n",
      "iteration number: 1719\t training loss: 0.0905\tvalidation loss: 0.0949\t validation accuracy: 0.9822\n",
      "iteration number: 1720\t training loss: 0.0900\tvalidation loss: 0.0940\t validation accuracy: 0.9800\n",
      "iteration number: 1721\t training loss: 0.0903\tvalidation loss: 0.0941\t validation accuracy: 0.9822\n",
      "iteration number: 1722\t training loss: 0.0907\tvalidation loss: 0.0942\t validation accuracy: 0.9822\n",
      "iteration number: 1723\t training loss: 0.0916\tvalidation loss: 0.0951\t validation accuracy: 0.9822\n",
      "iteration number: 1724\t training loss: 0.0917\tvalidation loss: 0.0944\t validation accuracy: 0.9800\n",
      "iteration number: 1725\t training loss: 0.0905\tvalidation loss: 0.0938\t validation accuracy: 0.9778\n",
      "iteration number: 1726\t training loss: 0.0899\tvalidation loss: 0.0958\t validation accuracy: 0.9756\n",
      "iteration number: 1727\t training loss: 0.0914\tvalidation loss: 0.0973\t validation accuracy: 0.9711\n",
      "iteration number: 1728\t training loss: 0.0917\tvalidation loss: 0.0975\t validation accuracy: 0.9711\n",
      "iteration number: 1729\t training loss: 0.0906\tvalidation loss: 0.0975\t validation accuracy: 0.9711\n",
      "iteration number: 1730\t training loss: 0.0895\tvalidation loss: 0.0961\t validation accuracy: 0.9711\n",
      "iteration number: 1731\t training loss: 0.0896\tvalidation loss: 0.0963\t validation accuracy: 0.9711\n",
      "iteration number: 1732\t training loss: 0.0894\tvalidation loss: 0.0961\t validation accuracy: 0.9711\n",
      "iteration number: 1733\t training loss: 0.0899\tvalidation loss: 0.0970\t validation accuracy: 0.9689\n",
      "iteration number: 1734\t training loss: 0.0899\tvalidation loss: 0.0969\t validation accuracy: 0.9711\n",
      "iteration number: 1735\t training loss: 0.0902\tvalidation loss: 0.0986\t validation accuracy: 0.9689\n",
      "iteration number: 1736\t training loss: 0.0903\tvalidation loss: 0.0979\t validation accuracy: 0.9733\n",
      "iteration number: 1737\t training loss: 0.0899\tvalidation loss: 0.0978\t validation accuracy: 0.9756\n",
      "iteration number: 1738\t training loss: 0.0891\tvalidation loss: 0.0975\t validation accuracy: 0.9711\n",
      "iteration number: 1739\t training loss: 0.0911\tvalidation loss: 0.0986\t validation accuracy: 0.9711\n",
      "iteration number: 1740\t training loss: 0.0894\tvalidation loss: 0.0970\t validation accuracy: 0.9711\n",
      "iteration number: 1741\t training loss: 0.0923\tvalidation loss: 0.0996\t validation accuracy: 0.9711\n",
      "iteration number: 1742\t training loss: 0.0920\tvalidation loss: 0.1006\t validation accuracy: 0.9689\n",
      "iteration number: 1743\t training loss: 0.0931\tvalidation loss: 0.1013\t validation accuracy: 0.9711\n",
      "iteration number: 1744\t training loss: 0.0927\tvalidation loss: 0.1023\t validation accuracy: 0.9667\n",
      "iteration number: 1745\t training loss: 0.0907\tvalidation loss: 0.1001\t validation accuracy: 0.9733\n",
      "iteration number: 1746\t training loss: 0.0907\tvalidation loss: 0.1000\t validation accuracy: 0.9711\n",
      "iteration number: 1747\t training loss: 0.0897\tvalidation loss: 0.0975\t validation accuracy: 0.9733\n",
      "iteration number: 1748\t training loss: 0.0895\tvalidation loss: 0.0970\t validation accuracy: 0.9778\n",
      "iteration number: 1749\t training loss: 0.0889\tvalidation loss: 0.0960\t validation accuracy: 0.9778\n",
      "iteration number: 1750\t training loss: 0.0886\tvalidation loss: 0.0955\t validation accuracy: 0.9778\n",
      "iteration number: 1751\t training loss: 0.0890\tvalidation loss: 0.0969\t validation accuracy: 0.9733\n",
      "iteration number: 1752\t training loss: 0.0893\tvalidation loss: 0.0964\t validation accuracy: 0.9756\n",
      "iteration number: 1753\t training loss: 0.0914\tvalidation loss: 0.0970\t validation accuracy: 0.9689\n",
      "iteration number: 1754\t training loss: 0.0892\tvalidation loss: 0.0978\t validation accuracy: 0.9711\n",
      "iteration number: 1755\t training loss: 0.0892\tvalidation loss: 0.0970\t validation accuracy: 0.9733\n",
      "iteration number: 1756\t training loss: 0.0901\tvalidation loss: 0.0977\t validation accuracy: 0.9733\n",
      "iteration number: 1757\t training loss: 0.0895\tvalidation loss: 0.0974\t validation accuracy: 0.9733\n",
      "iteration number: 1758\t training loss: 0.0886\tvalidation loss: 0.0967\t validation accuracy: 0.9733\n",
      "iteration number: 1759\t training loss: 0.0896\tvalidation loss: 0.0974\t validation accuracy: 0.9733\n",
      "iteration number: 1760\t training loss: 0.0890\tvalidation loss: 0.0966\t validation accuracy: 0.9756\n",
      "iteration number: 1761\t training loss: 0.0907\tvalidation loss: 0.0975\t validation accuracy: 0.9711\n",
      "iteration number: 1762\t training loss: 0.0907\tvalidation loss: 0.0959\t validation accuracy: 0.9711\n",
      "iteration number: 1763\t training loss: 0.0886\tvalidation loss: 0.0935\t validation accuracy: 0.9800\n",
      "iteration number: 1764\t training loss: 0.0885\tvalidation loss: 0.0937\t validation accuracy: 0.9800\n",
      "iteration number: 1765\t training loss: 0.0891\tvalidation loss: 0.0949\t validation accuracy: 0.9756\n",
      "iteration number: 1766\t training loss: 0.0901\tvalidation loss: 0.0962\t validation accuracy: 0.9756\n",
      "iteration number: 1767\t training loss: 0.0960\tvalidation loss: 0.1015\t validation accuracy: 0.9667\n",
      "iteration number: 1768\t training loss: 0.0941\tvalidation loss: 0.1003\t validation accuracy: 0.9689\n",
      "iteration number: 1769\t training loss: 0.0893\tvalidation loss: 0.0966\t validation accuracy: 0.9756\n",
      "iteration number: 1770\t training loss: 0.0882\tvalidation loss: 0.0972\t validation accuracy: 0.9778\n",
      "iteration number: 1771\t training loss: 0.0886\tvalidation loss: 0.0978\t validation accuracy: 0.9711\n",
      "iteration number: 1772\t training loss: 0.0886\tvalidation loss: 0.0973\t validation accuracy: 0.9711\n",
      "iteration number: 1773\t training loss: 0.0889\tvalidation loss: 0.0982\t validation accuracy: 0.9711\n",
      "iteration number: 1774\t training loss: 0.0891\tvalidation loss: 0.0970\t validation accuracy: 0.9778\n",
      "iteration number: 1775\t training loss: 0.0881\tvalidation loss: 0.0968\t validation accuracy: 0.9778\n",
      "iteration number: 1776\t training loss: 0.0880\tvalidation loss: 0.0975\t validation accuracy: 0.9733\n",
      "iteration number: 1777\t training loss: 0.0880\tvalidation loss: 0.0969\t validation accuracy: 0.9733\n",
      "iteration number: 1778\t training loss: 0.0876\tvalidation loss: 0.0960\t validation accuracy: 0.9756\n",
      "iteration number: 1779\t training loss: 0.0879\tvalidation loss: 0.0967\t validation accuracy: 0.9711\n",
      "iteration number: 1780\t training loss: 0.0877\tvalidation loss: 0.0963\t validation accuracy: 0.9733\n",
      "iteration number: 1781\t training loss: 0.0907\tvalidation loss: 0.1006\t validation accuracy: 0.9689\n",
      "iteration number: 1782\t training loss: 0.0889\tvalidation loss: 0.0990\t validation accuracy: 0.9711\n",
      "iteration number: 1783\t training loss: 0.0882\tvalidation loss: 0.0980\t validation accuracy: 0.9711\n",
      "iteration number: 1784\t training loss: 0.0903\tvalidation loss: 0.1002\t validation accuracy: 0.9689\n",
      "iteration number: 1785\t training loss: 0.0936\tvalidation loss: 0.1019\t validation accuracy: 0.9689\n",
      "iteration number: 1786\t training loss: 0.0890\tvalidation loss: 0.1003\t validation accuracy: 0.9711\n",
      "iteration number: 1787\t training loss: 0.0885\tvalidation loss: 0.0988\t validation accuracy: 0.9756\n",
      "iteration number: 1788\t training loss: 0.0875\tvalidation loss: 0.0959\t validation accuracy: 0.9756\n",
      "iteration number: 1789\t training loss: 0.0869\tvalidation loss: 0.0950\t validation accuracy: 0.9778\n",
      "iteration number: 1790\t training loss: 0.0874\tvalidation loss: 0.0947\t validation accuracy: 0.9756\n",
      "iteration number: 1791\t training loss: 0.0865\tvalidation loss: 0.0939\t validation accuracy: 0.9778\n",
      "iteration number: 1792\t training loss: 0.0881\tvalidation loss: 0.0957\t validation accuracy: 0.9733\n",
      "iteration number: 1793\t training loss: 0.0867\tvalidation loss: 0.0945\t validation accuracy: 0.9756\n",
      "iteration number: 1794\t training loss: 0.0865\tvalidation loss: 0.0938\t validation accuracy: 0.9756\n",
      "iteration number: 1795\t training loss: 0.0879\tvalidation loss: 0.0950\t validation accuracy: 0.9711\n",
      "iteration number: 1796\t training loss: 0.0882\tvalidation loss: 0.0946\t validation accuracy: 0.9733\n",
      "iteration number: 1797\t training loss: 0.0872\tvalidation loss: 0.0946\t validation accuracy: 0.9733\n",
      "iteration number: 1798\t training loss: 0.0875\tvalidation loss: 0.0948\t validation accuracy: 0.9733\n",
      "iteration number: 1799\t training loss: 0.0868\tvalidation loss: 0.0954\t validation accuracy: 0.9756\n",
      "iteration number: 1800\t training loss: 0.0872\tvalidation loss: 0.0964\t validation accuracy: 0.9711\n",
      "iteration number: 1801\t training loss: 0.0870\tvalidation loss: 0.0969\t validation accuracy: 0.9711\n",
      "iteration number: 1802\t training loss: 0.0870\tvalidation loss: 0.0964\t validation accuracy: 0.9733\n",
      "iteration number: 1803\t training loss: 0.0869\tvalidation loss: 0.0960\t validation accuracy: 0.9711\n",
      "iteration number: 1804\t training loss: 0.0869\tvalidation loss: 0.0963\t validation accuracy: 0.9711\n",
      "iteration number: 1805\t training loss: 0.0878\tvalidation loss: 0.0979\t validation accuracy: 0.9711\n",
      "iteration number: 1806\t training loss: 0.0882\tvalidation loss: 0.0982\t validation accuracy: 0.9689\n",
      "iteration number: 1807\t training loss: 0.0871\tvalidation loss: 0.0964\t validation accuracy: 0.9689\n",
      "iteration number: 1808\t training loss: 0.0869\tvalidation loss: 0.0954\t validation accuracy: 0.9711\n",
      "iteration number: 1809\t training loss: 0.0869\tvalidation loss: 0.0957\t validation accuracy: 0.9733\n",
      "iteration number: 1810\t training loss: 0.0876\tvalidation loss: 0.0959\t validation accuracy: 0.9711\n",
      "iteration number: 1811\t training loss: 0.0894\tvalidation loss: 0.0995\t validation accuracy: 0.9711\n",
      "iteration number: 1812\t training loss: 0.0875\tvalidation loss: 0.0964\t validation accuracy: 0.9711\n",
      "iteration number: 1813\t training loss: 0.0874\tvalidation loss: 0.0966\t validation accuracy: 0.9711\n",
      "iteration number: 1814\t training loss: 0.0873\tvalidation loss: 0.0970\t validation accuracy: 0.9711\n",
      "iteration number: 1815\t training loss: 0.0871\tvalidation loss: 0.0967\t validation accuracy: 0.9711\n",
      "iteration number: 1816\t training loss: 0.0877\tvalidation loss: 0.0959\t validation accuracy: 0.9733\n",
      "iteration number: 1817\t training loss: 0.0896\tvalidation loss: 0.0973\t validation accuracy: 0.9711\n",
      "iteration number: 1818\t training loss: 0.0874\tvalidation loss: 0.0946\t validation accuracy: 0.9778\n",
      "iteration number: 1819\t training loss: 0.0870\tvalidation loss: 0.0945\t validation accuracy: 0.9778\n",
      "iteration number: 1820\t training loss: 0.0869\tvalidation loss: 0.0946\t validation accuracy: 0.9778\n",
      "iteration number: 1821\t training loss: 0.0894\tvalidation loss: 0.0965\t validation accuracy: 0.9733\n",
      "iteration number: 1822\t training loss: 0.0910\tvalidation loss: 0.0976\t validation accuracy: 0.9733\n",
      "iteration number: 1823\t training loss: 0.0859\tvalidation loss: 0.0937\t validation accuracy: 0.9778\n",
      "iteration number: 1824\t training loss: 0.0859\tvalidation loss: 0.0924\t validation accuracy: 0.9800\n",
      "iteration number: 1825\t training loss: 0.0854\tvalidation loss: 0.0933\t validation accuracy: 0.9800\n",
      "iteration number: 1826\t training loss: 0.0853\tvalidation loss: 0.0923\t validation accuracy: 0.9800\n",
      "iteration number: 1827\t training loss: 0.0860\tvalidation loss: 0.0914\t validation accuracy: 0.9800\n",
      "iteration number: 1828\t training loss: 0.0859\tvalidation loss: 0.0917\t validation accuracy: 0.9800\n",
      "iteration number: 1829\t training loss: 0.0881\tvalidation loss: 0.0927\t validation accuracy: 0.9822\n",
      "iteration number: 1830\t training loss: 0.0857\tvalidation loss: 0.0933\t validation accuracy: 0.9778\n",
      "iteration number: 1831\t training loss: 0.0868\tvalidation loss: 0.0923\t validation accuracy: 0.9800\n",
      "iteration number: 1832\t training loss: 0.0885\tvalidation loss: 0.0933\t validation accuracy: 0.9822\n",
      "iteration number: 1833\t training loss: 0.0883\tvalidation loss: 0.0927\t validation accuracy: 0.9800\n",
      "iteration number: 1834\t training loss: 0.0890\tvalidation loss: 0.0942\t validation accuracy: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1835\t training loss: 0.0879\tvalidation loss: 0.0941\t validation accuracy: 0.9800\n",
      "iteration number: 1836\t training loss: 0.0900\tvalidation loss: 0.0974\t validation accuracy: 0.9756\n",
      "iteration number: 1837\t training loss: 0.0860\tvalidation loss: 0.0955\t validation accuracy: 0.9778\n",
      "iteration number: 1838\t training loss: 0.0853\tvalidation loss: 0.0947\t validation accuracy: 0.9778\n",
      "iteration number: 1839\t training loss: 0.0854\tvalidation loss: 0.0944\t validation accuracy: 0.9733\n",
      "iteration number: 1840\t training loss: 0.0857\tvalidation loss: 0.0930\t validation accuracy: 0.9733\n",
      "iteration number: 1841\t training loss: 0.0862\tvalidation loss: 0.0922\t validation accuracy: 0.9778\n",
      "iteration number: 1842\t training loss: 0.0858\tvalidation loss: 0.0919\t validation accuracy: 0.9800\n",
      "iteration number: 1843\t training loss: 0.0848\tvalidation loss: 0.0923\t validation accuracy: 0.9800\n",
      "iteration number: 1844\t training loss: 0.0843\tvalidation loss: 0.0918\t validation accuracy: 0.9778\n",
      "iteration number: 1845\t training loss: 0.0848\tvalidation loss: 0.0921\t validation accuracy: 0.9756\n",
      "iteration number: 1846\t training loss: 0.0860\tvalidation loss: 0.0927\t validation accuracy: 0.9711\n",
      "iteration number: 1847\t training loss: 0.0855\tvalidation loss: 0.0925\t validation accuracy: 0.9756\n",
      "iteration number: 1848\t training loss: 0.0848\tvalidation loss: 0.0912\t validation accuracy: 0.9778\n",
      "iteration number: 1849\t training loss: 0.0846\tvalidation loss: 0.0905\t validation accuracy: 0.9778\n",
      "iteration number: 1850\t training loss: 0.0848\tvalidation loss: 0.0906\t validation accuracy: 0.9778\n",
      "iteration number: 1851\t training loss: 0.0848\tvalidation loss: 0.0919\t validation accuracy: 0.9778\n",
      "iteration number: 1852\t training loss: 0.0865\tvalidation loss: 0.0934\t validation accuracy: 0.9778\n",
      "iteration number: 1853\t training loss: 0.0860\tvalidation loss: 0.0923\t validation accuracy: 0.9822\n",
      "iteration number: 1854\t training loss: 0.0862\tvalidation loss: 0.0923\t validation accuracy: 0.9822\n",
      "iteration number: 1855\t training loss: 0.0839\tvalidation loss: 0.0912\t validation accuracy: 0.9778\n",
      "iteration number: 1856\t training loss: 0.0839\tvalidation loss: 0.0915\t validation accuracy: 0.9800\n",
      "iteration number: 1857\t training loss: 0.0841\tvalidation loss: 0.0909\t validation accuracy: 0.9800\n",
      "iteration number: 1858\t training loss: 0.0841\tvalidation loss: 0.0916\t validation accuracy: 0.9756\n",
      "iteration number: 1859\t training loss: 0.0851\tvalidation loss: 0.0902\t validation accuracy: 0.9800\n",
      "iteration number: 1860\t training loss: 0.0841\tvalidation loss: 0.0900\t validation accuracy: 0.9778\n",
      "iteration number: 1861\t training loss: 0.0841\tvalidation loss: 0.0912\t validation accuracy: 0.9778\n",
      "iteration number: 1862\t training loss: 0.0851\tvalidation loss: 0.0943\t validation accuracy: 0.9711\n",
      "iteration number: 1863\t training loss: 0.0852\tvalidation loss: 0.0947\t validation accuracy: 0.9711\n",
      "iteration number: 1864\t training loss: 0.0854\tvalidation loss: 0.0951\t validation accuracy: 0.9711\n",
      "iteration number: 1865\t training loss: 0.0852\tvalidation loss: 0.0940\t validation accuracy: 0.9711\n",
      "iteration number: 1866\t training loss: 0.0845\tvalidation loss: 0.0939\t validation accuracy: 0.9733\n",
      "iteration number: 1867\t training loss: 0.0846\tvalidation loss: 0.0935\t validation accuracy: 0.9733\n",
      "iteration number: 1868\t training loss: 0.0854\tvalidation loss: 0.0951\t validation accuracy: 0.9711\n",
      "iteration number: 1869\t training loss: 0.0842\tvalidation loss: 0.0925\t validation accuracy: 0.9756\n",
      "iteration number: 1870\t training loss: 0.0843\tvalidation loss: 0.0925\t validation accuracy: 0.9756\n",
      "iteration number: 1871\t training loss: 0.0842\tvalidation loss: 0.0918\t validation accuracy: 0.9756\n",
      "iteration number: 1872\t training loss: 0.0836\tvalidation loss: 0.0913\t validation accuracy: 0.9756\n",
      "iteration number: 1873\t training loss: 0.0840\tvalidation loss: 0.0923\t validation accuracy: 0.9756\n",
      "iteration number: 1874\t training loss: 0.0836\tvalidation loss: 0.0924\t validation accuracy: 0.9733\n",
      "iteration number: 1875\t training loss: 0.0833\tvalidation loss: 0.0920\t validation accuracy: 0.9756\n",
      "iteration number: 1876\t training loss: 0.0836\tvalidation loss: 0.0928\t validation accuracy: 0.9733\n",
      "iteration number: 1877\t training loss: 0.0841\tvalidation loss: 0.0932\t validation accuracy: 0.9711\n",
      "iteration number: 1878\t training loss: 0.0848\tvalidation loss: 0.0944\t validation accuracy: 0.9689\n",
      "iteration number: 1879\t training loss: 0.0838\tvalidation loss: 0.0930\t validation accuracy: 0.9711\n",
      "iteration number: 1880\t training loss: 0.0843\tvalidation loss: 0.0922\t validation accuracy: 0.9733\n",
      "iteration number: 1881\t training loss: 0.0842\tvalidation loss: 0.0936\t validation accuracy: 0.9733\n",
      "iteration number: 1882\t training loss: 0.0856\tvalidation loss: 0.0957\t validation accuracy: 0.9711\n",
      "iteration number: 1883\t training loss: 0.0837\tvalidation loss: 0.0941\t validation accuracy: 0.9711\n",
      "iteration number: 1884\t training loss: 0.0832\tvalidation loss: 0.0924\t validation accuracy: 0.9733\n",
      "iteration number: 1885\t training loss: 0.0829\tvalidation loss: 0.0918\t validation accuracy: 0.9756\n",
      "iteration number: 1886\t training loss: 0.0840\tvalidation loss: 0.0945\t validation accuracy: 0.9711\n",
      "iteration number: 1887\t training loss: 0.0838\tvalidation loss: 0.0932\t validation accuracy: 0.9778\n",
      "iteration number: 1888\t training loss: 0.0841\tvalidation loss: 0.0937\t validation accuracy: 0.9778\n",
      "iteration number: 1889\t training loss: 0.0840\tvalidation loss: 0.0934\t validation accuracy: 0.9778\n",
      "iteration number: 1890\t training loss: 0.0832\tvalidation loss: 0.0917\t validation accuracy: 0.9778\n",
      "iteration number: 1891\t training loss: 0.0831\tvalidation loss: 0.0909\t validation accuracy: 0.9778\n",
      "iteration number: 1892\t training loss: 0.0844\tvalidation loss: 0.0920\t validation accuracy: 0.9778\n",
      "iteration number: 1893\t training loss: 0.0850\tvalidation loss: 0.0933\t validation accuracy: 0.9756\n",
      "iteration number: 1894\t training loss: 0.0858\tvalidation loss: 0.0949\t validation accuracy: 0.9711\n",
      "iteration number: 1895\t training loss: 0.0854\tvalidation loss: 0.0935\t validation accuracy: 0.9711\n",
      "iteration number: 1896\t training loss: 0.0855\tvalidation loss: 0.0933\t validation accuracy: 0.9711\n",
      "iteration number: 1897\t training loss: 0.0850\tvalidation loss: 0.0926\t validation accuracy: 0.9756\n",
      "iteration number: 1898\t training loss: 0.0856\tvalidation loss: 0.0927\t validation accuracy: 0.9778\n",
      "iteration number: 1899\t training loss: 0.0851\tvalidation loss: 0.0918\t validation accuracy: 0.9778\n",
      "iteration number: 1900\t training loss: 0.0844\tvalidation loss: 0.0908\t validation accuracy: 0.9800\n",
      "iteration number: 1901\t training loss: 0.0858\tvalidation loss: 0.0914\t validation accuracy: 0.9800\n",
      "iteration number: 1902\t training loss: 0.0881\tvalidation loss: 0.0934\t validation accuracy: 0.9778\n",
      "iteration number: 1903\t training loss: 0.0855\tvalidation loss: 0.0915\t validation accuracy: 0.9800\n",
      "iteration number: 1904\t training loss: 0.0852\tvalidation loss: 0.0914\t validation accuracy: 0.9756\n",
      "iteration number: 1905\t training loss: 0.0850\tvalidation loss: 0.0907\t validation accuracy: 0.9756\n",
      "iteration number: 1906\t training loss: 0.0824\tvalidation loss: 0.0902\t validation accuracy: 0.9778\n",
      "iteration number: 1907\t training loss: 0.0827\tvalidation loss: 0.0906\t validation accuracy: 0.9756\n",
      "iteration number: 1908\t training loss: 0.0826\tvalidation loss: 0.0904\t validation accuracy: 0.9756\n",
      "iteration number: 1909\t training loss: 0.0827\tvalidation loss: 0.0902\t validation accuracy: 0.9756\n",
      "iteration number: 1910\t training loss: 0.0825\tvalidation loss: 0.0904\t validation accuracy: 0.9778\n",
      "iteration number: 1911\t training loss: 0.0855\tvalidation loss: 0.0924\t validation accuracy: 0.9733\n",
      "iteration number: 1912\t training loss: 0.0844\tvalidation loss: 0.0949\t validation accuracy: 0.9689\n",
      "iteration number: 1913\t training loss: 0.0833\tvalidation loss: 0.0943\t validation accuracy: 0.9711\n",
      "iteration number: 1914\t training loss: 0.0823\tvalidation loss: 0.0931\t validation accuracy: 0.9778\n",
      "iteration number: 1915\t training loss: 0.0823\tvalidation loss: 0.0931\t validation accuracy: 0.9778\n",
      "iteration number: 1916\t training loss: 0.0819\tvalidation loss: 0.0920\t validation accuracy: 0.9756\n",
      "iteration number: 1917\t training loss: 0.0821\tvalidation loss: 0.0916\t validation accuracy: 0.9756\n",
      "iteration number: 1918\t training loss: 0.0833\tvalidation loss: 0.0932\t validation accuracy: 0.9711\n",
      "iteration number: 1919\t training loss: 0.0858\tvalidation loss: 0.0964\t validation accuracy: 0.9689\n",
      "iteration number: 1920\t training loss: 0.0838\tvalidation loss: 0.0941\t validation accuracy: 0.9711\n",
      "iteration number: 1921\t training loss: 0.0828\tvalidation loss: 0.0924\t validation accuracy: 0.9756\n",
      "iteration number: 1922\t training loss: 0.0830\tvalidation loss: 0.0932\t validation accuracy: 0.9733\n",
      "iteration number: 1923\t training loss: 0.0839\tvalidation loss: 0.0939\t validation accuracy: 0.9711\n",
      "iteration number: 1924\t training loss: 0.0836\tvalidation loss: 0.0929\t validation accuracy: 0.9733\n",
      "iteration number: 1925\t training loss: 0.0827\tvalidation loss: 0.0920\t validation accuracy: 0.9756\n",
      "iteration number: 1926\t training loss: 0.0821\tvalidation loss: 0.0926\t validation accuracy: 0.9733\n",
      "iteration number: 1927\t training loss: 0.0819\tvalidation loss: 0.0922\t validation accuracy: 0.9733\n",
      "iteration number: 1928\t training loss: 0.0815\tvalidation loss: 0.0909\t validation accuracy: 0.9756\n",
      "iteration number: 1929\t training loss: 0.0817\tvalidation loss: 0.0902\t validation accuracy: 0.9756\n",
      "iteration number: 1930\t training loss: 0.0813\tvalidation loss: 0.0906\t validation accuracy: 0.9778\n",
      "iteration number: 1931\t training loss: 0.0811\tvalidation loss: 0.0911\t validation accuracy: 0.9733\n",
      "iteration number: 1932\t training loss: 0.0819\tvalidation loss: 0.0925\t validation accuracy: 0.9733\n",
      "iteration number: 1933\t training loss: 0.0816\tvalidation loss: 0.0919\t validation accuracy: 0.9756\n",
      "iteration number: 1934\t training loss: 0.0821\tvalidation loss: 0.0941\t validation accuracy: 0.9711\n",
      "iteration number: 1935\t training loss: 0.0834\tvalidation loss: 0.0963\t validation accuracy: 0.9711\n",
      "iteration number: 1936\t training loss: 0.0819\tvalidation loss: 0.0939\t validation accuracy: 0.9733\n",
      "iteration number: 1937\t training loss: 0.0812\tvalidation loss: 0.0925\t validation accuracy: 0.9756\n",
      "iteration number: 1938\t training loss: 0.0820\tvalidation loss: 0.0915\t validation accuracy: 0.9778\n",
      "iteration number: 1939\t training loss: 0.0818\tvalidation loss: 0.0916\t validation accuracy: 0.9800\n",
      "iteration number: 1940\t training loss: 0.0810\tvalidation loss: 0.0930\t validation accuracy: 0.9733\n",
      "iteration number: 1941\t training loss: 0.0824\tvalidation loss: 0.0944\t validation accuracy: 0.9756\n",
      "iteration number: 1942\t training loss: 0.0819\tvalidation loss: 0.0929\t validation accuracy: 0.9733\n",
      "iteration number: 1943\t training loss: 0.0843\tvalidation loss: 0.0922\t validation accuracy: 0.9733\n",
      "iteration number: 1944\t training loss: 0.0811\tvalidation loss: 0.0908\t validation accuracy: 0.9778\n",
      "iteration number: 1945\t training loss: 0.0808\tvalidation loss: 0.0903\t validation accuracy: 0.9756\n",
      "iteration number: 1946\t training loss: 0.0808\tvalidation loss: 0.0897\t validation accuracy: 0.9756\n",
      "iteration number: 1947\t training loss: 0.0808\tvalidation loss: 0.0905\t validation accuracy: 0.9778\n",
      "iteration number: 1948\t training loss: 0.0807\tvalidation loss: 0.0897\t validation accuracy: 0.9756\n",
      "iteration number: 1949\t training loss: 0.0812\tvalidation loss: 0.0915\t validation accuracy: 0.9756\n",
      "iteration number: 1950\t training loss: 0.0809\tvalidation loss: 0.0911\t validation accuracy: 0.9756\n",
      "iteration number: 1951\t training loss: 0.0804\tvalidation loss: 0.0914\t validation accuracy: 0.9756\n",
      "iteration number: 1952\t training loss: 0.0826\tvalidation loss: 0.0954\t validation accuracy: 0.9711\n",
      "iteration number: 1953\t training loss: 0.0829\tvalidation loss: 0.0952\t validation accuracy: 0.9711\n",
      "iteration number: 1954\t training loss: 0.0835\tvalidation loss: 0.0951\t validation accuracy: 0.9689\n",
      "iteration number: 1955\t training loss: 0.0840\tvalidation loss: 0.0957\t validation accuracy: 0.9689\n",
      "iteration number: 1956\t training loss: 0.0824\tvalidation loss: 0.0938\t validation accuracy: 0.9711\n",
      "iteration number: 1957\t training loss: 0.0824\tvalidation loss: 0.0939\t validation accuracy: 0.9711\n",
      "iteration number: 1958\t training loss: 0.0825\tvalidation loss: 0.0941\t validation accuracy: 0.9689\n",
      "iteration number: 1959\t training loss: 0.0820\tvalidation loss: 0.0942\t validation accuracy: 0.9711\n",
      "iteration number: 1960\t training loss: 0.0818\tvalidation loss: 0.0937\t validation accuracy: 0.9711\n",
      "iteration number: 1961\t training loss: 0.0833\tvalidation loss: 0.0963\t validation accuracy: 0.9711\n",
      "iteration number: 1962\t training loss: 0.0819\tvalidation loss: 0.0927\t validation accuracy: 0.9733\n",
      "iteration number: 1963\t training loss: 0.0817\tvalidation loss: 0.0923\t validation accuracy: 0.9733\n",
      "iteration number: 1964\t training loss: 0.0818\tvalidation loss: 0.0916\t validation accuracy: 0.9756\n",
      "iteration number: 1965\t training loss: 0.0822\tvalidation loss: 0.0911\t validation accuracy: 0.9733\n",
      "iteration number: 1966\t training loss: 0.0827\tvalidation loss: 0.0932\t validation accuracy: 0.9689\n",
      "iteration number: 1967\t training loss: 0.0827\tvalidation loss: 0.0915\t validation accuracy: 0.9756\n",
      "iteration number: 1968\t training loss: 0.0825\tvalidation loss: 0.0922\t validation accuracy: 0.9778\n",
      "iteration number: 1969\t training loss: 0.0823\tvalidation loss: 0.0921\t validation accuracy: 0.9800\n",
      "iteration number: 1970\t training loss: 0.0805\tvalidation loss: 0.0921\t validation accuracy: 0.9778\n",
      "iteration number: 1971\t training loss: 0.0802\tvalidation loss: 0.0909\t validation accuracy: 0.9778\n",
      "iteration number: 1972\t training loss: 0.0806\tvalidation loss: 0.0907\t validation accuracy: 0.9778\n",
      "iteration number: 1973\t training loss: 0.0808\tvalidation loss: 0.0912\t validation accuracy: 0.9778\n",
      "iteration number: 1974\t training loss: 0.0809\tvalidation loss: 0.0902\t validation accuracy: 0.9800\n",
      "iteration number: 1975\t training loss: 0.0815\tvalidation loss: 0.0914\t validation accuracy: 0.9800\n",
      "iteration number: 1976\t training loss: 0.0813\tvalidation loss: 0.0908\t validation accuracy: 0.9800\n",
      "iteration number: 1977\t training loss: 0.0810\tvalidation loss: 0.0908\t validation accuracy: 0.9800\n",
      "iteration number: 1978\t training loss: 0.0806\tvalidation loss: 0.0903\t validation accuracy: 0.9800\n",
      "iteration number: 1979\t training loss: 0.0810\tvalidation loss: 0.0909\t validation accuracy: 0.9800\n",
      "iteration number: 1980\t training loss: 0.0816\tvalidation loss: 0.0920\t validation accuracy: 0.9800\n",
      "iteration number: 1981\t training loss: 0.0798\tvalidation loss: 0.0908\t validation accuracy: 0.9778\n",
      "iteration number: 1982\t training loss: 0.0794\tvalidation loss: 0.0900\t validation accuracy: 0.9778\n",
      "iteration number: 1983\t training loss: 0.0797\tvalidation loss: 0.0907\t validation accuracy: 0.9756\n",
      "iteration number: 1984\t training loss: 0.0799\tvalidation loss: 0.0887\t validation accuracy: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1985\t training loss: 0.0797\tvalidation loss: 0.0895\t validation accuracy: 0.9756\n",
      "iteration number: 1986\t training loss: 0.0803\tvalidation loss: 0.0903\t validation accuracy: 0.9756\n",
      "iteration number: 1987\t training loss: 0.0818\tvalidation loss: 0.0922\t validation accuracy: 0.9756\n",
      "iteration number: 1988\t training loss: 0.0802\tvalidation loss: 0.0903\t validation accuracy: 0.9756\n",
      "iteration number: 1989\t training loss: 0.0819\tvalidation loss: 0.0913\t validation accuracy: 0.9756\n",
      "iteration number: 1990\t training loss: 0.0820\tvalidation loss: 0.0916\t validation accuracy: 0.9733\n",
      "iteration number: 1991\t training loss: 0.0792\tvalidation loss: 0.0909\t validation accuracy: 0.9756\n",
      "iteration number: 1992\t training loss: 0.0796\tvalidation loss: 0.0904\t validation accuracy: 0.9756\n",
      "iteration number: 1993\t training loss: 0.0800\tvalidation loss: 0.0905\t validation accuracy: 0.9778\n",
      "iteration number: 1994\t training loss: 0.0801\tvalidation loss: 0.0910\t validation accuracy: 0.9778\n",
      "iteration number: 1995\t training loss: 0.0799\tvalidation loss: 0.0915\t validation accuracy: 0.9778\n",
      "iteration number: 1996\t training loss: 0.0794\tvalidation loss: 0.0898\t validation accuracy: 0.9778\n",
      "iteration number: 1997\t training loss: 0.0795\tvalidation loss: 0.0886\t validation accuracy: 0.9800\n",
      "iteration number: 1998\t training loss: 0.0796\tvalidation loss: 0.0895\t validation accuracy: 0.9778\n",
      "iteration number: 1999\t training loss: 0.0798\tvalidation loss: 0.0906\t validation accuracy: 0.9756\n",
      "iteration number: 2000\t training loss: 0.0792\tvalidation loss: 0.0897\t validation accuracy: 0.9756\n",
      "iteration number: 2001\t training loss: 0.0793\tvalidation loss: 0.0901\t validation accuracy: 0.9756\n",
      "iteration number: 2002\t training loss: 0.0794\tvalidation loss: 0.0904\t validation accuracy: 0.9756\n",
      "iteration number: 2003\t training loss: 0.0792\tvalidation loss: 0.0907\t validation accuracy: 0.9756\n",
      "iteration number: 2004\t training loss: 0.0794\tvalidation loss: 0.0909\t validation accuracy: 0.9756\n",
      "iteration number: 2005\t training loss: 0.0788\tvalidation loss: 0.0898\t validation accuracy: 0.9756\n",
      "iteration number: 2006\t training loss: 0.0801\tvalidation loss: 0.0896\t validation accuracy: 0.9800\n",
      "iteration number: 2007\t training loss: 0.0794\tvalidation loss: 0.0891\t validation accuracy: 0.9800\n",
      "iteration number: 2008\t training loss: 0.0788\tvalidation loss: 0.0888\t validation accuracy: 0.9778\n",
      "iteration number: 2009\t training loss: 0.0787\tvalidation loss: 0.0893\t validation accuracy: 0.9778\n",
      "iteration number: 2010\t training loss: 0.0787\tvalidation loss: 0.0886\t validation accuracy: 0.9800\n",
      "iteration number: 2011\t training loss: 0.0785\tvalidation loss: 0.0887\t validation accuracy: 0.9756\n",
      "iteration number: 2012\t training loss: 0.0791\tvalidation loss: 0.0896\t validation accuracy: 0.9756\n",
      "iteration number: 2013\t training loss: 0.0783\tvalidation loss: 0.0885\t validation accuracy: 0.9756\n",
      "iteration number: 2014\t training loss: 0.0804\tvalidation loss: 0.0919\t validation accuracy: 0.9756\n",
      "iteration number: 2015\t training loss: 0.0803\tvalidation loss: 0.0915\t validation accuracy: 0.9778\n",
      "iteration number: 2016\t training loss: 0.0795\tvalidation loss: 0.0918\t validation accuracy: 0.9756\n",
      "iteration number: 2017\t training loss: 0.0802\tvalidation loss: 0.0928\t validation accuracy: 0.9756\n",
      "iteration number: 2018\t training loss: 0.0793\tvalidation loss: 0.0929\t validation accuracy: 0.9756\n",
      "iteration number: 2019\t training loss: 0.0791\tvalidation loss: 0.0923\t validation accuracy: 0.9756\n",
      "iteration number: 2020\t training loss: 0.0806\tvalidation loss: 0.0917\t validation accuracy: 0.9800\n",
      "iteration number: 2021\t training loss: 0.0791\tvalidation loss: 0.0903\t validation accuracy: 0.9778\n",
      "iteration number: 2022\t training loss: 0.0790\tvalidation loss: 0.0901\t validation accuracy: 0.9778\n",
      "iteration number: 2023\t training loss: 0.0787\tvalidation loss: 0.0900\t validation accuracy: 0.9756\n",
      "iteration number: 2024\t training loss: 0.0785\tvalidation loss: 0.0903\t validation accuracy: 0.9756\n",
      "iteration number: 2025\t training loss: 0.0791\tvalidation loss: 0.0904\t validation accuracy: 0.9778\n",
      "iteration number: 2026\t training loss: 0.0787\tvalidation loss: 0.0899\t validation accuracy: 0.9778\n",
      "iteration number: 2027\t training loss: 0.0806\tvalidation loss: 0.0917\t validation accuracy: 0.9756\n",
      "iteration number: 2028\t training loss: 0.0802\tvalidation loss: 0.0919\t validation accuracy: 0.9756\n",
      "iteration number: 2029\t training loss: 0.0790\tvalidation loss: 0.0909\t validation accuracy: 0.9778\n",
      "iteration number: 2030\t training loss: 0.0831\tvalidation loss: 0.0941\t validation accuracy: 0.9756\n",
      "iteration number: 2031\t training loss: 0.0803\tvalidation loss: 0.0915\t validation accuracy: 0.9756\n",
      "iteration number: 2032\t training loss: 0.0810\tvalidation loss: 0.0917\t validation accuracy: 0.9778\n",
      "iteration number: 2033\t training loss: 0.0796\tvalidation loss: 0.0903\t validation accuracy: 0.9756\n",
      "iteration number: 2034\t training loss: 0.0801\tvalidation loss: 0.0902\t validation accuracy: 0.9778\n",
      "iteration number: 2035\t training loss: 0.0804\tvalidation loss: 0.0912\t validation accuracy: 0.9778\n",
      "iteration number: 2036\t training loss: 0.0803\tvalidation loss: 0.0899\t validation accuracy: 0.9778\n",
      "iteration number: 2037\t training loss: 0.0789\tvalidation loss: 0.0896\t validation accuracy: 0.9778\n",
      "iteration number: 2038\t training loss: 0.0785\tvalidation loss: 0.0888\t validation accuracy: 0.9778\n",
      "iteration number: 2039\t training loss: 0.0780\tvalidation loss: 0.0888\t validation accuracy: 0.9778\n",
      "iteration number: 2040\t training loss: 0.0780\tvalidation loss: 0.0883\t validation accuracy: 0.9778\n",
      "iteration number: 2041\t training loss: 0.0777\tvalidation loss: 0.0881\t validation accuracy: 0.9778\n",
      "iteration number: 2042\t training loss: 0.0786\tvalidation loss: 0.0890\t validation accuracy: 0.9778\n",
      "iteration number: 2043\t training loss: 0.0787\tvalidation loss: 0.0896\t validation accuracy: 0.9778\n",
      "iteration number: 2044\t training loss: 0.0798\tvalidation loss: 0.0904\t validation accuracy: 0.9778\n",
      "iteration number: 2045\t training loss: 0.0785\tvalidation loss: 0.0886\t validation accuracy: 0.9800\n",
      "iteration number: 2046\t training loss: 0.0780\tvalidation loss: 0.0881\t validation accuracy: 0.9800\n",
      "iteration number: 2047\t training loss: 0.0788\tvalidation loss: 0.0871\t validation accuracy: 0.9800\n",
      "iteration number: 2048\t training loss: 0.0780\tvalidation loss: 0.0869\t validation accuracy: 0.9822\n",
      "iteration number: 2049\t training loss: 0.0771\tvalidation loss: 0.0868\t validation accuracy: 0.9778\n",
      "iteration number: 2050\t training loss: 0.0772\tvalidation loss: 0.0860\t validation accuracy: 0.9778\n",
      "iteration number: 2051\t training loss: 0.0774\tvalidation loss: 0.0861\t validation accuracy: 0.9756\n",
      "iteration number: 2052\t training loss: 0.0773\tvalidation loss: 0.0865\t validation accuracy: 0.9756\n",
      "iteration number: 2053\t training loss: 0.0791\tvalidation loss: 0.0885\t validation accuracy: 0.9756\n",
      "iteration number: 2054\t training loss: 0.0822\tvalidation loss: 0.0907\t validation accuracy: 0.9733\n",
      "iteration number: 2055\t training loss: 0.0786\tvalidation loss: 0.0887\t validation accuracy: 0.9733\n",
      "iteration number: 2056\t training loss: 0.0783\tvalidation loss: 0.0878\t validation accuracy: 0.9756\n",
      "iteration number: 2057\t training loss: 0.0769\tvalidation loss: 0.0874\t validation accuracy: 0.9756\n",
      "iteration number: 2058\t training loss: 0.0774\tvalidation loss: 0.0879\t validation accuracy: 0.9756\n",
      "iteration number: 2059\t training loss: 0.0777\tvalidation loss: 0.0883\t validation accuracy: 0.9778\n",
      "iteration number: 2060\t training loss: 0.0785\tvalidation loss: 0.0888\t validation accuracy: 0.9778\n",
      "iteration number: 2061\t training loss: 0.0789\tvalidation loss: 0.0886\t validation accuracy: 0.9778\n",
      "iteration number: 2062\t training loss: 0.0796\tvalidation loss: 0.0893\t validation accuracy: 0.9756\n",
      "iteration number: 2063\t training loss: 0.0794\tvalidation loss: 0.0884\t validation accuracy: 0.9778\n",
      "iteration number: 2064\t training loss: 0.0779\tvalidation loss: 0.0862\t validation accuracy: 0.9778\n",
      "iteration number: 2065\t training loss: 0.0777\tvalidation loss: 0.0866\t validation accuracy: 0.9778\n",
      "iteration number: 2066\t training loss: 0.0774\tvalidation loss: 0.0863\t validation accuracy: 0.9778\n",
      "iteration number: 2067\t training loss: 0.0763\tvalidation loss: 0.0851\t validation accuracy: 0.9778\n",
      "iteration number: 2068\t training loss: 0.0764\tvalidation loss: 0.0847\t validation accuracy: 0.9778\n",
      "iteration number: 2069\t training loss: 0.0759\tvalidation loss: 0.0854\t validation accuracy: 0.9778\n",
      "iteration number: 2070\t training loss: 0.0763\tvalidation loss: 0.0860\t validation accuracy: 0.9756\n",
      "iteration number: 2071\t training loss: 0.0776\tvalidation loss: 0.0860\t validation accuracy: 0.9778\n",
      "iteration number: 2072\t training loss: 0.0793\tvalidation loss: 0.0874\t validation accuracy: 0.9778\n",
      "iteration number: 2073\t training loss: 0.0770\tvalidation loss: 0.0863\t validation accuracy: 0.9756\n",
      "iteration number: 2074\t training loss: 0.0761\tvalidation loss: 0.0857\t validation accuracy: 0.9800\n",
      "iteration number: 2075\t training loss: 0.0763\tvalidation loss: 0.0861\t validation accuracy: 0.9800\n",
      "iteration number: 2076\t training loss: 0.0766\tvalidation loss: 0.0869\t validation accuracy: 0.9822\n",
      "iteration number: 2077\t training loss: 0.0767\tvalidation loss: 0.0867\t validation accuracy: 0.9822\n",
      "iteration number: 2078\t training loss: 0.0791\tvalidation loss: 0.0884\t validation accuracy: 0.9800\n",
      "iteration number: 2079\t training loss: 0.0793\tvalidation loss: 0.0903\t validation accuracy: 0.9733\n",
      "iteration number: 2080\t training loss: 0.0782\tvalidation loss: 0.0906\t validation accuracy: 0.9733\n",
      "iteration number: 2081\t training loss: 0.0777\tvalidation loss: 0.0900\t validation accuracy: 0.9733\n",
      "iteration number: 2082\t training loss: 0.0773\tvalidation loss: 0.0880\t validation accuracy: 0.9711\n",
      "iteration number: 2083\t training loss: 0.0768\tvalidation loss: 0.0868\t validation accuracy: 0.9778\n",
      "iteration number: 2084\t training loss: 0.0763\tvalidation loss: 0.0860\t validation accuracy: 0.9778\n",
      "iteration number: 2085\t training loss: 0.0796\tvalidation loss: 0.0875\t validation accuracy: 0.9778\n",
      "iteration number: 2086\t training loss: 0.0787\tvalidation loss: 0.0872\t validation accuracy: 0.9800\n",
      "iteration number: 2087\t training loss: 0.0803\tvalidation loss: 0.0892\t validation accuracy: 0.9756\n",
      "iteration number: 2088\t training loss: 0.0794\tvalidation loss: 0.0880\t validation accuracy: 0.9778\n",
      "iteration number: 2089\t training loss: 0.0775\tvalidation loss: 0.0862\t validation accuracy: 0.9822\n",
      "iteration number: 2090\t training loss: 0.0797\tvalidation loss: 0.0881\t validation accuracy: 0.9778\n",
      "iteration number: 2091\t training loss: 0.0809\tvalidation loss: 0.0888\t validation accuracy: 0.9778\n",
      "iteration number: 2092\t training loss: 0.0786\tvalidation loss: 0.0873\t validation accuracy: 0.9778\n",
      "iteration number: 2093\t training loss: 0.0766\tvalidation loss: 0.0861\t validation accuracy: 0.9800\n",
      "iteration number: 2094\t training loss: 0.0775\tvalidation loss: 0.0865\t validation accuracy: 0.9800\n",
      "iteration number: 2095\t training loss: 0.0784\tvalidation loss: 0.0874\t validation accuracy: 0.9778\n",
      "iteration number: 2096\t training loss: 0.0783\tvalidation loss: 0.0876\t validation accuracy: 0.9778\n",
      "iteration number: 2097\t training loss: 0.0774\tvalidation loss: 0.0874\t validation accuracy: 0.9778\n",
      "iteration number: 2098\t training loss: 0.0769\tvalidation loss: 0.0866\t validation accuracy: 0.9778\n",
      "iteration number: 2099\t training loss: 0.0755\tvalidation loss: 0.0852\t validation accuracy: 0.9800\n",
      "iteration number: 2100\t training loss: 0.0751\tvalidation loss: 0.0856\t validation accuracy: 0.9778\n",
      "iteration number: 2101\t training loss: 0.0754\tvalidation loss: 0.0852\t validation accuracy: 0.9800\n",
      "iteration number: 2102\t training loss: 0.0752\tvalidation loss: 0.0849\t validation accuracy: 0.9800\n",
      "iteration number: 2103\t training loss: 0.0759\tvalidation loss: 0.0843\t validation accuracy: 0.9778\n",
      "iteration number: 2104\t training loss: 0.0753\tvalidation loss: 0.0844\t validation accuracy: 0.9800\n",
      "iteration number: 2105\t training loss: 0.0755\tvalidation loss: 0.0839\t validation accuracy: 0.9778\n",
      "iteration number: 2106\t training loss: 0.0764\tvalidation loss: 0.0843\t validation accuracy: 0.9822\n",
      "iteration number: 2107\t training loss: 0.0760\tvalidation loss: 0.0850\t validation accuracy: 0.9778\n",
      "iteration number: 2108\t training loss: 0.0750\tvalidation loss: 0.0850\t validation accuracy: 0.9778\n",
      "iteration number: 2109\t training loss: 0.0752\tvalidation loss: 0.0853\t validation accuracy: 0.9778\n",
      "iteration number: 2110\t training loss: 0.0748\tvalidation loss: 0.0848\t validation accuracy: 0.9778\n",
      "iteration number: 2111\t training loss: 0.0758\tvalidation loss: 0.0864\t validation accuracy: 0.9756\n",
      "iteration number: 2112\t training loss: 0.0771\tvalidation loss: 0.0894\t validation accuracy: 0.9733\n",
      "iteration number: 2113\t training loss: 0.0765\tvalidation loss: 0.0873\t validation accuracy: 0.9733\n",
      "iteration number: 2114\t training loss: 0.0765\tvalidation loss: 0.0876\t validation accuracy: 0.9733\n",
      "iteration number: 2115\t training loss: 0.0766\tvalidation loss: 0.0876\t validation accuracy: 0.9733\n",
      "iteration number: 2116\t training loss: 0.0788\tvalidation loss: 0.0883\t validation accuracy: 0.9778\n",
      "iteration number: 2117\t training loss: 0.0780\tvalidation loss: 0.0877\t validation accuracy: 0.9756\n",
      "iteration number: 2118\t training loss: 0.0758\tvalidation loss: 0.0860\t validation accuracy: 0.9778\n",
      "iteration number: 2119\t training loss: 0.0759\tvalidation loss: 0.0872\t validation accuracy: 0.9756\n",
      "iteration number: 2120\t training loss: 0.0761\tvalidation loss: 0.0871\t validation accuracy: 0.9756\n",
      "iteration number: 2121\t training loss: 0.0766\tvalidation loss: 0.0865\t validation accuracy: 0.9756\n",
      "iteration number: 2122\t training loss: 0.0779\tvalidation loss: 0.0875\t validation accuracy: 0.9711\n",
      "iteration number: 2123\t training loss: 0.0777\tvalidation loss: 0.0873\t validation accuracy: 0.9756\n",
      "iteration number: 2124\t training loss: 0.0765\tvalidation loss: 0.0868\t validation accuracy: 0.9756\n",
      "iteration number: 2125\t training loss: 0.0758\tvalidation loss: 0.0860\t validation accuracy: 0.9778\n",
      "iteration number: 2126\t training loss: 0.0756\tvalidation loss: 0.0852\t validation accuracy: 0.9800\n",
      "iteration number: 2127\t training loss: 0.0748\tvalidation loss: 0.0852\t validation accuracy: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2128\t training loss: 0.0753\tvalidation loss: 0.0855\t validation accuracy: 0.9778\n",
      "iteration number: 2129\t training loss: 0.0750\tvalidation loss: 0.0858\t validation accuracy: 0.9778\n",
      "iteration number: 2130\t training loss: 0.0744\tvalidation loss: 0.0845\t validation accuracy: 0.9778\n",
      "iteration number: 2131\t training loss: 0.0744\tvalidation loss: 0.0848\t validation accuracy: 0.9800\n",
      "iteration number: 2132\t training loss: 0.0742\tvalidation loss: 0.0843\t validation accuracy: 0.9800\n",
      "iteration number: 2133\t training loss: 0.0739\tvalidation loss: 0.0843\t validation accuracy: 0.9778\n",
      "iteration number: 2134\t training loss: 0.0742\tvalidation loss: 0.0850\t validation accuracy: 0.9756\n",
      "iteration number: 2135\t training loss: 0.0741\tvalidation loss: 0.0842\t validation accuracy: 0.9778\n",
      "iteration number: 2136\t training loss: 0.0739\tvalidation loss: 0.0840\t validation accuracy: 0.9778\n",
      "iteration number: 2137\t training loss: 0.0744\tvalidation loss: 0.0840\t validation accuracy: 0.9778\n",
      "iteration number: 2138\t training loss: 0.0747\tvalidation loss: 0.0844\t validation accuracy: 0.9778\n",
      "iteration number: 2139\t training loss: 0.0755\tvalidation loss: 0.0846\t validation accuracy: 0.9800\n",
      "iteration number: 2140\t training loss: 0.0742\tvalidation loss: 0.0832\t validation accuracy: 0.9778\n",
      "iteration number: 2141\t training loss: 0.0777\tvalidation loss: 0.0850\t validation accuracy: 0.9800\n",
      "iteration number: 2142\t training loss: 0.0757\tvalidation loss: 0.0834\t validation accuracy: 0.9778\n",
      "iteration number: 2143\t training loss: 0.0746\tvalidation loss: 0.0825\t validation accuracy: 0.9800\n",
      "iteration number: 2144\t training loss: 0.0744\tvalidation loss: 0.0822\t validation accuracy: 0.9800\n",
      "iteration number: 2145\t training loss: 0.0752\tvalidation loss: 0.0824\t validation accuracy: 0.9822\n",
      "iteration number: 2146\t training loss: 0.0756\tvalidation loss: 0.0825\t validation accuracy: 0.9800\n",
      "iteration number: 2147\t training loss: 0.0752\tvalidation loss: 0.0824\t validation accuracy: 0.9778\n",
      "iteration number: 2148\t training loss: 0.0744\tvalidation loss: 0.0821\t validation accuracy: 0.9778\n",
      "iteration number: 2149\t training loss: 0.0747\tvalidation loss: 0.0819\t validation accuracy: 0.9800\n",
      "iteration number: 2150\t training loss: 0.0740\tvalidation loss: 0.0818\t validation accuracy: 0.9800\n",
      "iteration number: 2151\t training loss: 0.0750\tvalidation loss: 0.0824\t validation accuracy: 0.9844\n",
      "iteration number: 2152\t training loss: 0.0746\tvalidation loss: 0.0815\t validation accuracy: 0.9844\n",
      "iteration number: 2153\t training loss: 0.0747\tvalidation loss: 0.0821\t validation accuracy: 0.9844\n",
      "iteration number: 2154\t training loss: 0.0744\tvalidation loss: 0.0825\t validation accuracy: 0.9844\n",
      "iteration number: 2155\t training loss: 0.0741\tvalidation loss: 0.0829\t validation accuracy: 0.9822\n",
      "iteration number: 2156\t training loss: 0.0740\tvalidation loss: 0.0833\t validation accuracy: 0.9800\n",
      "iteration number: 2157\t training loss: 0.0737\tvalidation loss: 0.0823\t validation accuracy: 0.9800\n",
      "iteration number: 2158\t training loss: 0.0735\tvalidation loss: 0.0825\t validation accuracy: 0.9800\n",
      "iteration number: 2159\t training loss: 0.0742\tvalidation loss: 0.0823\t validation accuracy: 0.9822\n",
      "iteration number: 2160\t training loss: 0.0740\tvalidation loss: 0.0831\t validation accuracy: 0.9800\n",
      "iteration number: 2161\t training loss: 0.0748\tvalidation loss: 0.0832\t validation accuracy: 0.9778\n",
      "iteration number: 2162\t training loss: 0.0741\tvalidation loss: 0.0821\t validation accuracy: 0.9778\n",
      "iteration number: 2163\t training loss: 0.0782\tvalidation loss: 0.0849\t validation accuracy: 0.9756\n",
      "iteration number: 2164\t training loss: 0.0759\tvalidation loss: 0.0843\t validation accuracy: 0.9778\n",
      "iteration number: 2165\t training loss: 0.0751\tvalidation loss: 0.0842\t validation accuracy: 0.9778\n",
      "iteration number: 2166\t training loss: 0.0737\tvalidation loss: 0.0824\t validation accuracy: 0.9800\n",
      "iteration number: 2167\t training loss: 0.0738\tvalidation loss: 0.0825\t validation accuracy: 0.9800\n",
      "iteration number: 2168\t training loss: 0.0735\tvalidation loss: 0.0829\t validation accuracy: 0.9778\n",
      "iteration number: 2169\t training loss: 0.0737\tvalidation loss: 0.0833\t validation accuracy: 0.9778\n",
      "iteration number: 2170\t training loss: 0.0736\tvalidation loss: 0.0835\t validation accuracy: 0.9778\n",
      "iteration number: 2171\t training loss: 0.0735\tvalidation loss: 0.0829\t validation accuracy: 0.9778\n",
      "iteration number: 2172\t training loss: 0.0734\tvalidation loss: 0.0820\t validation accuracy: 0.9756\n",
      "iteration number: 2173\t training loss: 0.0770\tvalidation loss: 0.0858\t validation accuracy: 0.9756\n",
      "iteration number: 2174\t training loss: 0.0748\tvalidation loss: 0.0840\t validation accuracy: 0.9756\n",
      "iteration number: 2175\t training loss: 0.0735\tvalidation loss: 0.0833\t validation accuracy: 0.9756\n",
      "iteration number: 2176\t training loss: 0.0733\tvalidation loss: 0.0834\t validation accuracy: 0.9756\n",
      "iteration number: 2177\t training loss: 0.0731\tvalidation loss: 0.0826\t validation accuracy: 0.9778\n",
      "iteration number: 2178\t training loss: 0.0742\tvalidation loss: 0.0834\t validation accuracy: 0.9778\n",
      "iteration number: 2179\t training loss: 0.0741\tvalidation loss: 0.0838\t validation accuracy: 0.9778\n",
      "iteration number: 2180\t training loss: 0.0752\tvalidation loss: 0.0839\t validation accuracy: 0.9778\n",
      "iteration number: 2181\t training loss: 0.0735\tvalidation loss: 0.0844\t validation accuracy: 0.9778\n",
      "iteration number: 2182\t training loss: 0.0730\tvalidation loss: 0.0842\t validation accuracy: 0.9778\n",
      "iteration number: 2183\t training loss: 0.0731\tvalidation loss: 0.0842\t validation accuracy: 0.9778\n",
      "iteration number: 2184\t training loss: 0.0724\tvalidation loss: 0.0837\t validation accuracy: 0.9778\n",
      "iteration number: 2185\t training loss: 0.0728\tvalidation loss: 0.0840\t validation accuracy: 0.9778\n",
      "iteration number: 2186\t training loss: 0.0735\tvalidation loss: 0.0852\t validation accuracy: 0.9756\n",
      "iteration number: 2187\t training loss: 0.0734\tvalidation loss: 0.0845\t validation accuracy: 0.9756\n",
      "iteration number: 2188\t training loss: 0.0749\tvalidation loss: 0.0860\t validation accuracy: 0.9756\n",
      "iteration number: 2189\t training loss: 0.0733\tvalidation loss: 0.0834\t validation accuracy: 0.9778\n",
      "iteration number: 2190\t training loss: 0.0745\tvalidation loss: 0.0826\t validation accuracy: 0.9800\n",
      "iteration number: 2191\t training loss: 0.0738\tvalidation loss: 0.0826\t validation accuracy: 0.9800\n",
      "iteration number: 2192\t training loss: 0.0747\tvalidation loss: 0.0841\t validation accuracy: 0.9778\n",
      "iteration number: 2193\t training loss: 0.0752\tvalidation loss: 0.0834\t validation accuracy: 0.9778\n",
      "iteration number: 2194\t training loss: 0.0755\tvalidation loss: 0.0841\t validation accuracy: 0.9778\n",
      "iteration number: 2195\t training loss: 0.0764\tvalidation loss: 0.0850\t validation accuracy: 0.9778\n",
      "iteration number: 2196\t training loss: 0.0764\tvalidation loss: 0.0844\t validation accuracy: 0.9778\n",
      "iteration number: 2197\t training loss: 0.0733\tvalidation loss: 0.0833\t validation accuracy: 0.9800\n",
      "iteration number: 2198\t training loss: 0.0734\tvalidation loss: 0.0835\t validation accuracy: 0.9800\n",
      "iteration number: 2199\t training loss: 0.0736\tvalidation loss: 0.0829\t validation accuracy: 0.9822\n",
      "iteration number: 2200\t training loss: 0.0732\tvalidation loss: 0.0822\t validation accuracy: 0.9800\n",
      "iteration number: 2201\t training loss: 0.0725\tvalidation loss: 0.0820\t validation accuracy: 0.9800\n",
      "iteration number: 2202\t training loss: 0.0726\tvalidation loss: 0.0829\t validation accuracy: 0.9800\n",
      "iteration number: 2203\t training loss: 0.0742\tvalidation loss: 0.0853\t validation accuracy: 0.9756\n",
      "iteration number: 2204\t training loss: 0.0721\tvalidation loss: 0.0831\t validation accuracy: 0.9756\n",
      "iteration number: 2205\t training loss: 0.0721\tvalidation loss: 0.0828\t validation accuracy: 0.9778\n",
      "iteration number: 2206\t training loss: 0.0721\tvalidation loss: 0.0831\t validation accuracy: 0.9778\n",
      "iteration number: 2207\t training loss: 0.0724\tvalidation loss: 0.0814\t validation accuracy: 0.9800\n",
      "iteration number: 2208\t training loss: 0.0725\tvalidation loss: 0.0818\t validation accuracy: 0.9800\n",
      "iteration number: 2209\t training loss: 0.0736\tvalidation loss: 0.0820\t validation accuracy: 0.9800\n",
      "iteration number: 2210\t training loss: 0.0735\tvalidation loss: 0.0818\t validation accuracy: 0.9800\n",
      "iteration number: 2211\t training loss: 0.0747\tvalidation loss: 0.0824\t validation accuracy: 0.9778\n",
      "iteration number: 2212\t training loss: 0.0750\tvalidation loss: 0.0833\t validation accuracy: 0.9778\n",
      "iteration number: 2213\t training loss: 0.0743\tvalidation loss: 0.0829\t validation accuracy: 0.9778\n",
      "iteration number: 2214\t training loss: 0.0755\tvalidation loss: 0.0835\t validation accuracy: 0.9800\n",
      "iteration number: 2215\t training loss: 0.0735\tvalidation loss: 0.0829\t validation accuracy: 0.9778\n",
      "iteration number: 2216\t training loss: 0.0744\tvalidation loss: 0.0843\t validation accuracy: 0.9778\n",
      "iteration number: 2217\t training loss: 0.0726\tvalidation loss: 0.0835\t validation accuracy: 0.9800\n",
      "iteration number: 2218\t training loss: 0.0718\tvalidation loss: 0.0829\t validation accuracy: 0.9800\n",
      "iteration number: 2219\t training loss: 0.0721\tvalidation loss: 0.0819\t validation accuracy: 0.9800\n",
      "iteration number: 2220\t training loss: 0.0729\tvalidation loss: 0.0830\t validation accuracy: 0.9800\n",
      "iteration number: 2221\t training loss: 0.0734\tvalidation loss: 0.0827\t validation accuracy: 0.9800\n",
      "iteration number: 2222\t training loss: 0.0741\tvalidation loss: 0.0836\t validation accuracy: 0.9800\n",
      "iteration number: 2223\t training loss: 0.0731\tvalidation loss: 0.0836\t validation accuracy: 0.9800\n",
      "iteration number: 2224\t training loss: 0.0720\tvalidation loss: 0.0828\t validation accuracy: 0.9800\n",
      "iteration number: 2225\t training loss: 0.0725\tvalidation loss: 0.0840\t validation accuracy: 0.9778\n",
      "iteration number: 2226\t training loss: 0.0713\tvalidation loss: 0.0817\t validation accuracy: 0.9800\n",
      "iteration number: 2227\t training loss: 0.0711\tvalidation loss: 0.0825\t validation accuracy: 0.9778\n",
      "iteration number: 2228\t training loss: 0.0719\tvalidation loss: 0.0832\t validation accuracy: 0.9756\n",
      "iteration number: 2229\t training loss: 0.0719\tvalidation loss: 0.0825\t validation accuracy: 0.9778\n",
      "iteration number: 2230\t training loss: 0.0713\tvalidation loss: 0.0828\t validation accuracy: 0.9756\n",
      "iteration number: 2231\t training loss: 0.0720\tvalidation loss: 0.0840\t validation accuracy: 0.9800\n",
      "iteration number: 2232\t training loss: 0.0712\tvalidation loss: 0.0831\t validation accuracy: 0.9778\n",
      "iteration number: 2233\t training loss: 0.0712\tvalidation loss: 0.0827\t validation accuracy: 0.9778\n",
      "iteration number: 2234\t training loss: 0.0710\tvalidation loss: 0.0827\t validation accuracy: 0.9778\n",
      "iteration number: 2235\t training loss: 0.0716\tvalidation loss: 0.0835\t validation accuracy: 0.9778\n",
      "iteration number: 2236\t training loss: 0.0718\tvalidation loss: 0.0846\t validation accuracy: 0.9733\n",
      "iteration number: 2237\t training loss: 0.0735\tvalidation loss: 0.0861\t validation accuracy: 0.9756\n",
      "iteration number: 2238\t training loss: 0.0733\tvalidation loss: 0.0864\t validation accuracy: 0.9733\n",
      "iteration number: 2239\t training loss: 0.0727\tvalidation loss: 0.0851\t validation accuracy: 0.9733\n",
      "iteration number: 2240\t training loss: 0.0720\tvalidation loss: 0.0853\t validation accuracy: 0.9733\n",
      "iteration number: 2241\t training loss: 0.0718\tvalidation loss: 0.0844\t validation accuracy: 0.9733\n",
      "iteration number: 2242\t training loss: 0.0710\tvalidation loss: 0.0824\t validation accuracy: 0.9756\n",
      "iteration number: 2243\t training loss: 0.0717\tvalidation loss: 0.0825\t validation accuracy: 0.9778\n",
      "iteration number: 2244\t training loss: 0.0717\tvalidation loss: 0.0830\t validation accuracy: 0.9778\n",
      "iteration number: 2245\t training loss: 0.0709\tvalidation loss: 0.0824\t validation accuracy: 0.9778\n",
      "iteration number: 2246\t training loss: 0.0707\tvalidation loss: 0.0828\t validation accuracy: 0.9778\n",
      "iteration number: 2247\t training loss: 0.0709\tvalidation loss: 0.0828\t validation accuracy: 0.9778\n",
      "iteration number: 2248\t training loss: 0.0728\tvalidation loss: 0.0836\t validation accuracy: 0.9800\n",
      "iteration number: 2249\t training loss: 0.0723\tvalidation loss: 0.0840\t validation accuracy: 0.9778\n",
      "iteration number: 2250\t training loss: 0.0721\tvalidation loss: 0.0836\t validation accuracy: 0.9778\n",
      "iteration number: 2251\t training loss: 0.0720\tvalidation loss: 0.0840\t validation accuracy: 0.9756\n",
      "iteration number: 2252\t training loss: 0.0713\tvalidation loss: 0.0843\t validation accuracy: 0.9733\n",
      "iteration number: 2253\t training loss: 0.0712\tvalidation loss: 0.0838\t validation accuracy: 0.9778\n",
      "iteration number: 2254\t training loss: 0.0710\tvalidation loss: 0.0837\t validation accuracy: 0.9711\n",
      "iteration number: 2255\t training loss: 0.0709\tvalidation loss: 0.0825\t validation accuracy: 0.9778\n",
      "iteration number: 2256\t training loss: 0.0704\tvalidation loss: 0.0821\t validation accuracy: 0.9756\n",
      "iteration number: 2257\t training loss: 0.0703\tvalidation loss: 0.0818\t validation accuracy: 0.9756\n",
      "iteration number: 2258\t training loss: 0.0704\tvalidation loss: 0.0816\t validation accuracy: 0.9756\n",
      "iteration number: 2259\t training loss: 0.0707\tvalidation loss: 0.0814\t validation accuracy: 0.9756\n",
      "iteration number: 2260\t training loss: 0.0709\tvalidation loss: 0.0817\t validation accuracy: 0.9756\n",
      "iteration number: 2261\t training loss: 0.0723\tvalidation loss: 0.0818\t validation accuracy: 0.9756\n",
      "iteration number: 2262\t training loss: 0.0729\tvalidation loss: 0.0828\t validation accuracy: 0.9756\n",
      "iteration number: 2263\t training loss: 0.0718\tvalidation loss: 0.0821\t validation accuracy: 0.9756\n",
      "iteration number: 2264\t training loss: 0.0720\tvalidation loss: 0.0819\t validation accuracy: 0.9800\n",
      "iteration number: 2265\t training loss: 0.0713\tvalidation loss: 0.0814\t validation accuracy: 0.9800\n",
      "iteration number: 2266\t training loss: 0.0716\tvalidation loss: 0.0816\t validation accuracy: 0.9800\n",
      "iteration number: 2267\t training loss: 0.0701\tvalidation loss: 0.0816\t validation accuracy: 0.9756\n",
      "iteration number: 2268\t training loss: 0.0700\tvalidation loss: 0.0814\t validation accuracy: 0.9778\n",
      "iteration number: 2269\t training loss: 0.0710\tvalidation loss: 0.0836\t validation accuracy: 0.9733\n",
      "iteration number: 2270\t training loss: 0.0725\tvalidation loss: 0.0859\t validation accuracy: 0.9733\n",
      "iteration number: 2271\t training loss: 0.0713\tvalidation loss: 0.0835\t validation accuracy: 0.9733\n",
      "iteration number: 2272\t training loss: 0.0705\tvalidation loss: 0.0824\t validation accuracy: 0.9778\n",
      "iteration number: 2273\t training loss: 0.0707\tvalidation loss: 0.0831\t validation accuracy: 0.9733\n",
      "iteration number: 2274\t training loss: 0.0705\tvalidation loss: 0.0836\t validation accuracy: 0.9756\n",
      "iteration number: 2275\t training loss: 0.0708\tvalidation loss: 0.0837\t validation accuracy: 0.9756\n",
      "iteration number: 2276\t training loss: 0.0701\tvalidation loss: 0.0821\t validation accuracy: 0.9756\n",
      "iteration number: 2277\t training loss: 0.0698\tvalidation loss: 0.0817\t validation accuracy: 0.9778\n",
      "iteration number: 2278\t training loss: 0.0709\tvalidation loss: 0.0836\t validation accuracy: 0.9778\n",
      "iteration number: 2279\t training loss: 0.0705\tvalidation loss: 0.0829\t validation accuracy: 0.9756\n",
      "iteration number: 2280\t training loss: 0.0706\tvalidation loss: 0.0828\t validation accuracy: 0.9778\n",
      "iteration number: 2281\t training loss: 0.0714\tvalidation loss: 0.0829\t validation accuracy: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2282\t training loss: 0.0726\tvalidation loss: 0.0836\t validation accuracy: 0.9778\n",
      "iteration number: 2283\t training loss: 0.0709\tvalidation loss: 0.0830\t validation accuracy: 0.9800\n",
      "iteration number: 2284\t training loss: 0.0705\tvalidation loss: 0.0824\t validation accuracy: 0.9756\n",
      "iteration number: 2285\t training loss: 0.0699\tvalidation loss: 0.0818\t validation accuracy: 0.9756\n",
      "iteration number: 2286\t training loss: 0.0697\tvalidation loss: 0.0813\t validation accuracy: 0.9756\n",
      "iteration number: 2287\t training loss: 0.0700\tvalidation loss: 0.0815\t validation accuracy: 0.9778\n",
      "iteration number: 2288\t training loss: 0.0705\tvalidation loss: 0.0817\t validation accuracy: 0.9800\n",
      "iteration number: 2289\t training loss: 0.0703\tvalidation loss: 0.0803\t validation accuracy: 0.9800\n",
      "iteration number: 2290\t training loss: 0.0695\tvalidation loss: 0.0802\t validation accuracy: 0.9778\n",
      "iteration number: 2291\t training loss: 0.0707\tvalidation loss: 0.0806\t validation accuracy: 0.9822\n",
      "iteration number: 2292\t training loss: 0.0696\tvalidation loss: 0.0808\t validation accuracy: 0.9778\n",
      "iteration number: 2293\t training loss: 0.0696\tvalidation loss: 0.0804\t validation accuracy: 0.9800\n",
      "iteration number: 2294\t training loss: 0.0694\tvalidation loss: 0.0802\t validation accuracy: 0.9800\n",
      "iteration number: 2295\t training loss: 0.0695\tvalidation loss: 0.0804\t validation accuracy: 0.9800\n",
      "iteration number: 2296\t training loss: 0.0694\tvalidation loss: 0.0802\t validation accuracy: 0.9778\n",
      "iteration number: 2297\t training loss: 0.0696\tvalidation loss: 0.0798\t validation accuracy: 0.9822\n",
      "iteration number: 2298\t training loss: 0.0695\tvalidation loss: 0.0803\t validation accuracy: 0.9778\n",
      "iteration number: 2299\t training loss: 0.0698\tvalidation loss: 0.0802\t validation accuracy: 0.9778\n",
      "iteration number: 2300\t training loss: 0.0736\tvalidation loss: 0.0831\t validation accuracy: 0.9756\n",
      "iteration number: 2301\t training loss: 0.0756\tvalidation loss: 0.0870\t validation accuracy: 0.9733\n",
      "iteration number: 2302\t training loss: 0.0727\tvalidation loss: 0.0854\t validation accuracy: 0.9756\n",
      "iteration number: 2303\t training loss: 0.0716\tvalidation loss: 0.0853\t validation accuracy: 0.9733\n",
      "iteration number: 2304\t training loss: 0.0708\tvalidation loss: 0.0847\t validation accuracy: 0.9733\n",
      "iteration number: 2305\t training loss: 0.0699\tvalidation loss: 0.0839\t validation accuracy: 0.9756\n",
      "iteration number: 2306\t training loss: 0.0702\tvalidation loss: 0.0835\t validation accuracy: 0.9756\n",
      "iteration number: 2307\t training loss: 0.0693\tvalidation loss: 0.0825\t validation accuracy: 0.9778\n",
      "iteration number: 2308\t training loss: 0.0693\tvalidation loss: 0.0822\t validation accuracy: 0.9756\n",
      "iteration number: 2309\t training loss: 0.0691\tvalidation loss: 0.0825\t validation accuracy: 0.9778\n",
      "iteration number: 2310\t training loss: 0.0694\tvalidation loss: 0.0828\t validation accuracy: 0.9733\n",
      "iteration number: 2311\t training loss: 0.0704\tvalidation loss: 0.0841\t validation accuracy: 0.9711\n",
      "iteration number: 2312\t training loss: 0.0692\tvalidation loss: 0.0820\t validation accuracy: 0.9756\n",
      "iteration number: 2313\t training loss: 0.0690\tvalidation loss: 0.0820\t validation accuracy: 0.9756\n",
      "iteration number: 2314\t training loss: 0.0689\tvalidation loss: 0.0823\t validation accuracy: 0.9778\n",
      "iteration number: 2315\t training loss: 0.0691\tvalidation loss: 0.0810\t validation accuracy: 0.9778\n",
      "iteration number: 2316\t training loss: 0.0690\tvalidation loss: 0.0800\t validation accuracy: 0.9778\n",
      "iteration number: 2317\t training loss: 0.0690\tvalidation loss: 0.0792\t validation accuracy: 0.9778\n",
      "iteration number: 2318\t training loss: 0.0688\tvalidation loss: 0.0795\t validation accuracy: 0.9778\n",
      "iteration number: 2319\t training loss: 0.0689\tvalidation loss: 0.0793\t validation accuracy: 0.9800\n",
      "iteration number: 2320\t training loss: 0.0696\tvalidation loss: 0.0794\t validation accuracy: 0.9778\n",
      "iteration number: 2321\t training loss: 0.0688\tvalidation loss: 0.0793\t validation accuracy: 0.9800\n",
      "iteration number: 2322\t training loss: 0.0688\tvalidation loss: 0.0797\t validation accuracy: 0.9800\n",
      "iteration number: 2323\t training loss: 0.0693\tvalidation loss: 0.0810\t validation accuracy: 0.9756\n",
      "iteration number: 2324\t training loss: 0.0688\tvalidation loss: 0.0809\t validation accuracy: 0.9756\n",
      "iteration number: 2325\t training loss: 0.0689\tvalidation loss: 0.0815\t validation accuracy: 0.9778\n",
      "iteration number: 2326\t training loss: 0.0691\tvalidation loss: 0.0817\t validation accuracy: 0.9756\n",
      "iteration number: 2327\t training loss: 0.0696\tvalidation loss: 0.0818\t validation accuracy: 0.9756\n",
      "iteration number: 2328\t training loss: 0.0705\tvalidation loss: 0.0830\t validation accuracy: 0.9756\n",
      "iteration number: 2329\t training loss: 0.0703\tvalidation loss: 0.0832\t validation accuracy: 0.9756\n",
      "iteration number: 2330\t training loss: 0.0703\tvalidation loss: 0.0824\t validation accuracy: 0.9778\n",
      "iteration number: 2331\t training loss: 0.0707\tvalidation loss: 0.0833\t validation accuracy: 0.9756\n",
      "iteration number: 2332\t training loss: 0.0713\tvalidation loss: 0.0833\t validation accuracy: 0.9733\n",
      "iteration number: 2333\t training loss: 0.0700\tvalidation loss: 0.0818\t validation accuracy: 0.9756\n",
      "iteration number: 2334\t training loss: 0.0693\tvalidation loss: 0.0809\t validation accuracy: 0.9756\n",
      "iteration number: 2335\t training loss: 0.0685\tvalidation loss: 0.0817\t validation accuracy: 0.9778\n",
      "iteration number: 2336\t training loss: 0.0689\tvalidation loss: 0.0817\t validation accuracy: 0.9800\n",
      "iteration number: 2337\t training loss: 0.0725\tvalidation loss: 0.0850\t validation accuracy: 0.9778\n",
      "iteration number: 2338\t training loss: 0.0705\tvalidation loss: 0.0836\t validation accuracy: 0.9822\n",
      "iteration number: 2339\t training loss: 0.0713\tvalidation loss: 0.0836\t validation accuracy: 0.9800\n",
      "iteration number: 2340\t training loss: 0.0709\tvalidation loss: 0.0845\t validation accuracy: 0.9800\n",
      "iteration number: 2341\t training loss: 0.0715\tvalidation loss: 0.0843\t validation accuracy: 0.9822\n",
      "iteration number: 2342\t training loss: 0.0705\tvalidation loss: 0.0836\t validation accuracy: 0.9800\n",
      "iteration number: 2343\t training loss: 0.0695\tvalidation loss: 0.0823\t validation accuracy: 0.9800\n",
      "iteration number: 2344\t training loss: 0.0706\tvalidation loss: 0.0836\t validation accuracy: 0.9800\n",
      "iteration number: 2345\t training loss: 0.0705\tvalidation loss: 0.0832\t validation accuracy: 0.9756\n",
      "iteration number: 2346\t training loss: 0.0697\tvalidation loss: 0.0829\t validation accuracy: 0.9733\n",
      "iteration number: 2347\t training loss: 0.0688\tvalidation loss: 0.0819\t validation accuracy: 0.9756\n",
      "iteration number: 2348\t training loss: 0.0681\tvalidation loss: 0.0815\t validation accuracy: 0.9733\n",
      "iteration number: 2349\t training loss: 0.0682\tvalidation loss: 0.0816\t validation accuracy: 0.9733\n",
      "iteration number: 2350\t training loss: 0.0684\tvalidation loss: 0.0802\t validation accuracy: 0.9778\n",
      "iteration number: 2351\t training loss: 0.0680\tvalidation loss: 0.0791\t validation accuracy: 0.9778\n",
      "iteration number: 2352\t training loss: 0.0682\tvalidation loss: 0.0795\t validation accuracy: 0.9778\n",
      "iteration number: 2353\t training loss: 0.0687\tvalidation loss: 0.0808\t validation accuracy: 0.9778\n",
      "iteration number: 2354\t training loss: 0.0681\tvalidation loss: 0.0814\t validation accuracy: 0.9800\n",
      "iteration number: 2355\t training loss: 0.0680\tvalidation loss: 0.0811\t validation accuracy: 0.9778\n",
      "iteration number: 2356\t training loss: 0.0677\tvalidation loss: 0.0807\t validation accuracy: 0.9778\n",
      "iteration number: 2357\t training loss: 0.0674\tvalidation loss: 0.0806\t validation accuracy: 0.9756\n",
      "iteration number: 2358\t training loss: 0.0679\tvalidation loss: 0.0811\t validation accuracy: 0.9756\n",
      "iteration number: 2359\t training loss: 0.0680\tvalidation loss: 0.0829\t validation accuracy: 0.9733\n",
      "iteration number: 2360\t training loss: 0.0675\tvalidation loss: 0.0815\t validation accuracy: 0.9756\n",
      "iteration number: 2361\t training loss: 0.0679\tvalidation loss: 0.0816\t validation accuracy: 0.9778\n",
      "iteration number: 2362\t training loss: 0.0683\tvalidation loss: 0.0811\t validation accuracy: 0.9800\n",
      "iteration number: 2363\t training loss: 0.0681\tvalidation loss: 0.0806\t validation accuracy: 0.9778\n",
      "iteration number: 2364\t training loss: 0.0695\tvalidation loss: 0.0820\t validation accuracy: 0.9778\n",
      "iteration number: 2365\t training loss: 0.0696\tvalidation loss: 0.0826\t validation accuracy: 0.9756\n",
      "iteration number: 2366\t training loss: 0.0689\tvalidation loss: 0.0818\t validation accuracy: 0.9778\n",
      "iteration number: 2367\t training loss: 0.0677\tvalidation loss: 0.0811\t validation accuracy: 0.9778\n",
      "iteration number: 2368\t training loss: 0.0680\tvalidation loss: 0.0822\t validation accuracy: 0.9733\n",
      "iteration number: 2369\t training loss: 0.0673\tvalidation loss: 0.0802\t validation accuracy: 0.9756\n",
      "iteration number: 2370\t training loss: 0.0672\tvalidation loss: 0.0802\t validation accuracy: 0.9756\n",
      "iteration number: 2371\t training loss: 0.0675\tvalidation loss: 0.0794\t validation accuracy: 0.9778\n",
      "iteration number: 2372\t training loss: 0.0675\tvalidation loss: 0.0802\t validation accuracy: 0.9756\n",
      "iteration number: 2373\t training loss: 0.0674\tvalidation loss: 0.0797\t validation accuracy: 0.9756\n",
      "iteration number: 2374\t training loss: 0.0676\tvalidation loss: 0.0797\t validation accuracy: 0.9778\n",
      "iteration number: 2375\t training loss: 0.0674\tvalidation loss: 0.0792\t validation accuracy: 0.9800\n",
      "iteration number: 2376\t training loss: 0.0671\tvalidation loss: 0.0789\t validation accuracy: 0.9800\n",
      "iteration number: 2377\t training loss: 0.0674\tvalidation loss: 0.0791\t validation accuracy: 0.9800\n",
      "iteration number: 2378\t training loss: 0.0674\tvalidation loss: 0.0790\t validation accuracy: 0.9778\n",
      "iteration number: 2379\t training loss: 0.0668\tvalidation loss: 0.0785\t validation accuracy: 0.9778\n",
      "iteration number: 2380\t training loss: 0.0675\tvalidation loss: 0.0781\t validation accuracy: 0.9800\n",
      "iteration number: 2381\t training loss: 0.0676\tvalidation loss: 0.0778\t validation accuracy: 0.9800\n",
      "iteration number: 2382\t training loss: 0.0674\tvalidation loss: 0.0784\t validation accuracy: 0.9800\n",
      "iteration number: 2383\t training loss: 0.0688\tvalidation loss: 0.0796\t validation accuracy: 0.9778\n",
      "iteration number: 2384\t training loss: 0.0701\tvalidation loss: 0.0805\t validation accuracy: 0.9756\n",
      "iteration number: 2385\t training loss: 0.0678\tvalidation loss: 0.0789\t validation accuracy: 0.9756\n",
      "iteration number: 2386\t training loss: 0.0701\tvalidation loss: 0.0812\t validation accuracy: 0.9778\n",
      "iteration number: 2387\t training loss: 0.0675\tvalidation loss: 0.0794\t validation accuracy: 0.9800\n",
      "iteration number: 2388\t training loss: 0.0676\tvalidation loss: 0.0794\t validation accuracy: 0.9800\n",
      "iteration number: 2389\t training loss: 0.0700\tvalidation loss: 0.0818\t validation accuracy: 0.9778\n",
      "iteration number: 2390\t training loss: 0.0696\tvalidation loss: 0.0805\t validation accuracy: 0.9778\n",
      "iteration number: 2391\t training loss: 0.0703\tvalidation loss: 0.0802\t validation accuracy: 0.9778\n",
      "iteration number: 2392\t training loss: 0.0712\tvalidation loss: 0.0810\t validation accuracy: 0.9778\n",
      "iteration number: 2393\t training loss: 0.0696\tvalidation loss: 0.0801\t validation accuracy: 0.9800\n",
      "iteration number: 2394\t training loss: 0.0684\tvalidation loss: 0.0792\t validation accuracy: 0.9800\n",
      "iteration number: 2395\t training loss: 0.0676\tvalidation loss: 0.0792\t validation accuracy: 0.9800\n",
      "iteration number: 2396\t training loss: 0.0677\tvalidation loss: 0.0793\t validation accuracy: 0.9800\n",
      "iteration number: 2397\t training loss: 0.0692\tvalidation loss: 0.0794\t validation accuracy: 0.9800\n",
      "iteration number: 2398\t training loss: 0.0686\tvalidation loss: 0.0791\t validation accuracy: 0.9800\n",
      "iteration number: 2399\t training loss: 0.0691\tvalidation loss: 0.0791\t validation accuracy: 0.9800\n",
      "iteration number: 2400\t training loss: 0.0685\tvalidation loss: 0.0787\t validation accuracy: 0.9778\n",
      "iteration number: 2401\t training loss: 0.0683\tvalidation loss: 0.0786\t validation accuracy: 0.9778\n",
      "iteration number: 2402\t training loss: 0.0674\tvalidation loss: 0.0792\t validation accuracy: 0.9778\n",
      "iteration number: 2403\t training loss: 0.0674\tvalidation loss: 0.0797\t validation accuracy: 0.9756\n",
      "iteration number: 2404\t training loss: 0.0674\tvalidation loss: 0.0812\t validation accuracy: 0.9756\n",
      "iteration number: 2405\t training loss: 0.0675\tvalidation loss: 0.0810\t validation accuracy: 0.9756\n",
      "iteration number: 2406\t training loss: 0.0670\tvalidation loss: 0.0805\t validation accuracy: 0.9756\n",
      "iteration number: 2407\t training loss: 0.0669\tvalidation loss: 0.0803\t validation accuracy: 0.9756\n",
      "iteration number: 2408\t training loss: 0.0674\tvalidation loss: 0.0809\t validation accuracy: 0.9778\n",
      "iteration number: 2409\t training loss: 0.0673\tvalidation loss: 0.0810\t validation accuracy: 0.9778\n",
      "iteration number: 2410\t training loss: 0.0672\tvalidation loss: 0.0803\t validation accuracy: 0.9778\n",
      "iteration number: 2411\t training loss: 0.0684\tvalidation loss: 0.0811\t validation accuracy: 0.9756\n",
      "iteration number: 2412\t training loss: 0.0713\tvalidation loss: 0.0836\t validation accuracy: 0.9733\n",
      "iteration number: 2413\t training loss: 0.0693\tvalidation loss: 0.0828\t validation accuracy: 0.9756\n",
      "iteration number: 2414\t training loss: 0.0664\tvalidation loss: 0.0806\t validation accuracy: 0.9756\n",
      "iteration number: 2415\t training loss: 0.0666\tvalidation loss: 0.0803\t validation accuracy: 0.9778\n",
      "iteration number: 2416\t training loss: 0.0694\tvalidation loss: 0.0831\t validation accuracy: 0.9733\n",
      "iteration number: 2417\t training loss: 0.0667\tvalidation loss: 0.0806\t validation accuracy: 0.9733\n",
      "iteration number: 2418\t training loss: 0.0669\tvalidation loss: 0.0814\t validation accuracy: 0.9756\n",
      "iteration number: 2419\t training loss: 0.0670\tvalidation loss: 0.0815\t validation accuracy: 0.9733\n",
      "iteration number: 2420\t training loss: 0.0659\tvalidation loss: 0.0795\t validation accuracy: 0.9756\n",
      "iteration number: 2421\t training loss: 0.0664\tvalidation loss: 0.0809\t validation accuracy: 0.9733\n",
      "iteration number: 2422\t training loss: 0.0664\tvalidation loss: 0.0810\t validation accuracy: 0.9733\n",
      "iteration number: 2423\t training loss: 0.0688\tvalidation loss: 0.0837\t validation accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2424\t training loss: 0.0671\tvalidation loss: 0.0815\t validation accuracy: 0.9733\n",
      "iteration number: 2425\t training loss: 0.0675\tvalidation loss: 0.0824\t validation accuracy: 0.9711\n",
      "iteration number: 2426\t training loss: 0.0668\tvalidation loss: 0.0808\t validation accuracy: 0.9733\n",
      "iteration number: 2427\t training loss: 0.0658\tvalidation loss: 0.0802\t validation accuracy: 0.9756\n",
      "iteration number: 2428\t training loss: 0.0663\tvalidation loss: 0.0805\t validation accuracy: 0.9778\n",
      "iteration number: 2429\t training loss: 0.0679\tvalidation loss: 0.0813\t validation accuracy: 0.9733\n",
      "iteration number: 2430\t training loss: 0.0710\tvalidation loss: 0.0863\t validation accuracy: 0.9689\n",
      "iteration number: 2431\t training loss: 0.0690\tvalidation loss: 0.0843\t validation accuracy: 0.9733\n",
      "iteration number: 2432\t training loss: 0.0699\tvalidation loss: 0.0839\t validation accuracy: 0.9711\n",
      "iteration number: 2433\t training loss: 0.0713\tvalidation loss: 0.0856\t validation accuracy: 0.9689\n",
      "iteration number: 2434\t training loss: 0.0674\tvalidation loss: 0.0820\t validation accuracy: 0.9733\n",
      "iteration number: 2435\t training loss: 0.0669\tvalidation loss: 0.0814\t validation accuracy: 0.9733\n",
      "iteration number: 2436\t training loss: 0.0666\tvalidation loss: 0.0806\t validation accuracy: 0.9778\n",
      "iteration number: 2437\t training loss: 0.0663\tvalidation loss: 0.0805\t validation accuracy: 0.9778\n",
      "iteration number: 2438\t training loss: 0.0665\tvalidation loss: 0.0812\t validation accuracy: 0.9733\n",
      "iteration number: 2439\t training loss: 0.0665\tvalidation loss: 0.0814\t validation accuracy: 0.9733\n",
      "iteration number: 2440\t training loss: 0.0679\tvalidation loss: 0.0829\t validation accuracy: 0.9733\n",
      "iteration number: 2441\t training loss: 0.0677\tvalidation loss: 0.0825\t validation accuracy: 0.9756\n",
      "iteration number: 2442\t training loss: 0.0667\tvalidation loss: 0.0822\t validation accuracy: 0.9711\n",
      "iteration number: 2443\t training loss: 0.0671\tvalidation loss: 0.0822\t validation accuracy: 0.9711\n",
      "iteration number: 2444\t training loss: 0.0666\tvalidation loss: 0.0816\t validation accuracy: 0.9733\n",
      "iteration number: 2445\t training loss: 0.0667\tvalidation loss: 0.0817\t validation accuracy: 0.9711\n",
      "iteration number: 2446\t training loss: 0.0664\tvalidation loss: 0.0811\t validation accuracy: 0.9733\n",
      "iteration number: 2447\t training loss: 0.0660\tvalidation loss: 0.0804\t validation accuracy: 0.9733\n",
      "iteration number: 2448\t training loss: 0.0654\tvalidation loss: 0.0800\t validation accuracy: 0.9778\n",
      "iteration number: 2449\t training loss: 0.0669\tvalidation loss: 0.0822\t validation accuracy: 0.9733\n",
      "iteration number: 2450\t training loss: 0.0665\tvalidation loss: 0.0812\t validation accuracy: 0.9733\n",
      "iteration number: 2451\t training loss: 0.0659\tvalidation loss: 0.0799\t validation accuracy: 0.9756\n",
      "iteration number: 2452\t training loss: 0.0659\tvalidation loss: 0.0798\t validation accuracy: 0.9756\n",
      "iteration number: 2453\t training loss: 0.0661\tvalidation loss: 0.0806\t validation accuracy: 0.9756\n",
      "iteration number: 2454\t training loss: 0.0653\tvalidation loss: 0.0785\t validation accuracy: 0.9778\n",
      "iteration number: 2455\t training loss: 0.0655\tvalidation loss: 0.0791\t validation accuracy: 0.9778\n",
      "iteration number: 2456\t training loss: 0.0657\tvalidation loss: 0.0797\t validation accuracy: 0.9756\n",
      "iteration number: 2457\t training loss: 0.0661\tvalidation loss: 0.0802\t validation accuracy: 0.9756\n",
      "iteration number: 2458\t training loss: 0.0659\tvalidation loss: 0.0798\t validation accuracy: 0.9756\n",
      "iteration number: 2459\t training loss: 0.0653\tvalidation loss: 0.0786\t validation accuracy: 0.9778\n",
      "iteration number: 2460\t training loss: 0.0658\tvalidation loss: 0.0793\t validation accuracy: 0.9800\n",
      "iteration number: 2461\t training loss: 0.0670\tvalidation loss: 0.0803\t validation accuracy: 0.9756\n",
      "iteration number: 2462\t training loss: 0.0664\tvalidation loss: 0.0799\t validation accuracy: 0.9778\n",
      "iteration number: 2463\t training loss: 0.0665\tvalidation loss: 0.0794\t validation accuracy: 0.9778\n",
      "iteration number: 2464\t training loss: 0.0656\tvalidation loss: 0.0787\t validation accuracy: 0.9800\n",
      "iteration number: 2465\t training loss: 0.0674\tvalidation loss: 0.0800\t validation accuracy: 0.9756\n",
      "iteration number: 2466\t training loss: 0.0671\tvalidation loss: 0.0797\t validation accuracy: 0.9800\n",
      "iteration number: 2467\t training loss: 0.0688\tvalidation loss: 0.0817\t validation accuracy: 0.9800\n",
      "iteration number: 2468\t training loss: 0.0666\tvalidation loss: 0.0794\t validation accuracy: 0.9800\n",
      "iteration number: 2469\t training loss: 0.0654\tvalidation loss: 0.0789\t validation accuracy: 0.9778\n",
      "iteration number: 2470\t training loss: 0.0655\tvalidation loss: 0.0793\t validation accuracy: 0.9778\n",
      "iteration number: 2471\t training loss: 0.0664\tvalidation loss: 0.0797\t validation accuracy: 0.9800\n",
      "iteration number: 2472\t training loss: 0.0662\tvalidation loss: 0.0805\t validation accuracy: 0.9800\n",
      "iteration number: 2473\t training loss: 0.0672\tvalidation loss: 0.0808\t validation accuracy: 0.9800\n",
      "iteration number: 2474\t training loss: 0.0699\tvalidation loss: 0.0829\t validation accuracy: 0.9756\n",
      "iteration number: 2475\t training loss: 0.0676\tvalidation loss: 0.0815\t validation accuracy: 0.9800\n",
      "iteration number: 2476\t training loss: 0.0661\tvalidation loss: 0.0805\t validation accuracy: 0.9800\n",
      "iteration number: 2477\t training loss: 0.0660\tvalidation loss: 0.0802\t validation accuracy: 0.9800\n",
      "iteration number: 2478\t training loss: 0.0661\tvalidation loss: 0.0800\t validation accuracy: 0.9800\n",
      "iteration number: 2479\t training loss: 0.0691\tvalidation loss: 0.0837\t validation accuracy: 0.9800\n",
      "iteration number: 2480\t training loss: 0.0682\tvalidation loss: 0.0823\t validation accuracy: 0.9800\n",
      "iteration number: 2481\t training loss: 0.0678\tvalidation loss: 0.0821\t validation accuracy: 0.9800\n",
      "iteration number: 2482\t training loss: 0.0672\tvalidation loss: 0.0810\t validation accuracy: 0.9800\n",
      "iteration number: 2483\t training loss: 0.0653\tvalidation loss: 0.0794\t validation accuracy: 0.9778\n",
      "iteration number: 2484\t training loss: 0.0650\tvalidation loss: 0.0790\t validation accuracy: 0.9778\n",
      "iteration number: 2485\t training loss: 0.0647\tvalidation loss: 0.0784\t validation accuracy: 0.9778\n",
      "iteration number: 2486\t training loss: 0.0645\tvalidation loss: 0.0761\t validation accuracy: 0.9778\n",
      "iteration number: 2487\t training loss: 0.0661\tvalidation loss: 0.0768\t validation accuracy: 0.9800\n",
      "iteration number: 2488\t training loss: 0.0645\tvalidation loss: 0.0765\t validation accuracy: 0.9778\n",
      "iteration number: 2489\t training loss: 0.0644\tvalidation loss: 0.0766\t validation accuracy: 0.9800\n",
      "iteration number: 2490\t training loss: 0.0655\tvalidation loss: 0.0781\t validation accuracy: 0.9800\n",
      "iteration number: 2491\t training loss: 0.0651\tvalidation loss: 0.0786\t validation accuracy: 0.9800\n",
      "iteration number: 2492\t training loss: 0.0645\tvalidation loss: 0.0778\t validation accuracy: 0.9800\n",
      "iteration number: 2493\t training loss: 0.0662\tvalidation loss: 0.0795\t validation accuracy: 0.9800\n",
      "iteration number: 2494\t training loss: 0.0687\tvalidation loss: 0.0813\t validation accuracy: 0.9778\n",
      "iteration number: 2495\t training loss: 0.0681\tvalidation loss: 0.0799\t validation accuracy: 0.9778\n",
      "iteration number: 2496\t training loss: 0.0677\tvalidation loss: 0.0802\t validation accuracy: 0.9756\n",
      "iteration number: 2497\t training loss: 0.0678\tvalidation loss: 0.0803\t validation accuracy: 0.9800\n",
      "iteration number: 2498\t training loss: 0.0661\tvalidation loss: 0.0789\t validation accuracy: 0.9800\n",
      "iteration number: 2499\t training loss: 0.0663\tvalidation loss: 0.0794\t validation accuracy: 0.9800\n",
      "iteration number: 2500\t training loss: 0.0659\tvalidation loss: 0.0787\t validation accuracy: 0.9800\n",
      "iteration number: 2501\t training loss: 0.0652\tvalidation loss: 0.0772\t validation accuracy: 0.9800\n",
      "iteration number: 2502\t training loss: 0.0653\tvalidation loss: 0.0775\t validation accuracy: 0.9800\n",
      "iteration number: 2503\t training loss: 0.0665\tvalidation loss: 0.0788\t validation accuracy: 0.9778\n",
      "iteration number: 2504\t training loss: 0.0660\tvalidation loss: 0.0791\t validation accuracy: 0.9778\n",
      "iteration number: 2505\t training loss: 0.0640\tvalidation loss: 0.0780\t validation accuracy: 0.9800\n",
      "iteration number: 2506\t training loss: 0.0638\tvalidation loss: 0.0768\t validation accuracy: 0.9800\n",
      "iteration number: 2507\t training loss: 0.0653\tvalidation loss: 0.0782\t validation accuracy: 0.9800\n",
      "iteration number: 2508\t training loss: 0.0658\tvalidation loss: 0.0791\t validation accuracy: 0.9778\n",
      "iteration number: 2509\t training loss: 0.0650\tvalidation loss: 0.0788\t validation accuracy: 0.9778\n",
      "iteration number: 2510\t training loss: 0.0653\tvalidation loss: 0.0794\t validation accuracy: 0.9756\n",
      "iteration number: 2511\t training loss: 0.0647\tvalidation loss: 0.0789\t validation accuracy: 0.9778\n",
      "iteration number: 2512\t training loss: 0.0678\tvalidation loss: 0.0805\t validation accuracy: 0.9756\n",
      "iteration number: 2513\t training loss: 0.0657\tvalidation loss: 0.0788\t validation accuracy: 0.9778\n",
      "iteration number: 2514\t training loss: 0.0658\tvalidation loss: 0.0788\t validation accuracy: 0.9778\n",
      "iteration number: 2515\t training loss: 0.0668\tvalidation loss: 0.0794\t validation accuracy: 0.9756\n",
      "iteration number: 2516\t training loss: 0.0664\tvalidation loss: 0.0789\t validation accuracy: 0.9756\n",
      "iteration number: 2517\t training loss: 0.0637\tvalidation loss: 0.0768\t validation accuracy: 0.9800\n",
      "iteration number: 2518\t training loss: 0.0643\tvalidation loss: 0.0776\t validation accuracy: 0.9800\n",
      "iteration number: 2519\t training loss: 0.0642\tvalidation loss: 0.0775\t validation accuracy: 0.9778\n",
      "iteration number: 2520\t training loss: 0.0652\tvalidation loss: 0.0785\t validation accuracy: 0.9800\n",
      "iteration number: 2521\t training loss: 0.0642\tvalidation loss: 0.0785\t validation accuracy: 0.9778\n",
      "iteration number: 2522\t training loss: 0.0642\tvalidation loss: 0.0775\t validation accuracy: 0.9778\n",
      "iteration number: 2523\t training loss: 0.0637\tvalidation loss: 0.0774\t validation accuracy: 0.9756\n",
      "iteration number: 2524\t training loss: 0.0637\tvalidation loss: 0.0780\t validation accuracy: 0.9756\n",
      "iteration number: 2525\t training loss: 0.0642\tvalidation loss: 0.0773\t validation accuracy: 0.9756\n",
      "iteration number: 2526\t training loss: 0.0634\tvalidation loss: 0.0768\t validation accuracy: 0.9756\n",
      "iteration number: 2527\t training loss: 0.0632\tvalidation loss: 0.0766\t validation accuracy: 0.9756\n",
      "iteration number: 2528\t training loss: 0.0630\tvalidation loss: 0.0765\t validation accuracy: 0.9778\n",
      "iteration number: 2529\t training loss: 0.0630\tvalidation loss: 0.0762\t validation accuracy: 0.9778\n",
      "iteration number: 2530\t training loss: 0.0631\tvalidation loss: 0.0770\t validation accuracy: 0.9756\n",
      "iteration number: 2531\t training loss: 0.0630\tvalidation loss: 0.0756\t validation accuracy: 0.9778\n",
      "iteration number: 2532\t training loss: 0.0632\tvalidation loss: 0.0755\t validation accuracy: 0.9800\n",
      "iteration number: 2533\t training loss: 0.0629\tvalidation loss: 0.0756\t validation accuracy: 0.9778\n",
      "iteration number: 2534\t training loss: 0.0630\tvalidation loss: 0.0758\t validation accuracy: 0.9778\n",
      "iteration number: 2535\t training loss: 0.0631\tvalidation loss: 0.0762\t validation accuracy: 0.9778\n",
      "iteration number: 2536\t training loss: 0.0631\tvalidation loss: 0.0766\t validation accuracy: 0.9778\n",
      "iteration number: 2537\t training loss: 0.0638\tvalidation loss: 0.0766\t validation accuracy: 0.9756\n",
      "iteration number: 2538\t training loss: 0.0633\tvalidation loss: 0.0760\t validation accuracy: 0.9800\n",
      "iteration number: 2539\t training loss: 0.0639\tvalidation loss: 0.0771\t validation accuracy: 0.9756\n",
      "iteration number: 2540\t training loss: 0.0637\tvalidation loss: 0.0763\t validation accuracy: 0.9800\n",
      "iteration number: 2541\t training loss: 0.0636\tvalidation loss: 0.0769\t validation accuracy: 0.9778\n",
      "iteration number: 2542\t training loss: 0.0638\tvalidation loss: 0.0770\t validation accuracy: 0.9800\n",
      "iteration number: 2543\t training loss: 0.0639\tvalidation loss: 0.0768\t validation accuracy: 0.9822\n",
      "iteration number: 2544\t training loss: 0.0642\tvalidation loss: 0.0770\t validation accuracy: 0.9778\n",
      "iteration number: 2545\t training loss: 0.0638\tvalidation loss: 0.0762\t validation accuracy: 0.9778\n",
      "iteration number: 2546\t training loss: 0.0630\tvalidation loss: 0.0755\t validation accuracy: 0.9800\n",
      "iteration number: 2547\t training loss: 0.0637\tvalidation loss: 0.0755\t validation accuracy: 0.9800\n",
      "iteration number: 2548\t training loss: 0.0637\tvalidation loss: 0.0767\t validation accuracy: 0.9800\n",
      "iteration number: 2549\t training loss: 0.0635\tvalidation loss: 0.0760\t validation accuracy: 0.9800\n",
      "iteration number: 2550\t training loss: 0.0633\tvalidation loss: 0.0761\t validation accuracy: 0.9800\n",
      "iteration number: 2551\t training loss: 0.0638\tvalidation loss: 0.0760\t validation accuracy: 0.9800\n",
      "iteration number: 2552\t training loss: 0.0639\tvalidation loss: 0.0760\t validation accuracy: 0.9800\n",
      "iteration number: 2553\t training loss: 0.0643\tvalidation loss: 0.0764\t validation accuracy: 0.9800\n",
      "iteration number: 2554\t training loss: 0.0635\tvalidation loss: 0.0761\t validation accuracy: 0.9800\n",
      "iteration number: 2555\t training loss: 0.0632\tvalidation loss: 0.0764\t validation accuracy: 0.9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2556\t training loss: 0.0628\tvalidation loss: 0.0762\t validation accuracy: 0.9822\n",
      "iteration number: 2557\t training loss: 0.0627\tvalidation loss: 0.0760\t validation accuracy: 0.9822\n",
      "iteration number: 2558\t training loss: 0.0632\tvalidation loss: 0.0761\t validation accuracy: 0.9822\n",
      "iteration number: 2559\t training loss: 0.0633\tvalidation loss: 0.0759\t validation accuracy: 0.9800\n",
      "iteration number: 2560\t training loss: 0.0640\tvalidation loss: 0.0771\t validation accuracy: 0.9800\n",
      "iteration number: 2561\t training loss: 0.0632\tvalidation loss: 0.0766\t validation accuracy: 0.9822\n",
      "iteration number: 2562\t training loss: 0.0632\tvalidation loss: 0.0762\t validation accuracy: 0.9822\n",
      "iteration number: 2563\t training loss: 0.0642\tvalidation loss: 0.0762\t validation accuracy: 0.9800\n",
      "iteration number: 2564\t training loss: 0.0648\tvalidation loss: 0.0764\t validation accuracy: 0.9800\n",
      "iteration number: 2565\t training loss: 0.0637\tvalidation loss: 0.0758\t validation accuracy: 0.9822\n",
      "iteration number: 2566\t training loss: 0.0650\tvalidation loss: 0.0764\t validation accuracy: 0.9778\n",
      "iteration number: 2567\t training loss: 0.0632\tvalidation loss: 0.0760\t validation accuracy: 0.9822\n",
      "iteration number: 2568\t training loss: 0.0643\tvalidation loss: 0.0773\t validation accuracy: 0.9800\n",
      "iteration number: 2569\t training loss: 0.0643\tvalidation loss: 0.0768\t validation accuracy: 0.9800\n",
      "iteration number: 2570\t training loss: 0.0656\tvalidation loss: 0.0776\t validation accuracy: 0.9822\n",
      "iteration number: 2571\t training loss: 0.0643\tvalidation loss: 0.0769\t validation accuracy: 0.9822\n",
      "iteration number: 2572\t training loss: 0.0646\tvalidation loss: 0.0772\t validation accuracy: 0.9822\n",
      "iteration number: 2573\t training loss: 0.0633\tvalidation loss: 0.0768\t validation accuracy: 0.9822\n",
      "iteration number: 2574\t training loss: 0.0636\tvalidation loss: 0.0764\t validation accuracy: 0.9822\n",
      "iteration number: 2575\t training loss: 0.0624\tvalidation loss: 0.0754\t validation accuracy: 0.9844\n",
      "iteration number: 2576\t training loss: 0.0627\tvalidation loss: 0.0754\t validation accuracy: 0.9800\n",
      "iteration number: 2577\t training loss: 0.0625\tvalidation loss: 0.0753\t validation accuracy: 0.9822\n",
      "iteration number: 2578\t training loss: 0.0629\tvalidation loss: 0.0754\t validation accuracy: 0.9822\n",
      "iteration number: 2579\t training loss: 0.0632\tvalidation loss: 0.0763\t validation accuracy: 0.9822\n",
      "iteration number: 2580\t training loss: 0.0634\tvalidation loss: 0.0765\t validation accuracy: 0.9822\n",
      "iteration number: 2581\t training loss: 0.0633\tvalidation loss: 0.0767\t validation accuracy: 0.9800\n",
      "iteration number: 2582\t training loss: 0.0641\tvalidation loss: 0.0770\t validation accuracy: 0.9800\n",
      "iteration number: 2583\t training loss: 0.0639\tvalidation loss: 0.0772\t validation accuracy: 0.9822\n",
      "iteration number: 2584\t training loss: 0.0640\tvalidation loss: 0.0766\t validation accuracy: 0.9778\n",
      "iteration number: 2585\t training loss: 0.0636\tvalidation loss: 0.0761\t validation accuracy: 0.9800\n",
      "iteration number: 2586\t training loss: 0.0637\tvalidation loss: 0.0764\t validation accuracy: 0.9800\n",
      "iteration number: 2587\t training loss: 0.0640\tvalidation loss: 0.0773\t validation accuracy: 0.9778\n",
      "iteration number: 2588\t training loss: 0.0631\tvalidation loss: 0.0772\t validation accuracy: 0.9778\n",
      "iteration number: 2589\t training loss: 0.0635\tvalidation loss: 0.0765\t validation accuracy: 0.9778\n",
      "iteration number: 2590\t training loss: 0.0627\tvalidation loss: 0.0762\t validation accuracy: 0.9778\n",
      "iteration number: 2591\t training loss: 0.0633\tvalidation loss: 0.0761\t validation accuracy: 0.9778\n",
      "iteration number: 2592\t training loss: 0.0623\tvalidation loss: 0.0750\t validation accuracy: 0.9822\n",
      "iteration number: 2593\t training loss: 0.0623\tvalidation loss: 0.0745\t validation accuracy: 0.9822\n",
      "iteration number: 2594\t training loss: 0.0629\tvalidation loss: 0.0747\t validation accuracy: 0.9822\n",
      "iteration number: 2595\t training loss: 0.0628\tvalidation loss: 0.0754\t validation accuracy: 0.9822\n",
      "iteration number: 2596\t training loss: 0.0643\tvalidation loss: 0.0756\t validation accuracy: 0.9800\n",
      "iteration number: 2597\t training loss: 0.0624\tvalidation loss: 0.0757\t validation accuracy: 0.9800\n",
      "iteration number: 2598\t training loss: 0.0626\tvalidation loss: 0.0760\t validation accuracy: 0.9800\n",
      "iteration number: 2599\t training loss: 0.0635\tvalidation loss: 0.0770\t validation accuracy: 0.9800\n",
      "iteration number: 2600\t training loss: 0.0631\tvalidation loss: 0.0769\t validation accuracy: 0.9778\n",
      "iteration number: 2601\t training loss: 0.0637\tvalidation loss: 0.0769\t validation accuracy: 0.9778\n",
      "iteration number: 2602\t training loss: 0.0644\tvalidation loss: 0.0782\t validation accuracy: 0.9778\n",
      "iteration number: 2603\t training loss: 0.0632\tvalidation loss: 0.0782\t validation accuracy: 0.9778\n",
      "iteration number: 2604\t training loss: 0.0633\tvalidation loss: 0.0782\t validation accuracy: 0.9756\n",
      "iteration number: 2605\t training loss: 0.0623\tvalidation loss: 0.0783\t validation accuracy: 0.9756\n",
      "iteration number: 2606\t training loss: 0.0627\tvalidation loss: 0.0786\t validation accuracy: 0.9756\n",
      "iteration number: 2607\t training loss: 0.0631\tvalidation loss: 0.0788\t validation accuracy: 0.9778\n",
      "iteration number: 2608\t training loss: 0.0630\tvalidation loss: 0.0791\t validation accuracy: 0.9778\n",
      "iteration number: 2609\t training loss: 0.0637\tvalidation loss: 0.0803\t validation accuracy: 0.9733\n",
      "iteration number: 2610\t training loss: 0.0627\tvalidation loss: 0.0794\t validation accuracy: 0.9733\n",
      "iteration number: 2611\t training loss: 0.0618\tvalidation loss: 0.0783\t validation accuracy: 0.9756\n",
      "iteration number: 2612\t training loss: 0.0632\tvalidation loss: 0.0806\t validation accuracy: 0.9756\n",
      "iteration number: 2613\t training loss: 0.0637\tvalidation loss: 0.0815\t validation accuracy: 0.9733\n",
      "iteration number: 2614\t training loss: 0.0637\tvalidation loss: 0.0818\t validation accuracy: 0.9733\n",
      "iteration number: 2615\t training loss: 0.0635\tvalidation loss: 0.0817\t validation accuracy: 0.9733\n",
      "iteration number: 2616\t training loss: 0.0630\tvalidation loss: 0.0800\t validation accuracy: 0.9756\n",
      "iteration number: 2617\t training loss: 0.0629\tvalidation loss: 0.0793\t validation accuracy: 0.9778\n",
      "iteration number: 2618\t training loss: 0.0623\tvalidation loss: 0.0780\t validation accuracy: 0.9778\n",
      "iteration number: 2619\t training loss: 0.0622\tvalidation loss: 0.0772\t validation accuracy: 0.9822\n",
      "iteration number: 2620\t training loss: 0.0622\tvalidation loss: 0.0778\t validation accuracy: 0.9778\n",
      "iteration number: 2621\t training loss: 0.0620\tvalidation loss: 0.0783\t validation accuracy: 0.9756\n",
      "iteration number: 2622\t training loss: 0.0615\tvalidation loss: 0.0788\t validation accuracy: 0.9756\n",
      "iteration number: 2623\t training loss: 0.0626\tvalidation loss: 0.0805\t validation accuracy: 0.9756\n",
      "iteration number: 2624\t training loss: 0.0622\tvalidation loss: 0.0790\t validation accuracy: 0.9778\n",
      "iteration number: 2625\t training loss: 0.0615\tvalidation loss: 0.0781\t validation accuracy: 0.9756\n",
      "iteration number: 2626\t training loss: 0.0620\tvalidation loss: 0.0783\t validation accuracy: 0.9778\n",
      "iteration number: 2627\t training loss: 0.0612\tvalidation loss: 0.0772\t validation accuracy: 0.9778\n",
      "iteration number: 2628\t training loss: 0.0612\tvalidation loss: 0.0771\t validation accuracy: 0.9778\n",
      "iteration number: 2629\t training loss: 0.0625\tvalidation loss: 0.0777\t validation accuracy: 0.9778\n",
      "iteration number: 2630\t training loss: 0.0624\tvalidation loss: 0.0779\t validation accuracy: 0.9778\n",
      "iteration number: 2631\t training loss: 0.0620\tvalidation loss: 0.0769\t validation accuracy: 0.9800\n",
      "iteration number: 2632\t training loss: 0.0619\tvalidation loss: 0.0771\t validation accuracy: 0.9778\n",
      "iteration number: 2633\t training loss: 0.0623\tvalidation loss: 0.0781\t validation accuracy: 0.9778\n",
      "iteration number: 2634\t training loss: 0.0620\tvalidation loss: 0.0778\t validation accuracy: 0.9756\n",
      "iteration number: 2635\t training loss: 0.0617\tvalidation loss: 0.0769\t validation accuracy: 0.9778\n",
      "iteration number: 2636\t training loss: 0.0612\tvalidation loss: 0.0762\t validation accuracy: 0.9778\n",
      "iteration number: 2637\t training loss: 0.0611\tvalidation loss: 0.0750\t validation accuracy: 0.9822\n",
      "iteration number: 2638\t training loss: 0.0645\tvalidation loss: 0.0774\t validation accuracy: 0.9822\n",
      "iteration number: 2639\t training loss: 0.0630\tvalidation loss: 0.0771\t validation accuracy: 0.9778\n",
      "iteration number: 2640\t training loss: 0.0626\tvalidation loss: 0.0765\t validation accuracy: 0.9800\n",
      "iteration number: 2641\t training loss: 0.0613\tvalidation loss: 0.0757\t validation accuracy: 0.9756\n",
      "iteration number: 2642\t training loss: 0.0613\tvalidation loss: 0.0757\t validation accuracy: 0.9778\n",
      "iteration number: 2643\t training loss: 0.0608\tvalidation loss: 0.0759\t validation accuracy: 0.9756\n",
      "iteration number: 2644\t training loss: 0.0630\tvalidation loss: 0.0791\t validation accuracy: 0.9711\n",
      "iteration number: 2645\t training loss: 0.0616\tvalidation loss: 0.0774\t validation accuracy: 0.9756\n",
      "iteration number: 2646\t training loss: 0.0612\tvalidation loss: 0.0772\t validation accuracy: 0.9756\n",
      "iteration number: 2647\t training loss: 0.0617\tvalidation loss: 0.0779\t validation accuracy: 0.9756\n",
      "iteration number: 2648\t training loss: 0.0607\tvalidation loss: 0.0766\t validation accuracy: 0.9778\n",
      "iteration number: 2649\t training loss: 0.0606\tvalidation loss: 0.0766\t validation accuracy: 0.9778\n",
      "iteration number: 2650\t training loss: 0.0608\tvalidation loss: 0.0764\t validation accuracy: 0.9756\n",
      "iteration number: 2651\t training loss: 0.0608\tvalidation loss: 0.0771\t validation accuracy: 0.9778\n",
      "iteration number: 2652\t training loss: 0.0606\tvalidation loss: 0.0753\t validation accuracy: 0.9778\n",
      "iteration number: 2653\t training loss: 0.0606\tvalidation loss: 0.0748\t validation accuracy: 0.9778\n",
      "iteration number: 2654\t training loss: 0.0608\tvalidation loss: 0.0749\t validation accuracy: 0.9778\n",
      "iteration number: 2655\t training loss: 0.0607\tvalidation loss: 0.0750\t validation accuracy: 0.9778\n",
      "iteration number: 2656\t training loss: 0.0616\tvalidation loss: 0.0756\t validation accuracy: 0.9778\n",
      "iteration number: 2657\t training loss: 0.0614\tvalidation loss: 0.0765\t validation accuracy: 0.9778\n",
      "iteration number: 2658\t training loss: 0.0607\tvalidation loss: 0.0757\t validation accuracy: 0.9800\n",
      "iteration number: 2659\t training loss: 0.0614\tvalidation loss: 0.0758\t validation accuracy: 0.9778\n",
      "iteration number: 2660\t training loss: 0.0605\tvalidation loss: 0.0742\t validation accuracy: 0.9800\n",
      "iteration number: 2661\t training loss: 0.0604\tvalidation loss: 0.0740\t validation accuracy: 0.9822\n",
      "iteration number: 2662\t training loss: 0.0604\tvalidation loss: 0.0745\t validation accuracy: 0.9800\n",
      "iteration number: 2663\t training loss: 0.0612\tvalidation loss: 0.0752\t validation accuracy: 0.9800\n",
      "iteration number: 2664\t training loss: 0.0612\tvalidation loss: 0.0756\t validation accuracy: 0.9778\n",
      "iteration number: 2665\t training loss: 0.0612\tvalidation loss: 0.0759\t validation accuracy: 0.9778\n",
      "iteration number: 2666\t training loss: 0.0608\tvalidation loss: 0.0752\t validation accuracy: 0.9778\n",
      "iteration number: 2667\t training loss: 0.0615\tvalidation loss: 0.0772\t validation accuracy: 0.9800\n",
      "iteration number: 2668\t training loss: 0.0611\tvalidation loss: 0.0765\t validation accuracy: 0.9800\n",
      "iteration number: 2669\t training loss: 0.0602\tvalidation loss: 0.0752\t validation accuracy: 0.9800\n",
      "iteration number: 2670\t training loss: 0.0599\tvalidation loss: 0.0746\t validation accuracy: 0.9800\n",
      "iteration number: 2671\t training loss: 0.0599\tvalidation loss: 0.0748\t validation accuracy: 0.9778\n",
      "iteration number: 2672\t training loss: 0.0610\tvalidation loss: 0.0757\t validation accuracy: 0.9800\n",
      "iteration number: 2673\t training loss: 0.0627\tvalidation loss: 0.0766\t validation accuracy: 0.9756\n",
      "iteration number: 2674\t training loss: 0.0633\tvalidation loss: 0.0772\t validation accuracy: 0.9756\n",
      "iteration number: 2675\t training loss: 0.0627\tvalidation loss: 0.0768\t validation accuracy: 0.9756\n",
      "iteration number: 2676\t training loss: 0.0608\tvalidation loss: 0.0750\t validation accuracy: 0.9800\n",
      "iteration number: 2677\t training loss: 0.0606\tvalidation loss: 0.0746\t validation accuracy: 0.9822\n",
      "iteration number: 2678\t training loss: 0.0605\tvalidation loss: 0.0746\t validation accuracy: 0.9822\n",
      "iteration number: 2679\t training loss: 0.0601\tvalidation loss: 0.0738\t validation accuracy: 0.9822\n",
      "iteration number: 2680\t training loss: 0.0620\tvalidation loss: 0.0758\t validation accuracy: 0.9778\n",
      "iteration number: 2681\t training loss: 0.0686\tvalidation loss: 0.0808\t validation accuracy: 0.9756\n",
      "iteration number: 2682\t training loss: 0.0636\tvalidation loss: 0.0771\t validation accuracy: 0.9778\n",
      "iteration number: 2683\t training loss: 0.0603\tvalidation loss: 0.0740\t validation accuracy: 0.9844\n",
      "iteration number: 2684\t training loss: 0.0599\tvalidation loss: 0.0738\t validation accuracy: 0.9844\n",
      "iteration number: 2685\t training loss: 0.0602\tvalidation loss: 0.0739\t validation accuracy: 0.9822\n",
      "iteration number: 2686\t training loss: 0.0614\tvalidation loss: 0.0773\t validation accuracy: 0.9733\n",
      "iteration number: 2687\t training loss: 0.0608\tvalidation loss: 0.0764\t validation accuracy: 0.9778\n",
      "iteration number: 2688\t training loss: 0.0609\tvalidation loss: 0.0761\t validation accuracy: 0.9778\n",
      "iteration number: 2689\t training loss: 0.0619\tvalidation loss: 0.0765\t validation accuracy: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2690\t training loss: 0.0619\tvalidation loss: 0.0781\t validation accuracy: 0.9733\n",
      "iteration number: 2691\t training loss: 0.0616\tvalidation loss: 0.0773\t validation accuracy: 0.9733\n",
      "iteration number: 2692\t training loss: 0.0616\tvalidation loss: 0.0773\t validation accuracy: 0.9733\n",
      "iteration number: 2693\t training loss: 0.0611\tvalidation loss: 0.0772\t validation accuracy: 0.9778\n",
      "iteration number: 2694\t training loss: 0.0602\tvalidation loss: 0.0750\t validation accuracy: 0.9800\n",
      "iteration number: 2695\t training loss: 0.0600\tvalidation loss: 0.0748\t validation accuracy: 0.9778\n",
      "iteration number: 2696\t training loss: 0.0599\tvalidation loss: 0.0747\t validation accuracy: 0.9800\n",
      "iteration number: 2697\t training loss: 0.0613\tvalidation loss: 0.0758\t validation accuracy: 0.9800\n",
      "iteration number: 2698\t training loss: 0.0599\tvalidation loss: 0.0743\t validation accuracy: 0.9822\n",
      "iteration number: 2699\t training loss: 0.0600\tvalidation loss: 0.0740\t validation accuracy: 0.9822\n",
      "iteration number: 2700\t training loss: 0.0603\tvalidation loss: 0.0744\t validation accuracy: 0.9800\n",
      "iteration number: 2701\t training loss: 0.0602\tvalidation loss: 0.0740\t validation accuracy: 0.9800\n",
      "iteration number: 2702\t training loss: 0.0598\tvalidation loss: 0.0733\t validation accuracy: 0.9800\n",
      "iteration number: 2703\t training loss: 0.0601\tvalidation loss: 0.0735\t validation accuracy: 0.9822\n",
      "iteration number: 2704\t training loss: 0.0609\tvalidation loss: 0.0745\t validation accuracy: 0.9844\n",
      "iteration number: 2705\t training loss: 0.0614\tvalidation loss: 0.0747\t validation accuracy: 0.9822\n",
      "iteration number: 2706\t training loss: 0.0598\tvalidation loss: 0.0743\t validation accuracy: 0.9800\n",
      "iteration number: 2707\t training loss: 0.0603\tvalidation loss: 0.0745\t validation accuracy: 0.9800\n",
      "iteration number: 2708\t training loss: 0.0607\tvalidation loss: 0.0747\t validation accuracy: 0.9822\n",
      "iteration number: 2709\t training loss: 0.0598\tvalidation loss: 0.0741\t validation accuracy: 0.9822\n",
      "iteration number: 2710\t training loss: 0.0596\tvalidation loss: 0.0741\t validation accuracy: 0.9822\n",
      "iteration number: 2711\t training loss: 0.0596\tvalidation loss: 0.0737\t validation accuracy: 0.9822\n",
      "iteration number: 2712\t training loss: 0.0600\tvalidation loss: 0.0736\t validation accuracy: 0.9822\n",
      "iteration number: 2713\t training loss: 0.0603\tvalidation loss: 0.0735\t validation accuracy: 0.9822\n",
      "iteration number: 2714\t training loss: 0.0598\tvalidation loss: 0.0739\t validation accuracy: 0.9800\n",
      "iteration number: 2715\t training loss: 0.0600\tvalidation loss: 0.0743\t validation accuracy: 0.9800\n",
      "iteration number: 2716\t training loss: 0.0604\tvalidation loss: 0.0745\t validation accuracy: 0.9800\n",
      "iteration number: 2717\t training loss: 0.0597\tvalidation loss: 0.0743\t validation accuracy: 0.9800\n",
      "iteration number: 2718\t training loss: 0.0593\tvalidation loss: 0.0746\t validation accuracy: 0.9800\n",
      "iteration number: 2719\t training loss: 0.0595\tvalidation loss: 0.0749\t validation accuracy: 0.9800\n",
      "iteration number: 2720\t training loss: 0.0603\tvalidation loss: 0.0752\t validation accuracy: 0.9800\n",
      "iteration number: 2721\t training loss: 0.0615\tvalidation loss: 0.0764\t validation accuracy: 0.9800\n",
      "iteration number: 2722\t training loss: 0.0599\tvalidation loss: 0.0750\t validation accuracy: 0.9822\n",
      "iteration number: 2723\t training loss: 0.0593\tvalidation loss: 0.0743\t validation accuracy: 0.9778\n",
      "iteration number: 2724\t training loss: 0.0591\tvalidation loss: 0.0742\t validation accuracy: 0.9778\n",
      "iteration number: 2725\t training loss: 0.0608\tvalidation loss: 0.0749\t validation accuracy: 0.9822\n",
      "iteration number: 2726\t training loss: 0.0605\tvalidation loss: 0.0760\t validation accuracy: 0.9800\n",
      "iteration number: 2727\t training loss: 0.0594\tvalidation loss: 0.0754\t validation accuracy: 0.9756\n",
      "iteration number: 2728\t training loss: 0.0584\tvalidation loss: 0.0739\t validation accuracy: 0.9800\n",
      "iteration number: 2729\t training loss: 0.0585\tvalidation loss: 0.0739\t validation accuracy: 0.9822\n",
      "iteration number: 2730\t training loss: 0.0585\tvalidation loss: 0.0737\t validation accuracy: 0.9822\n",
      "iteration number: 2731\t training loss: 0.0586\tvalidation loss: 0.0740\t validation accuracy: 0.9800\n",
      "iteration number: 2732\t training loss: 0.0588\tvalidation loss: 0.0739\t validation accuracy: 0.9800\n",
      "iteration number: 2733\t training loss: 0.0590\tvalidation loss: 0.0737\t validation accuracy: 0.9822\n",
      "iteration number: 2734\t training loss: 0.0593\tvalidation loss: 0.0737\t validation accuracy: 0.9844\n",
      "iteration number: 2735\t training loss: 0.0591\tvalidation loss: 0.0735\t validation accuracy: 0.9822\n",
      "iteration number: 2736\t training loss: 0.0596\tvalidation loss: 0.0741\t validation accuracy: 0.9800\n",
      "iteration number: 2737\t training loss: 0.0593\tvalidation loss: 0.0738\t validation accuracy: 0.9800\n",
      "iteration number: 2738\t training loss: 0.0592\tvalidation loss: 0.0736\t validation accuracy: 0.9800\n",
      "iteration number: 2739\t training loss: 0.0600\tvalidation loss: 0.0746\t validation accuracy: 0.9800\n",
      "iteration number: 2740\t training loss: 0.0595\tvalidation loss: 0.0748\t validation accuracy: 0.9822\n",
      "iteration number: 2741\t training loss: 0.0592\tvalidation loss: 0.0747\t validation accuracy: 0.9800\n",
      "iteration number: 2742\t training loss: 0.0590\tvalidation loss: 0.0745\t validation accuracy: 0.9756\n",
      "iteration number: 2743\t training loss: 0.0594\tvalidation loss: 0.0752\t validation accuracy: 0.9800\n",
      "iteration number: 2744\t training loss: 0.0584\tvalidation loss: 0.0749\t validation accuracy: 0.9756\n",
      "iteration number: 2745\t training loss: 0.0592\tvalidation loss: 0.0759\t validation accuracy: 0.9778\n",
      "iteration number: 2746\t training loss: 0.0595\tvalidation loss: 0.0756\t validation accuracy: 0.9756\n",
      "iteration number: 2747\t training loss: 0.0605\tvalidation loss: 0.0763\t validation accuracy: 0.9756\n",
      "iteration number: 2748\t training loss: 0.0599\tvalidation loss: 0.0753\t validation accuracy: 0.9778\n",
      "iteration number: 2749\t training loss: 0.0592\tvalidation loss: 0.0751\t validation accuracy: 0.9800\n",
      "iteration number: 2750\t training loss: 0.0586\tvalidation loss: 0.0735\t validation accuracy: 0.9800\n",
      "iteration number: 2751\t training loss: 0.0584\tvalidation loss: 0.0740\t validation accuracy: 0.9800\n",
      "iteration number: 2752\t training loss: 0.0584\tvalidation loss: 0.0739\t validation accuracy: 0.9800\n",
      "iteration number: 2753\t training loss: 0.0587\tvalidation loss: 0.0748\t validation accuracy: 0.9800\n",
      "iteration number: 2754\t training loss: 0.0594\tvalidation loss: 0.0756\t validation accuracy: 0.9800\n",
      "iteration number: 2755\t training loss: 0.0586\tvalidation loss: 0.0749\t validation accuracy: 0.9778\n",
      "iteration number: 2756\t training loss: 0.0590\tvalidation loss: 0.0761\t validation accuracy: 0.9778\n",
      "iteration number: 2757\t training loss: 0.0590\tvalidation loss: 0.0749\t validation accuracy: 0.9778\n",
      "iteration number: 2758\t training loss: 0.0585\tvalidation loss: 0.0735\t validation accuracy: 0.9800\n",
      "iteration number: 2759\t training loss: 0.0583\tvalidation loss: 0.0749\t validation accuracy: 0.9778\n",
      "iteration number: 2760\t training loss: 0.0582\tvalidation loss: 0.0749\t validation accuracy: 0.9778\n",
      "iteration number: 2761\t training loss: 0.0582\tvalidation loss: 0.0743\t validation accuracy: 0.9778\n",
      "iteration number: 2762\t training loss: 0.0577\tvalidation loss: 0.0735\t validation accuracy: 0.9800\n",
      "iteration number: 2763\t training loss: 0.0592\tvalidation loss: 0.0749\t validation accuracy: 0.9800\n",
      "iteration number: 2764\t training loss: 0.0591\tvalidation loss: 0.0745\t validation accuracy: 0.9800\n",
      "iteration number: 2765\t training loss: 0.0595\tvalidation loss: 0.0755\t validation accuracy: 0.9800\n",
      "iteration number: 2766\t training loss: 0.0589\tvalidation loss: 0.0748\t validation accuracy: 0.9800\n",
      "iteration number: 2767\t training loss: 0.0584\tvalidation loss: 0.0744\t validation accuracy: 0.9822\n",
      "iteration number: 2768\t training loss: 0.0592\tvalidation loss: 0.0750\t validation accuracy: 0.9800\n",
      "iteration number: 2769\t training loss: 0.0586\tvalidation loss: 0.0735\t validation accuracy: 0.9822\n",
      "iteration number: 2770\t training loss: 0.0581\tvalidation loss: 0.0732\t validation accuracy: 0.9822\n",
      "iteration number: 2771\t training loss: 0.0588\tvalidation loss: 0.0724\t validation accuracy: 0.9844\n",
      "iteration number: 2772\t training loss: 0.0594\tvalidation loss: 0.0728\t validation accuracy: 0.9844\n",
      "iteration number: 2773\t training loss: 0.0584\tvalidation loss: 0.0726\t validation accuracy: 0.9844\n",
      "iteration number: 2774\t training loss: 0.0583\tvalidation loss: 0.0724\t validation accuracy: 0.9822\n",
      "iteration number: 2775\t training loss: 0.0579\tvalidation loss: 0.0720\t validation accuracy: 0.9822\n",
      "iteration number: 2776\t training loss: 0.0577\tvalidation loss: 0.0728\t validation accuracy: 0.9778\n",
      "iteration number: 2777\t training loss: 0.0606\tvalidation loss: 0.0747\t validation accuracy: 0.9778\n",
      "iteration number: 2778\t training loss: 0.0587\tvalidation loss: 0.0730\t validation accuracy: 0.9778\n",
      "iteration number: 2779\t training loss: 0.0604\tvalidation loss: 0.0747\t validation accuracy: 0.9778\n",
      "iteration number: 2780\t training loss: 0.0582\tvalidation loss: 0.0734\t validation accuracy: 0.9800\n",
      "iteration number: 2781\t training loss: 0.0577\tvalidation loss: 0.0728\t validation accuracy: 0.9822\n",
      "iteration number: 2782\t training loss: 0.0575\tvalidation loss: 0.0734\t validation accuracy: 0.9822\n",
      "iteration number: 2783\t training loss: 0.0580\tvalidation loss: 0.0746\t validation accuracy: 0.9756\n",
      "iteration number: 2784\t training loss: 0.0577\tvalidation loss: 0.0736\t validation accuracy: 0.9778\n",
      "iteration number: 2785\t training loss: 0.0599\tvalidation loss: 0.0775\t validation accuracy: 0.9733\n",
      "iteration number: 2786\t training loss: 0.0587\tvalidation loss: 0.0754\t validation accuracy: 0.9756\n",
      "iteration number: 2787\t training loss: 0.0589\tvalidation loss: 0.0759\t validation accuracy: 0.9756\n",
      "iteration number: 2788\t training loss: 0.0593\tvalidation loss: 0.0758\t validation accuracy: 0.9778\n",
      "iteration number: 2789\t training loss: 0.0582\tvalidation loss: 0.0744\t validation accuracy: 0.9800\n",
      "iteration number: 2790\t training loss: 0.0582\tvalidation loss: 0.0739\t validation accuracy: 0.9778\n",
      "iteration number: 2791\t training loss: 0.0584\tvalidation loss: 0.0741\t validation accuracy: 0.9778\n",
      "iteration number: 2792\t training loss: 0.0582\tvalidation loss: 0.0736\t validation accuracy: 0.9778\n",
      "iteration number: 2793\t training loss: 0.0583\tvalidation loss: 0.0729\t validation accuracy: 0.9800\n",
      "iteration number: 2794\t training loss: 0.0578\tvalidation loss: 0.0727\t validation accuracy: 0.9800\n",
      "iteration number: 2795\t training loss: 0.0595\tvalidation loss: 0.0764\t validation accuracy: 0.9733\n",
      "iteration number: 2796\t training loss: 0.0591\tvalidation loss: 0.0754\t validation accuracy: 0.9778\n",
      "iteration number: 2797\t training loss: 0.0593\tvalidation loss: 0.0762\t validation accuracy: 0.9733\n",
      "iteration number: 2798\t training loss: 0.0592\tvalidation loss: 0.0755\t validation accuracy: 0.9756\n",
      "iteration number: 2799\t training loss: 0.0591\tvalidation loss: 0.0756\t validation accuracy: 0.9778\n",
      "iteration number: 2800\t training loss: 0.0589\tvalidation loss: 0.0754\t validation accuracy: 0.9800\n",
      "iteration number: 2801\t training loss: 0.0581\tvalidation loss: 0.0746\t validation accuracy: 0.9800\n",
      "iteration number: 2802\t training loss: 0.0577\tvalidation loss: 0.0741\t validation accuracy: 0.9800\n",
      "iteration number: 2803\t training loss: 0.0574\tvalidation loss: 0.0748\t validation accuracy: 0.9800\n",
      "iteration number: 2804\t training loss: 0.0573\tvalidation loss: 0.0741\t validation accuracy: 0.9800\n",
      "iteration number: 2805\t training loss: 0.0582\tvalidation loss: 0.0743\t validation accuracy: 0.9778\n",
      "iteration number: 2806\t training loss: 0.0571\tvalidation loss: 0.0741\t validation accuracy: 0.9778\n",
      "iteration number: 2807\t training loss: 0.0569\tvalidation loss: 0.0738\t validation accuracy: 0.9778\n",
      "iteration number: 2808\t training loss: 0.0568\tvalidation loss: 0.0736\t validation accuracy: 0.9800\n",
      "iteration number: 2809\t training loss: 0.0569\tvalidation loss: 0.0731\t validation accuracy: 0.9778\n",
      "iteration number: 2810\t training loss: 0.0571\tvalidation loss: 0.0732\t validation accuracy: 0.9800\n",
      "iteration number: 2811\t training loss: 0.0582\tvalidation loss: 0.0767\t validation accuracy: 0.9756\n",
      "iteration number: 2812\t training loss: 0.0584\tvalidation loss: 0.0770\t validation accuracy: 0.9733\n",
      "iteration number: 2813\t training loss: 0.0577\tvalidation loss: 0.0755\t validation accuracy: 0.9756\n",
      "iteration number: 2814\t training loss: 0.0571\tvalidation loss: 0.0745\t validation accuracy: 0.9756\n",
      "iteration number: 2815\t training loss: 0.0580\tvalidation loss: 0.0765\t validation accuracy: 0.9733\n",
      "iteration number: 2816\t training loss: 0.0579\tvalidation loss: 0.0758\t validation accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2817\t training loss: 0.0582\tvalidation loss: 0.0761\t validation accuracy: 0.9756\n",
      "iteration number: 2818\t training loss: 0.0596\tvalidation loss: 0.0769\t validation accuracy: 0.9733\n",
      "iteration number: 2819\t training loss: 0.0583\tvalidation loss: 0.0759\t validation accuracy: 0.9733\n",
      "iteration number: 2820\t training loss: 0.0592\tvalidation loss: 0.0766\t validation accuracy: 0.9711\n",
      "iteration number: 2821\t training loss: 0.0585\tvalidation loss: 0.0762\t validation accuracy: 0.9733\n",
      "iteration number: 2822\t training loss: 0.0585\tvalidation loss: 0.0758\t validation accuracy: 0.9756\n",
      "iteration number: 2823\t training loss: 0.0572\tvalidation loss: 0.0736\t validation accuracy: 0.9756\n",
      "iteration number: 2824\t training loss: 0.0571\tvalidation loss: 0.0736\t validation accuracy: 0.9800\n",
      "iteration number: 2825\t training loss: 0.0573\tvalidation loss: 0.0748\t validation accuracy: 0.9800\n",
      "iteration number: 2826\t training loss: 0.0581\tvalidation loss: 0.0756\t validation accuracy: 0.9778\n",
      "iteration number: 2827\t training loss: 0.0584\tvalidation loss: 0.0747\t validation accuracy: 0.9800\n",
      "iteration number: 2828\t training loss: 0.0598\tvalidation loss: 0.0755\t validation accuracy: 0.9756\n",
      "iteration number: 2829\t training loss: 0.0579\tvalidation loss: 0.0736\t validation accuracy: 0.9800\n",
      "iteration number: 2830\t training loss: 0.0570\tvalidation loss: 0.0728\t validation accuracy: 0.9800\n",
      "iteration number: 2831\t training loss: 0.0570\tvalidation loss: 0.0733\t validation accuracy: 0.9800\n",
      "iteration number: 2832\t training loss: 0.0565\tvalidation loss: 0.0733\t validation accuracy: 0.9800\n",
      "iteration number: 2833\t training loss: 0.0568\tvalidation loss: 0.0731\t validation accuracy: 0.9778\n",
      "iteration number: 2834\t training loss: 0.0576\tvalidation loss: 0.0748\t validation accuracy: 0.9778\n",
      "iteration number: 2835\t training loss: 0.0566\tvalidation loss: 0.0736\t validation accuracy: 0.9778\n",
      "iteration number: 2836\t training loss: 0.0565\tvalidation loss: 0.0739\t validation accuracy: 0.9778\n",
      "iteration number: 2837\t training loss: 0.0564\tvalidation loss: 0.0738\t validation accuracy: 0.9778\n",
      "iteration number: 2838\t training loss: 0.0568\tvalidation loss: 0.0735\t validation accuracy: 0.9778\n",
      "iteration number: 2839\t training loss: 0.0565\tvalidation loss: 0.0736\t validation accuracy: 0.9778\n",
      "iteration number: 2840\t training loss: 0.0569\tvalidation loss: 0.0744\t validation accuracy: 0.9778\n",
      "iteration number: 2841\t training loss: 0.0563\tvalidation loss: 0.0733\t validation accuracy: 0.9778\n",
      "iteration number: 2842\t training loss: 0.0572\tvalidation loss: 0.0745\t validation accuracy: 0.9733\n",
      "iteration number: 2843\t training loss: 0.0564\tvalidation loss: 0.0725\t validation accuracy: 0.9822\n",
      "iteration number: 2844\t training loss: 0.0562\tvalidation loss: 0.0721\t validation accuracy: 0.9800\n",
      "iteration number: 2845\t training loss: 0.0559\tvalidation loss: 0.0718\t validation accuracy: 0.9800\n",
      "iteration number: 2846\t training loss: 0.0563\tvalidation loss: 0.0722\t validation accuracy: 0.9800\n",
      "iteration number: 2847\t training loss: 0.0565\tvalidation loss: 0.0729\t validation accuracy: 0.9778\n",
      "iteration number: 2848\t training loss: 0.0574\tvalidation loss: 0.0734\t validation accuracy: 0.9800\n",
      "iteration number: 2849\t training loss: 0.0576\tvalidation loss: 0.0743\t validation accuracy: 0.9800\n",
      "iteration number: 2850\t training loss: 0.0568\tvalidation loss: 0.0736\t validation accuracy: 0.9800\n",
      "iteration number: 2851\t training loss: 0.0574\tvalidation loss: 0.0739\t validation accuracy: 0.9800\n",
      "iteration number: 2852\t training loss: 0.0574\tvalidation loss: 0.0748\t validation accuracy: 0.9778\n",
      "iteration number: 2853\t training loss: 0.0571\tvalidation loss: 0.0745\t validation accuracy: 0.9800\n",
      "iteration number: 2854\t training loss: 0.0587\tvalidation loss: 0.0752\t validation accuracy: 0.9778\n",
      "iteration number: 2855\t training loss: 0.0589\tvalidation loss: 0.0756\t validation accuracy: 0.9800\n",
      "iteration number: 2856\t training loss: 0.0583\tvalidation loss: 0.0747\t validation accuracy: 0.9778\n",
      "iteration number: 2857\t training loss: 0.0579\tvalidation loss: 0.0736\t validation accuracy: 0.9800\n",
      "iteration number: 2858\t training loss: 0.0574\tvalidation loss: 0.0732\t validation accuracy: 0.9800\n",
      "iteration number: 2859\t training loss: 0.0570\tvalidation loss: 0.0726\t validation accuracy: 0.9822\n",
      "iteration number: 2860\t training loss: 0.0623\tvalidation loss: 0.0759\t validation accuracy: 0.9778\n",
      "iteration number: 2861\t training loss: 0.0605\tvalidation loss: 0.0752\t validation accuracy: 0.9756\n",
      "iteration number: 2862\t training loss: 0.0598\tvalidation loss: 0.0751\t validation accuracy: 0.9778\n",
      "iteration number: 2863\t training loss: 0.0591\tvalidation loss: 0.0752\t validation accuracy: 0.9800\n",
      "iteration number: 2864\t training loss: 0.0568\tvalidation loss: 0.0721\t validation accuracy: 0.9800\n",
      "iteration number: 2865\t training loss: 0.0563\tvalidation loss: 0.0713\t validation accuracy: 0.9844\n",
      "iteration number: 2866\t training loss: 0.0560\tvalidation loss: 0.0712\t validation accuracy: 0.9844\n",
      "iteration number: 2867\t training loss: 0.0562\tvalidation loss: 0.0713\t validation accuracy: 0.9822\n",
      "iteration number: 2868\t training loss: 0.0559\tvalidation loss: 0.0716\t validation accuracy: 0.9822\n",
      "iteration number: 2869\t training loss: 0.0559\tvalidation loss: 0.0720\t validation accuracy: 0.9822\n",
      "iteration number: 2870\t training loss: 0.0563\tvalidation loss: 0.0725\t validation accuracy: 0.9800\n",
      "iteration number: 2871\t training loss: 0.0562\tvalidation loss: 0.0725\t validation accuracy: 0.9800\n",
      "iteration number: 2872\t training loss: 0.0556\tvalidation loss: 0.0723\t validation accuracy: 0.9800\n",
      "iteration number: 2873\t training loss: 0.0563\tvalidation loss: 0.0736\t validation accuracy: 0.9800\n",
      "iteration number: 2874\t training loss: 0.0576\tvalidation loss: 0.0743\t validation accuracy: 0.9778\n",
      "iteration number: 2875\t training loss: 0.0571\tvalidation loss: 0.0727\t validation accuracy: 0.9844\n",
      "iteration number: 2876\t training loss: 0.0565\tvalidation loss: 0.0732\t validation accuracy: 0.9822\n",
      "iteration number: 2877\t training loss: 0.0565\tvalidation loss: 0.0734\t validation accuracy: 0.9822\n",
      "iteration number: 2878\t training loss: 0.0577\tvalidation loss: 0.0738\t validation accuracy: 0.9800\n",
      "iteration number: 2879\t training loss: 0.0563\tvalidation loss: 0.0736\t validation accuracy: 0.9800\n",
      "iteration number: 2880\t training loss: 0.0567\tvalidation loss: 0.0741\t validation accuracy: 0.9800\n",
      "iteration number: 2881\t training loss: 0.0566\tvalidation loss: 0.0739\t validation accuracy: 0.9800\n",
      "iteration number: 2882\t training loss: 0.0564\tvalidation loss: 0.0733\t validation accuracy: 0.9800\n",
      "iteration number: 2883\t training loss: 0.0569\tvalidation loss: 0.0740\t validation accuracy: 0.9800\n",
      "iteration number: 2884\t training loss: 0.0570\tvalidation loss: 0.0751\t validation accuracy: 0.9778\n",
      "iteration number: 2885\t training loss: 0.0570\tvalidation loss: 0.0749\t validation accuracy: 0.9756\n",
      "iteration number: 2886\t training loss: 0.0573\tvalidation loss: 0.0755\t validation accuracy: 0.9756\n",
      "iteration number: 2887\t training loss: 0.0559\tvalidation loss: 0.0735\t validation accuracy: 0.9800\n",
      "iteration number: 2888\t training loss: 0.0560\tvalidation loss: 0.0729\t validation accuracy: 0.9822\n",
      "iteration number: 2889\t training loss: 0.0554\tvalidation loss: 0.0721\t validation accuracy: 0.9822\n",
      "iteration number: 2890\t training loss: 0.0562\tvalidation loss: 0.0727\t validation accuracy: 0.9822\n",
      "iteration number: 2891\t training loss: 0.0560\tvalidation loss: 0.0728\t validation accuracy: 0.9822\n",
      "iteration number: 2892\t training loss: 0.0565\tvalidation loss: 0.0734\t validation accuracy: 0.9822\n",
      "iteration number: 2893\t training loss: 0.0559\tvalidation loss: 0.0729\t validation accuracy: 0.9822\n",
      "iteration number: 2894\t training loss: 0.0554\tvalidation loss: 0.0725\t validation accuracy: 0.9822\n",
      "iteration number: 2895\t training loss: 0.0555\tvalidation loss: 0.0724\t validation accuracy: 0.9822\n",
      "iteration number: 2896\t training loss: 0.0556\tvalidation loss: 0.0722\t validation accuracy: 0.9800\n",
      "iteration number: 2897\t training loss: 0.0587\tvalidation loss: 0.0745\t validation accuracy: 0.9778\n",
      "iteration number: 2898\t training loss: 0.0634\tvalidation loss: 0.0785\t validation accuracy: 0.9778\n",
      "iteration number: 2899\t training loss: 0.0558\tvalidation loss: 0.0740\t validation accuracy: 0.9756\n",
      "iteration number: 2900\t training loss: 0.0553\tvalidation loss: 0.0726\t validation accuracy: 0.9756\n",
      "iteration number: 2901\t training loss: 0.0552\tvalidation loss: 0.0723\t validation accuracy: 0.9778\n",
      "iteration number: 2902\t training loss: 0.0554\tvalidation loss: 0.0714\t validation accuracy: 0.9778\n",
      "iteration number: 2903\t training loss: 0.0550\tvalidation loss: 0.0712\t validation accuracy: 0.9778\n",
      "iteration number: 2904\t training loss: 0.0551\tvalidation loss: 0.0715\t validation accuracy: 0.9778\n",
      "iteration number: 2905\t training loss: 0.0552\tvalidation loss: 0.0717\t validation accuracy: 0.9778\n",
      "iteration number: 2906\t training loss: 0.0562\tvalidation loss: 0.0738\t validation accuracy: 0.9778\n",
      "iteration number: 2907\t training loss: 0.0561\tvalidation loss: 0.0729\t validation accuracy: 0.9778\n",
      "iteration number: 2908\t training loss: 0.0553\tvalidation loss: 0.0712\t validation accuracy: 0.9822\n",
      "iteration number: 2909\t training loss: 0.0555\tvalidation loss: 0.0715\t validation accuracy: 0.9822\n",
      "iteration number: 2910\t training loss: 0.0551\tvalidation loss: 0.0712\t validation accuracy: 0.9822\n",
      "iteration number: 2911\t training loss: 0.0550\tvalidation loss: 0.0710\t validation accuracy: 0.9822\n",
      "iteration number: 2912\t training loss: 0.0551\tvalidation loss: 0.0710\t validation accuracy: 0.9822\n",
      "iteration number: 2913\t training loss: 0.0552\tvalidation loss: 0.0706\t validation accuracy: 0.9822\n",
      "iteration number: 2914\t training loss: 0.0553\tvalidation loss: 0.0721\t validation accuracy: 0.9800\n",
      "iteration number: 2915\t training loss: 0.0554\tvalidation loss: 0.0724\t validation accuracy: 0.9800\n",
      "iteration number: 2916\t training loss: 0.0550\tvalidation loss: 0.0713\t validation accuracy: 0.9800\n",
      "iteration number: 2917\t training loss: 0.0551\tvalidation loss: 0.0709\t validation accuracy: 0.9822\n",
      "iteration number: 2918\t training loss: 0.0553\tvalidation loss: 0.0706\t validation accuracy: 0.9822\n",
      "iteration number: 2919\t training loss: 0.0564\tvalidation loss: 0.0705\t validation accuracy: 0.9800\n",
      "iteration number: 2920\t training loss: 0.0556\tvalidation loss: 0.0698\t validation accuracy: 0.9822\n",
      "iteration number: 2921\t training loss: 0.0553\tvalidation loss: 0.0698\t validation accuracy: 0.9844\n",
      "iteration number: 2922\t training loss: 0.0562\tvalidation loss: 0.0700\t validation accuracy: 0.9844\n",
      "iteration number: 2923\t training loss: 0.0562\tvalidation loss: 0.0710\t validation accuracy: 0.9822\n",
      "iteration number: 2924\t training loss: 0.0559\tvalidation loss: 0.0707\t validation accuracy: 0.9844\n",
      "iteration number: 2925\t training loss: 0.0558\tvalidation loss: 0.0713\t validation accuracy: 0.9822\n",
      "iteration number: 2926\t training loss: 0.0558\tvalidation loss: 0.0709\t validation accuracy: 0.9822\n",
      "iteration number: 2927\t training loss: 0.0552\tvalidation loss: 0.0705\t validation accuracy: 0.9822\n",
      "iteration number: 2928\t training loss: 0.0562\tvalidation loss: 0.0715\t validation accuracy: 0.9822\n",
      "iteration number: 2929\t training loss: 0.0558\tvalidation loss: 0.0720\t validation accuracy: 0.9800\n",
      "iteration number: 2930\t training loss: 0.0554\tvalidation loss: 0.0711\t validation accuracy: 0.9822\n",
      "iteration number: 2931\t training loss: 0.0563\tvalidation loss: 0.0715\t validation accuracy: 0.9800\n",
      "iteration number: 2932\t training loss: 0.0559\tvalidation loss: 0.0722\t validation accuracy: 0.9844\n",
      "iteration number: 2933\t training loss: 0.0549\tvalidation loss: 0.0722\t validation accuracy: 0.9800\n",
      "iteration number: 2934\t training loss: 0.0548\tvalidation loss: 0.0722\t validation accuracy: 0.9800\n",
      "iteration number: 2935\t training loss: 0.0549\tvalidation loss: 0.0717\t validation accuracy: 0.9822\n",
      "iteration number: 2936\t training loss: 0.0559\tvalidation loss: 0.0736\t validation accuracy: 0.9756\n",
      "iteration number: 2937\t training loss: 0.0549\tvalidation loss: 0.0729\t validation accuracy: 0.9778\n",
      "iteration number: 2938\t training loss: 0.0547\tvalidation loss: 0.0733\t validation accuracy: 0.9778\n",
      "iteration number: 2939\t training loss: 0.0547\tvalidation loss: 0.0734\t validation accuracy: 0.9756\n",
      "iteration number: 2940\t training loss: 0.0547\tvalidation loss: 0.0733\t validation accuracy: 0.9778\n",
      "iteration number: 2941\t training loss: 0.0551\tvalidation loss: 0.0718\t validation accuracy: 0.9778\n",
      "iteration number: 2942\t training loss: 0.0555\tvalidation loss: 0.0721\t validation accuracy: 0.9778\n",
      "iteration number: 2943\t training loss: 0.0542\tvalidation loss: 0.0717\t validation accuracy: 0.9778\n",
      "iteration number: 2944\t training loss: 0.0543\tvalidation loss: 0.0724\t validation accuracy: 0.9778\n",
      "iteration number: 2945\t training loss: 0.0541\tvalidation loss: 0.0719\t validation accuracy: 0.9778\n",
      "iteration number: 2946\t training loss: 0.0541\tvalidation loss: 0.0719\t validation accuracy: 0.9778\n",
      "iteration number: 2947\t training loss: 0.0545\tvalidation loss: 0.0726\t validation accuracy: 0.9756\n",
      "iteration number: 2948\t training loss: 0.0543\tvalidation loss: 0.0725\t validation accuracy: 0.9756\n",
      "iteration number: 2949\t training loss: 0.0545\tvalidation loss: 0.0724\t validation accuracy: 0.9756\n",
      "iteration number: 2950\t training loss: 0.0541\tvalidation loss: 0.0717\t validation accuracy: 0.9778\n",
      "iteration number: 2951\t training loss: 0.0539\tvalidation loss: 0.0711\t validation accuracy: 0.9800\n",
      "iteration number: 2952\t training loss: 0.0551\tvalidation loss: 0.0717\t validation accuracy: 0.9800\n",
      "iteration number: 2953\t training loss: 0.0543\tvalidation loss: 0.0710\t validation accuracy: 0.9822\n",
      "iteration number: 2954\t training loss: 0.0541\tvalidation loss: 0.0712\t validation accuracy: 0.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2955\t training loss: 0.0544\tvalidation loss: 0.0715\t validation accuracy: 0.9800\n",
      "iteration number: 2956\t training loss: 0.0548\tvalidation loss: 0.0721\t validation accuracy: 0.9778\n",
      "iteration number: 2957\t training loss: 0.0560\tvalidation loss: 0.0723\t validation accuracy: 0.9822\n",
      "iteration number: 2958\t training loss: 0.0565\tvalidation loss: 0.0723\t validation accuracy: 0.9822\n",
      "iteration number: 2959\t training loss: 0.0541\tvalidation loss: 0.0713\t validation accuracy: 0.9800\n",
      "iteration number: 2960\t training loss: 0.0540\tvalidation loss: 0.0711\t validation accuracy: 0.9800\n",
      "iteration number: 2961\t training loss: 0.0542\tvalidation loss: 0.0710\t validation accuracy: 0.9822\n",
      "iteration number: 2962\t training loss: 0.0535\tvalidation loss: 0.0718\t validation accuracy: 0.9800\n",
      "iteration number: 2963\t training loss: 0.0537\tvalidation loss: 0.0726\t validation accuracy: 0.9778\n",
      "iteration number: 2964\t training loss: 0.0540\tvalidation loss: 0.0735\t validation accuracy: 0.9800\n",
      "iteration number: 2965\t training loss: 0.0535\tvalidation loss: 0.0727\t validation accuracy: 0.9778\n",
      "iteration number: 2966\t training loss: 0.0540\tvalidation loss: 0.0734\t validation accuracy: 0.9778\n",
      "iteration number: 2967\t training loss: 0.0544\tvalidation loss: 0.0730\t validation accuracy: 0.9778\n",
      "iteration number: 2968\t training loss: 0.0580\tvalidation loss: 0.0758\t validation accuracy: 0.9756\n",
      "iteration number: 2969\t training loss: 0.0579\tvalidation loss: 0.0747\t validation accuracy: 0.9778\n",
      "iteration number: 2970\t training loss: 0.0542\tvalidation loss: 0.0724\t validation accuracy: 0.9800\n",
      "iteration number: 2971\t training loss: 0.0554\tvalidation loss: 0.0738\t validation accuracy: 0.9778\n",
      "iteration number: 2972\t training loss: 0.0559\tvalidation loss: 0.0740\t validation accuracy: 0.9778\n",
      "iteration number: 2973\t training loss: 0.0543\tvalidation loss: 0.0720\t validation accuracy: 0.9800\n",
      "iteration number: 2974\t training loss: 0.0544\tvalidation loss: 0.0728\t validation accuracy: 0.9756\n",
      "iteration number: 2975\t training loss: 0.0545\tvalidation loss: 0.0733\t validation accuracy: 0.9778\n",
      "iteration number: 2976\t training loss: 0.0542\tvalidation loss: 0.0731\t validation accuracy: 0.9800\n",
      "iteration number: 2977\t training loss: 0.0539\tvalidation loss: 0.0734\t validation accuracy: 0.9778\n",
      "iteration number: 2978\t training loss: 0.0542\tvalidation loss: 0.0736\t validation accuracy: 0.9778\n",
      "iteration number: 2979\t training loss: 0.0547\tvalidation loss: 0.0735\t validation accuracy: 0.9778\n",
      "iteration number: 2980\t training loss: 0.0546\tvalidation loss: 0.0742\t validation accuracy: 0.9778\n",
      "iteration number: 2981\t training loss: 0.0539\tvalidation loss: 0.0732\t validation accuracy: 0.9800\n",
      "iteration number: 2982\t training loss: 0.0547\tvalidation loss: 0.0738\t validation accuracy: 0.9778\n",
      "iteration number: 2983\t training loss: 0.0551\tvalidation loss: 0.0734\t validation accuracy: 0.9778\n",
      "iteration number: 2984\t training loss: 0.0536\tvalidation loss: 0.0736\t validation accuracy: 0.9778\n",
      "iteration number: 2985\t training loss: 0.0532\tvalidation loss: 0.0730\t validation accuracy: 0.9778\n",
      "iteration number: 2986\t training loss: 0.0535\tvalidation loss: 0.0731\t validation accuracy: 0.9778\n",
      "iteration number: 2987\t training loss: 0.0538\tvalidation loss: 0.0740\t validation accuracy: 0.9756\n",
      "iteration number: 2988\t training loss: 0.0543\tvalidation loss: 0.0746\t validation accuracy: 0.9756\n",
      "iteration number: 2989\t training loss: 0.0537\tvalidation loss: 0.0742\t validation accuracy: 0.9756\n",
      "iteration number: 2990\t training loss: 0.0542\tvalidation loss: 0.0746\t validation accuracy: 0.9778\n",
      "iteration number: 2991\t training loss: 0.0549\tvalidation loss: 0.0748\t validation accuracy: 0.9778\n",
      "iteration number: 2992\t training loss: 0.0539\tvalidation loss: 0.0747\t validation accuracy: 0.9733\n",
      "iteration number: 2993\t training loss: 0.0545\tvalidation loss: 0.0755\t validation accuracy: 0.9733\n",
      "iteration number: 2994\t training loss: 0.0548\tvalidation loss: 0.0756\t validation accuracy: 0.9711\n",
      "iteration number: 2995\t training loss: 0.0539\tvalidation loss: 0.0738\t validation accuracy: 0.9756\n",
      "iteration number: 2996\t training loss: 0.0534\tvalidation loss: 0.0725\t validation accuracy: 0.9756\n",
      "iteration number: 2997\t training loss: 0.0534\tvalidation loss: 0.0725\t validation accuracy: 0.9778\n",
      "iteration number: 2998\t training loss: 0.0535\tvalidation loss: 0.0715\t validation accuracy: 0.9778\n",
      "iteration number: 2999\t training loss: 0.0528\tvalidation loss: 0.0711\t validation accuracy: 0.9778\n",
      "iteration number: 3000\t training loss: 0.0528\tvalidation loss: 0.0712\t validation accuracy: 0.9800\n",
      "iteration number: 3001\t training loss: 0.0528\tvalidation loss: 0.0711\t validation accuracy: 0.9800\n",
      "iteration number: 3002\t training loss: 0.0534\tvalidation loss: 0.0708\t validation accuracy: 0.9778\n",
      "iteration number: 3003\t training loss: 0.0526\tvalidation loss: 0.0710\t validation accuracy: 0.9800\n",
      "iteration number: 3004\t training loss: 0.0531\tvalidation loss: 0.0711\t validation accuracy: 0.9778\n",
      "iteration number: 3005\t training loss: 0.0530\tvalidation loss: 0.0709\t validation accuracy: 0.9778\n",
      "iteration number: 3006\t training loss: 0.0531\tvalidation loss: 0.0707\t validation accuracy: 0.9800\n",
      "iteration number: 3007\t training loss: 0.0532\tvalidation loss: 0.0713\t validation accuracy: 0.9822\n",
      "iteration number: 3008\t training loss: 0.0532\tvalidation loss: 0.0711\t validation accuracy: 0.9800\n",
      "iteration number: 3009\t training loss: 0.0535\tvalidation loss: 0.0712\t validation accuracy: 0.9822\n",
      "iteration number: 3010\t training loss: 0.0540\tvalidation loss: 0.0715\t validation accuracy: 0.9822\n",
      "iteration number: 3011\t training loss: 0.0536\tvalidation loss: 0.0712\t validation accuracy: 0.9822\n",
      "iteration number: 3012\t training loss: 0.0534\tvalidation loss: 0.0726\t validation accuracy: 0.9756\n",
      "iteration number: 3013\t training loss: 0.0530\tvalidation loss: 0.0718\t validation accuracy: 0.9800\n",
      "iteration number: 3014\t training loss: 0.0535\tvalidation loss: 0.0717\t validation accuracy: 0.9800\n",
      "iteration number: 3015\t training loss: 0.0533\tvalidation loss: 0.0713\t validation accuracy: 0.9822\n",
      "iteration number: 3016\t training loss: 0.0533\tvalidation loss: 0.0722\t validation accuracy: 0.9778\n",
      "iteration number: 3017\t training loss: 0.0531\tvalidation loss: 0.0714\t validation accuracy: 0.9800\n",
      "iteration number: 3018\t training loss: 0.0535\tvalidation loss: 0.0714\t validation accuracy: 0.9822\n",
      "iteration number: 3019\t training loss: 0.0533\tvalidation loss: 0.0713\t validation accuracy: 0.9822\n",
      "iteration number: 3020\t training loss: 0.0530\tvalidation loss: 0.0719\t validation accuracy: 0.9778\n",
      "iteration number: 3021\t training loss: 0.0536\tvalidation loss: 0.0723\t validation accuracy: 0.9800\n",
      "iteration number: 3022\t training loss: 0.0537\tvalidation loss: 0.0721\t validation accuracy: 0.9778\n",
      "iteration number: 3023\t training loss: 0.0536\tvalidation loss: 0.0719\t validation accuracy: 0.9778\n",
      "iteration number: 3024\t training loss: 0.0530\tvalidation loss: 0.0713\t validation accuracy: 0.9822\n",
      "iteration number: 3025\t training loss: 0.0543\tvalidation loss: 0.0719\t validation accuracy: 0.9778\n",
      "iteration number: 3026\t training loss: 0.0532\tvalidation loss: 0.0717\t validation accuracy: 0.9822\n",
      "iteration number: 3027\t training loss: 0.0530\tvalidation loss: 0.0714\t validation accuracy: 0.9844\n",
      "iteration number: 3028\t training loss: 0.0538\tvalidation loss: 0.0718\t validation accuracy: 0.9822\n",
      "iteration number: 3029\t training loss: 0.0541\tvalidation loss: 0.0722\t validation accuracy: 0.9756\n",
      "iteration number: 3030\t training loss: 0.0536\tvalidation loss: 0.0718\t validation accuracy: 0.9778\n",
      "iteration number: 3031\t training loss: 0.0534\tvalidation loss: 0.0711\t validation accuracy: 0.9800\n",
      "iteration number: 3032\t training loss: 0.0534\tvalidation loss: 0.0709\t validation accuracy: 0.9822\n",
      "iteration number: 3033\t training loss: 0.0538\tvalidation loss: 0.0710\t validation accuracy: 0.9800\n",
      "iteration number: 3034\t training loss: 0.0532\tvalidation loss: 0.0707\t validation accuracy: 0.9822\n",
      "iteration number: 3035\t training loss: 0.0535\tvalidation loss: 0.0710\t validation accuracy: 0.9822\n",
      "iteration number: 3036\t training loss: 0.0539\tvalidation loss: 0.0712\t validation accuracy: 0.9778\n",
      "iteration number: 3037\t training loss: 0.0535\tvalidation loss: 0.0714\t validation accuracy: 0.9778\n",
      "iteration number: 3038\t training loss: 0.0533\tvalidation loss: 0.0714\t validation accuracy: 0.9800\n",
      "iteration number: 3039\t training loss: 0.0530\tvalidation loss: 0.0710\t validation accuracy: 0.9800\n",
      "iteration number: 3040\t training loss: 0.0533\tvalidation loss: 0.0708\t validation accuracy: 0.9800\n",
      "iteration number: 3041\t training loss: 0.0537\tvalidation loss: 0.0712\t validation accuracy: 0.9800\n",
      "iteration number: 3042\t training loss: 0.0550\tvalidation loss: 0.0722\t validation accuracy: 0.9800\n",
      "iteration number: 3043\t training loss: 0.0565\tvalidation loss: 0.0737\t validation accuracy: 0.9778\n",
      "iteration number: 3044\t training loss: 0.0558\tvalidation loss: 0.0726\t validation accuracy: 0.9778\n",
      "iteration number: 3045\t training loss: 0.0547\tvalidation loss: 0.0714\t validation accuracy: 0.9778\n",
      "iteration number: 3046\t training loss: 0.0557\tvalidation loss: 0.0714\t validation accuracy: 0.9778\n",
      "iteration number: 3047\t training loss: 0.0545\tvalidation loss: 0.0708\t validation accuracy: 0.9800\n",
      "iteration number: 3048\t training loss: 0.0545\tvalidation loss: 0.0707\t validation accuracy: 0.9800\n",
      "iteration number: 3049\t training loss: 0.0536\tvalidation loss: 0.0700\t validation accuracy: 0.9800\n",
      "iteration number: 3050\t training loss: 0.0529\tvalidation loss: 0.0697\t validation accuracy: 0.9822\n",
      "iteration number: 3051\t training loss: 0.0527\tvalidation loss: 0.0693\t validation accuracy: 0.9822\n",
      "iteration number: 3052\t training loss: 0.0519\tvalidation loss: 0.0690\t validation accuracy: 0.9822\n",
      "iteration number: 3053\t training loss: 0.0522\tvalidation loss: 0.0692\t validation accuracy: 0.9800\n",
      "iteration number: 3054\t training loss: 0.0526\tvalidation loss: 0.0691\t validation accuracy: 0.9800\n",
      "iteration number: 3055\t training loss: 0.0524\tvalidation loss: 0.0685\t validation accuracy: 0.9844\n",
      "iteration number: 3056\t training loss: 0.0521\tvalidation loss: 0.0689\t validation accuracy: 0.9844\n",
      "iteration number: 3057\t training loss: 0.0519\tvalidation loss: 0.0687\t validation accuracy: 0.9844\n",
      "iteration number: 3058\t training loss: 0.0521\tvalidation loss: 0.0688\t validation accuracy: 0.9844\n",
      "iteration number: 3059\t training loss: 0.0522\tvalidation loss: 0.0685\t validation accuracy: 0.9844\n",
      "iteration number: 3060\t training loss: 0.0520\tvalidation loss: 0.0685\t validation accuracy: 0.9844\n",
      "iteration number: 3061\t training loss: 0.0523\tvalidation loss: 0.0687\t validation accuracy: 0.9844\n",
      "iteration number: 3062\t training loss: 0.0524\tvalidation loss: 0.0697\t validation accuracy: 0.9844\n",
      "iteration number: 3063\t training loss: 0.0534\tvalidation loss: 0.0702\t validation accuracy: 0.9844\n",
      "iteration number: 3064\t training loss: 0.0531\tvalidation loss: 0.0700\t validation accuracy: 0.9800\n",
      "iteration number: 3065\t training loss: 0.0525\tvalidation loss: 0.0698\t validation accuracy: 0.9844\n",
      "iteration number: 3066\t training loss: 0.0520\tvalidation loss: 0.0684\t validation accuracy: 0.9844\n",
      "iteration number: 3067\t training loss: 0.0520\tvalidation loss: 0.0684\t validation accuracy: 0.9844\n",
      "iteration number: 3068\t training loss: 0.0522\tvalidation loss: 0.0685\t validation accuracy: 0.9822\n",
      "iteration number: 3069\t training loss: 0.0519\tvalidation loss: 0.0688\t validation accuracy: 0.9822\n",
      "iteration number: 3070\t training loss: 0.0517\tvalidation loss: 0.0695\t validation accuracy: 0.9800\n",
      "iteration number: 3071\t training loss: 0.0515\tvalidation loss: 0.0693\t validation accuracy: 0.9800\n",
      "iteration number: 3072\t training loss: 0.0515\tvalidation loss: 0.0692\t validation accuracy: 0.9800\n",
      "iteration number: 3073\t training loss: 0.0516\tvalidation loss: 0.0698\t validation accuracy: 0.9778\n",
      "iteration number: 3074\t training loss: 0.0514\tvalidation loss: 0.0690\t validation accuracy: 0.9800\n",
      "iteration number: 3075\t training loss: 0.0515\tvalidation loss: 0.0699\t validation accuracy: 0.9800\n",
      "iteration number: 3076\t training loss: 0.0520\tvalidation loss: 0.0703\t validation accuracy: 0.9778\n",
      "iteration number: 3077\t training loss: 0.0516\tvalidation loss: 0.0696\t validation accuracy: 0.9800\n",
      "iteration number: 3078\t training loss: 0.0517\tvalidation loss: 0.0701\t validation accuracy: 0.9778\n",
      "iteration number: 3079\t training loss: 0.0515\tvalidation loss: 0.0699\t validation accuracy: 0.9800\n",
      "iteration number: 3080\t training loss: 0.0516\tvalidation loss: 0.0697\t validation accuracy: 0.9800\n",
      "iteration number: 3081\t training loss: 0.0519\tvalidation loss: 0.0697\t validation accuracy: 0.9822\n",
      "iteration number: 3082\t training loss: 0.0522\tvalidation loss: 0.0714\t validation accuracy: 0.9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3083\t training loss: 0.0536\tvalidation loss: 0.0720\t validation accuracy: 0.9800\n",
      "iteration number: 3084\t training loss: 0.0542\tvalidation loss: 0.0732\t validation accuracy: 0.9756\n",
      "iteration number: 3085\t training loss: 0.0519\tvalidation loss: 0.0711\t validation accuracy: 0.9800\n",
      "iteration number: 3086\t training loss: 0.0523\tvalidation loss: 0.0714\t validation accuracy: 0.9800\n",
      "iteration number: 3087\t training loss: 0.0523\tvalidation loss: 0.0720\t validation accuracy: 0.9778\n",
      "iteration number: 3088\t training loss: 0.0523\tvalidation loss: 0.0717\t validation accuracy: 0.9800\n",
      "iteration number: 3089\t training loss: 0.0536\tvalidation loss: 0.0738\t validation accuracy: 0.9756\n",
      "iteration number: 3090\t training loss: 0.0524\tvalidation loss: 0.0733\t validation accuracy: 0.9733\n",
      "iteration number: 3091\t training loss: 0.0516\tvalidation loss: 0.0721\t validation accuracy: 0.9778\n",
      "iteration number: 3092\t training loss: 0.0522\tvalidation loss: 0.0728\t validation accuracy: 0.9756\n",
      "iteration number: 3093\t training loss: 0.0517\tvalidation loss: 0.0720\t validation accuracy: 0.9778\n",
      "iteration number: 3094\t training loss: 0.0515\tvalidation loss: 0.0717\t validation accuracy: 0.9756\n",
      "iteration number: 3095\t training loss: 0.0515\tvalidation loss: 0.0718\t validation accuracy: 0.9778\n",
      "iteration number: 3096\t training loss: 0.0529\tvalidation loss: 0.0725\t validation accuracy: 0.9756\n",
      "iteration number: 3097\t training loss: 0.0526\tvalidation loss: 0.0728\t validation accuracy: 0.9756\n",
      "iteration number: 3098\t training loss: 0.0528\tvalidation loss: 0.0729\t validation accuracy: 0.9756\n",
      "iteration number: 3099\t training loss: 0.0525\tvalidation loss: 0.0729\t validation accuracy: 0.9733\n",
      "iteration number: 3100\t training loss: 0.0518\tvalidation loss: 0.0720\t validation accuracy: 0.9733\n",
      "iteration number: 3101\t training loss: 0.0518\tvalidation loss: 0.0720\t validation accuracy: 0.9733\n",
      "iteration number: 3102\t training loss: 0.0517\tvalidation loss: 0.0713\t validation accuracy: 0.9756\n",
      "iteration number: 3103\t training loss: 0.0523\tvalidation loss: 0.0717\t validation accuracy: 0.9756\n",
      "iteration number: 3104\t training loss: 0.0524\tvalidation loss: 0.0720\t validation accuracy: 0.9756\n",
      "iteration number: 3105\t training loss: 0.0519\tvalidation loss: 0.0714\t validation accuracy: 0.9756\n",
      "iteration number: 3106\t training loss: 0.0509\tvalidation loss: 0.0698\t validation accuracy: 0.9822\n",
      "iteration number: 3107\t training loss: 0.0510\tvalidation loss: 0.0700\t validation accuracy: 0.9822\n",
      "iteration number: 3108\t training loss: 0.0512\tvalidation loss: 0.0708\t validation accuracy: 0.9778\n",
      "iteration number: 3109\t training loss: 0.0514\tvalidation loss: 0.0712\t validation accuracy: 0.9800\n",
      "iteration number: 3110\t training loss: 0.0512\tvalidation loss: 0.0704\t validation accuracy: 0.9800\n",
      "iteration number: 3111\t training loss: 0.0514\tvalidation loss: 0.0707\t validation accuracy: 0.9822\n",
      "iteration number: 3112\t training loss: 0.0519\tvalidation loss: 0.0713\t validation accuracy: 0.9822\n",
      "iteration number: 3113\t training loss: 0.0517\tvalidation loss: 0.0705\t validation accuracy: 0.9822\n",
      "iteration number: 3114\t training loss: 0.0517\tvalidation loss: 0.0706\t validation accuracy: 0.9800\n",
      "iteration number: 3115\t training loss: 0.0513\tvalidation loss: 0.0705\t validation accuracy: 0.9800\n",
      "iteration number: 3116\t training loss: 0.0512\tvalidation loss: 0.0701\t validation accuracy: 0.9800\n",
      "iteration number: 3117\t training loss: 0.0514\tvalidation loss: 0.0698\t validation accuracy: 0.9800\n",
      "iteration number: 3118\t training loss: 0.0514\tvalidation loss: 0.0695\t validation accuracy: 0.9800\n",
      "iteration number: 3119\t training loss: 0.0513\tvalidation loss: 0.0690\t validation accuracy: 0.9800\n",
      "iteration number: 3120\t training loss: 0.0511\tvalidation loss: 0.0687\t validation accuracy: 0.9822\n",
      "iteration number: 3121\t training loss: 0.0515\tvalidation loss: 0.0684\t validation accuracy: 0.9822\n",
      "iteration number: 3122\t training loss: 0.0518\tvalidation loss: 0.0687\t validation accuracy: 0.9822\n",
      "iteration number: 3123\t training loss: 0.0514\tvalidation loss: 0.0694\t validation accuracy: 0.9800\n",
      "iteration number: 3124\t training loss: 0.0523\tvalidation loss: 0.0704\t validation accuracy: 0.9778\n",
      "iteration number: 3125\t training loss: 0.0521\tvalidation loss: 0.0703\t validation accuracy: 0.9778\n",
      "iteration number: 3126\t training loss: 0.0521\tvalidation loss: 0.0697\t validation accuracy: 0.9778\n",
      "iteration number: 3127\t training loss: 0.0511\tvalidation loss: 0.0688\t validation accuracy: 0.9800\n",
      "iteration number: 3128\t training loss: 0.0540\tvalidation loss: 0.0711\t validation accuracy: 0.9800\n",
      "iteration number: 3129\t training loss: 0.0528\tvalidation loss: 0.0706\t validation accuracy: 0.9800\n",
      "iteration number: 3130\t training loss: 0.0520\tvalidation loss: 0.0696\t validation accuracy: 0.9800\n",
      "iteration number: 3131\t training loss: 0.0529\tvalidation loss: 0.0707\t validation accuracy: 0.9778\n",
      "iteration number: 3132\t training loss: 0.0525\tvalidation loss: 0.0706\t validation accuracy: 0.9778\n",
      "iteration number: 3133\t training loss: 0.0519\tvalidation loss: 0.0696\t validation accuracy: 0.9800\n",
      "iteration number: 3134\t training loss: 0.0522\tvalidation loss: 0.0701\t validation accuracy: 0.9778\n",
      "iteration number: 3135\t training loss: 0.0520\tvalidation loss: 0.0698\t validation accuracy: 0.9778\n",
      "iteration number: 3136\t training loss: 0.0516\tvalidation loss: 0.0693\t validation accuracy: 0.9822\n",
      "iteration number: 3137\t training loss: 0.0508\tvalidation loss: 0.0694\t validation accuracy: 0.9778\n",
      "iteration number: 3138\t training loss: 0.0510\tvalidation loss: 0.0700\t validation accuracy: 0.9756\n",
      "iteration number: 3139\t training loss: 0.0513\tvalidation loss: 0.0702\t validation accuracy: 0.9756\n",
      "iteration number: 3140\t training loss: 0.0519\tvalidation loss: 0.0703\t validation accuracy: 0.9778\n",
      "iteration number: 3141\t training loss: 0.0513\tvalidation loss: 0.0700\t validation accuracy: 0.9778\n",
      "iteration number: 3142\t training loss: 0.0518\tvalidation loss: 0.0698\t validation accuracy: 0.9800\n",
      "iteration number: 3143\t training loss: 0.0513\tvalidation loss: 0.0704\t validation accuracy: 0.9778\n",
      "iteration number: 3144\t training loss: 0.0503\tvalidation loss: 0.0691\t validation accuracy: 0.9822\n",
      "iteration number: 3145\t training loss: 0.0507\tvalidation loss: 0.0694\t validation accuracy: 0.9800\n",
      "iteration number: 3146\t training loss: 0.0508\tvalidation loss: 0.0694\t validation accuracy: 0.9778\n",
      "iteration number: 3147\t training loss: 0.0507\tvalidation loss: 0.0691\t validation accuracy: 0.9800\n",
      "iteration number: 3148\t training loss: 0.0507\tvalidation loss: 0.0692\t validation accuracy: 0.9822\n",
      "iteration number: 3149\t training loss: 0.0509\tvalidation loss: 0.0690\t validation accuracy: 0.9800\n",
      "iteration number: 3150\t training loss: 0.0512\tvalidation loss: 0.0688\t validation accuracy: 0.9822\n",
      "iteration number: 3151\t training loss: 0.0514\tvalidation loss: 0.0688\t validation accuracy: 0.9844\n",
      "iteration number: 3152\t training loss: 0.0511\tvalidation loss: 0.0684\t validation accuracy: 0.9844\n",
      "iteration number: 3153\t training loss: 0.0512\tvalidation loss: 0.0686\t validation accuracy: 0.9822\n",
      "iteration number: 3154\t training loss: 0.0520\tvalidation loss: 0.0690\t validation accuracy: 0.9822\n",
      "iteration number: 3155\t training loss: 0.0520\tvalidation loss: 0.0689\t validation accuracy: 0.9822\n",
      "iteration number: 3156\t training loss: 0.0513\tvalidation loss: 0.0687\t validation accuracy: 0.9800\n",
      "iteration number: 3157\t training loss: 0.0511\tvalidation loss: 0.0684\t validation accuracy: 0.9822\n",
      "iteration number: 3158\t training loss: 0.0525\tvalidation loss: 0.0694\t validation accuracy: 0.9800\n",
      "iteration number: 3159\t training loss: 0.0521\tvalidation loss: 0.0693\t validation accuracy: 0.9800\n",
      "iteration number: 3160\t training loss: 0.0523\tvalidation loss: 0.0693\t validation accuracy: 0.9822\n",
      "iteration number: 3161\t training loss: 0.0520\tvalidation loss: 0.0690\t validation accuracy: 0.9800\n",
      "iteration number: 3162\t training loss: 0.0506\tvalidation loss: 0.0683\t validation accuracy: 0.9822\n",
      "iteration number: 3163\t training loss: 0.0517\tvalidation loss: 0.0687\t validation accuracy: 0.9800\n",
      "iteration number: 3164\t training loss: 0.0508\tvalidation loss: 0.0679\t validation accuracy: 0.9822\n",
      "iteration number: 3165\t training loss: 0.0509\tvalidation loss: 0.0678\t validation accuracy: 0.9822\n",
      "iteration number: 3166\t training loss: 0.0505\tvalidation loss: 0.0678\t validation accuracy: 0.9822\n",
      "iteration number: 3167\t training loss: 0.0506\tvalidation loss: 0.0685\t validation accuracy: 0.9844\n",
      "iteration number: 3168\t training loss: 0.0510\tvalidation loss: 0.0690\t validation accuracy: 0.9844\n",
      "iteration number: 3169\t training loss: 0.0502\tvalidation loss: 0.0680\t validation accuracy: 0.9844\n",
      "iteration number: 3170\t training loss: 0.0504\tvalidation loss: 0.0676\t validation accuracy: 0.9844\n",
      "iteration number: 3171\t training loss: 0.0500\tvalidation loss: 0.0676\t validation accuracy: 0.9822\n",
      "iteration number: 3172\t training loss: 0.0510\tvalidation loss: 0.0682\t validation accuracy: 0.9800\n",
      "iteration number: 3173\t training loss: 0.0505\tvalidation loss: 0.0681\t validation accuracy: 0.9822\n",
      "iteration number: 3174\t training loss: 0.0503\tvalidation loss: 0.0680\t validation accuracy: 0.9822\n",
      "iteration number: 3175\t training loss: 0.0501\tvalidation loss: 0.0680\t validation accuracy: 0.9822\n",
      "iteration number: 3176\t training loss: 0.0506\tvalidation loss: 0.0692\t validation accuracy: 0.9822\n",
      "iteration number: 3177\t training loss: 0.0498\tvalidation loss: 0.0686\t validation accuracy: 0.9822\n",
      "iteration number: 3178\t training loss: 0.0523\tvalidation loss: 0.0705\t validation accuracy: 0.9800\n",
      "iteration number: 3179\t training loss: 0.0497\tvalidation loss: 0.0685\t validation accuracy: 0.9800\n",
      "iteration number: 3180\t training loss: 0.0498\tvalidation loss: 0.0686\t validation accuracy: 0.9822\n",
      "iteration number: 3181\t training loss: 0.0506\tvalidation loss: 0.0697\t validation accuracy: 0.9800\n",
      "iteration number: 3182\t training loss: 0.0505\tvalidation loss: 0.0694\t validation accuracy: 0.9800\n",
      "iteration number: 3183\t training loss: 0.0503\tvalidation loss: 0.0694\t validation accuracy: 0.9800\n",
      "iteration number: 3184\t training loss: 0.0502\tvalidation loss: 0.0693\t validation accuracy: 0.9800\n",
      "iteration number: 3185\t training loss: 0.0499\tvalidation loss: 0.0679\t validation accuracy: 0.9800\n",
      "iteration number: 3186\t training loss: 0.0501\tvalidation loss: 0.0682\t validation accuracy: 0.9800\n",
      "iteration number: 3187\t training loss: 0.0502\tvalidation loss: 0.0685\t validation accuracy: 0.9800\n",
      "iteration number: 3188\t training loss: 0.0501\tvalidation loss: 0.0691\t validation accuracy: 0.9800\n",
      "iteration number: 3189\t training loss: 0.0507\tvalidation loss: 0.0696\t validation accuracy: 0.9800\n",
      "iteration number: 3190\t training loss: 0.0502\tvalidation loss: 0.0691\t validation accuracy: 0.9800\n",
      "iteration number: 3191\t training loss: 0.0505\tvalidation loss: 0.0702\t validation accuracy: 0.9800\n",
      "iteration number: 3192\t training loss: 0.0496\tvalidation loss: 0.0689\t validation accuracy: 0.9800\n",
      "iteration number: 3193\t training loss: 0.0499\tvalidation loss: 0.0693\t validation accuracy: 0.9800\n",
      "iteration number: 3194\t training loss: 0.0501\tvalidation loss: 0.0700\t validation accuracy: 0.9800\n",
      "iteration number: 3195\t training loss: 0.0506\tvalidation loss: 0.0699\t validation accuracy: 0.9800\n",
      "iteration number: 3196\t training loss: 0.0502\tvalidation loss: 0.0696\t validation accuracy: 0.9800\n",
      "iteration number: 3197\t training loss: 0.0501\tvalidation loss: 0.0694\t validation accuracy: 0.9778\n",
      "iteration number: 3198\t training loss: 0.0503\tvalidation loss: 0.0691\t validation accuracy: 0.9822\n",
      "iteration number: 3199\t training loss: 0.0496\tvalidation loss: 0.0684\t validation accuracy: 0.9822\n",
      "iteration number: 3200\t training loss: 0.0504\tvalidation loss: 0.0691\t validation accuracy: 0.9822\n",
      "iteration number: 3201\t training loss: 0.0509\tvalidation loss: 0.0703\t validation accuracy: 0.9822\n",
      "iteration number: 3202\t training loss: 0.0502\tvalidation loss: 0.0696\t validation accuracy: 0.9800\n",
      "iteration number: 3203\t training loss: 0.0503\tvalidation loss: 0.0693\t validation accuracy: 0.9822\n",
      "iteration number: 3204\t training loss: 0.0502\tvalidation loss: 0.0692\t validation accuracy: 0.9822\n",
      "iteration number: 3205\t training loss: 0.0509\tvalidation loss: 0.0703\t validation accuracy: 0.9822\n",
      "iteration number: 3206\t training loss: 0.0508\tvalidation loss: 0.0707\t validation accuracy: 0.9822\n",
      "iteration number: 3207\t training loss: 0.0505\tvalidation loss: 0.0694\t validation accuracy: 0.9822\n",
      "iteration number: 3208\t training loss: 0.0496\tvalidation loss: 0.0688\t validation accuracy: 0.9800\n",
      "iteration number: 3209\t training loss: 0.0492\tvalidation loss: 0.0685\t validation accuracy: 0.9800\n",
      "iteration number: 3210\t training loss: 0.0494\tvalidation loss: 0.0682\t validation accuracy: 0.9822\n",
      "iteration number: 3211\t training loss: 0.0503\tvalidation loss: 0.0693\t validation accuracy: 0.9822\n",
      "iteration number: 3212\t training loss: 0.0508\tvalidation loss: 0.0703\t validation accuracy: 0.9800\n",
      "iteration number: 3213\t training loss: 0.0513\tvalidation loss: 0.0705\t validation accuracy: 0.9822\n",
      "iteration number: 3214\t training loss: 0.0502\tvalidation loss: 0.0704\t validation accuracy: 0.9844\n",
      "iteration number: 3215\t training loss: 0.0498\tvalidation loss: 0.0697\t validation accuracy: 0.9844\n",
      "iteration number: 3216\t training loss: 0.0503\tvalidation loss: 0.0704\t validation accuracy: 0.9800\n",
      "iteration number: 3217\t training loss: 0.0509\tvalidation loss: 0.0705\t validation accuracy: 0.9822\n",
      "iteration number: 3218\t training loss: 0.0513\tvalidation loss: 0.0705\t validation accuracy: 0.9800\n",
      "iteration number: 3219\t training loss: 0.0489\tvalidation loss: 0.0690\t validation accuracy: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3220\t training loss: 0.0491\tvalidation loss: 0.0690\t validation accuracy: 0.9778\n",
      "iteration number: 3221\t training loss: 0.0489\tvalidation loss: 0.0688\t validation accuracy: 0.9800\n",
      "iteration number: 3222\t training loss: 0.0493\tvalidation loss: 0.0686\t validation accuracy: 0.9800\n",
      "iteration number: 3223\t training loss: 0.0491\tvalidation loss: 0.0688\t validation accuracy: 0.9800\n",
      "iteration number: 3224\t training loss: 0.0491\tvalidation loss: 0.0690\t validation accuracy: 0.9778\n",
      "iteration number: 3225\t training loss: 0.0493\tvalidation loss: 0.0694\t validation accuracy: 0.9778\n",
      "iteration number: 3226\t training loss: 0.0493\tvalidation loss: 0.0692\t validation accuracy: 0.9778\n",
      "iteration number: 3227\t training loss: 0.0492\tvalidation loss: 0.0692\t validation accuracy: 0.9778\n",
      "iteration number: 3228\t training loss: 0.0494\tvalidation loss: 0.0685\t validation accuracy: 0.9778\n",
      "iteration number: 3229\t training loss: 0.0494\tvalidation loss: 0.0683\t validation accuracy: 0.9778\n",
      "iteration number: 3230\t training loss: 0.0506\tvalidation loss: 0.0687\t validation accuracy: 0.9800\n",
      "iteration number: 3231\t training loss: 0.0500\tvalidation loss: 0.0685\t validation accuracy: 0.9778\n",
      "iteration number: 3232\t training loss: 0.0490\tvalidation loss: 0.0682\t validation accuracy: 0.9800\n",
      "iteration number: 3233\t training loss: 0.0492\tvalidation loss: 0.0682\t validation accuracy: 0.9800\n",
      "iteration number: 3234\t training loss: 0.0493\tvalidation loss: 0.0685\t validation accuracy: 0.9844\n",
      "iteration number: 3235\t training loss: 0.0488\tvalidation loss: 0.0690\t validation accuracy: 0.9800\n",
      "iteration number: 3236\t training loss: 0.0492\tvalidation loss: 0.0694\t validation accuracy: 0.9800\n",
      "iteration number: 3237\t training loss: 0.0490\tvalidation loss: 0.0689\t validation accuracy: 0.9800\n",
      "iteration number: 3238\t training loss: 0.0494\tvalidation loss: 0.0690\t validation accuracy: 0.9822\n",
      "iteration number: 3239\t training loss: 0.0492\tvalidation loss: 0.0689\t validation accuracy: 0.9822\n",
      "iteration number: 3240\t training loss: 0.0488\tvalidation loss: 0.0687\t validation accuracy: 0.9800\n",
      "iteration number: 3241\t training loss: 0.0498\tvalidation loss: 0.0697\t validation accuracy: 0.9822\n",
      "iteration number: 3242\t training loss: 0.0489\tvalidation loss: 0.0697\t validation accuracy: 0.9822\n",
      "iteration number: 3243\t training loss: 0.0492\tvalidation loss: 0.0695\t validation accuracy: 0.9822\n",
      "iteration number: 3244\t training loss: 0.0493\tvalidation loss: 0.0693\t validation accuracy: 0.9822\n",
      "iteration number: 3245\t training loss: 0.0491\tvalidation loss: 0.0686\t validation accuracy: 0.9822\n",
      "iteration number: 3246\t training loss: 0.0486\tvalidation loss: 0.0689\t validation accuracy: 0.9800\n",
      "iteration number: 3247\t training loss: 0.0486\tvalidation loss: 0.0683\t validation accuracy: 0.9822\n",
      "iteration number: 3248\t training loss: 0.0484\tvalidation loss: 0.0688\t validation accuracy: 0.9822\n",
      "iteration number: 3249\t training loss: 0.0487\tvalidation loss: 0.0684\t validation accuracy: 0.9822\n",
      "iteration number: 3250\t training loss: 0.0489\tvalidation loss: 0.0681\t validation accuracy: 0.9822\n",
      "iteration number: 3251\t training loss: 0.0485\tvalidation loss: 0.0686\t validation accuracy: 0.9822\n",
      "iteration number: 3252\t training loss: 0.0483\tvalidation loss: 0.0688\t validation accuracy: 0.9822\n",
      "iteration number: 3253\t training loss: 0.0485\tvalidation loss: 0.0691\t validation accuracy: 0.9800\n",
      "iteration number: 3254\t training loss: 0.0488\tvalidation loss: 0.0691\t validation accuracy: 0.9800\n",
      "iteration number: 3255\t training loss: 0.0490\tvalidation loss: 0.0687\t validation accuracy: 0.9800\n",
      "iteration number: 3256\t training loss: 0.0488\tvalidation loss: 0.0692\t validation accuracy: 0.9800\n",
      "iteration number: 3257\t training loss: 0.0486\tvalidation loss: 0.0683\t validation accuracy: 0.9800\n",
      "iteration number: 3258\t training loss: 0.0486\tvalidation loss: 0.0675\t validation accuracy: 0.9844\n",
      "iteration number: 3259\t training loss: 0.0484\tvalidation loss: 0.0674\t validation accuracy: 0.9800\n",
      "iteration number: 3260\t training loss: 0.0483\tvalidation loss: 0.0672\t validation accuracy: 0.9800\n",
      "iteration number: 3261\t training loss: 0.0483\tvalidation loss: 0.0678\t validation accuracy: 0.9800\n",
      "iteration number: 3262\t training loss: 0.0484\tvalidation loss: 0.0681\t validation accuracy: 0.9800\n",
      "iteration number: 3263\t training loss: 0.0483\tvalidation loss: 0.0678\t validation accuracy: 0.9800\n",
      "iteration number: 3264\t training loss: 0.0483\tvalidation loss: 0.0680\t validation accuracy: 0.9800\n",
      "iteration number: 3265\t training loss: 0.0483\tvalidation loss: 0.0674\t validation accuracy: 0.9822\n",
      "iteration number: 3266\t training loss: 0.0484\tvalidation loss: 0.0685\t validation accuracy: 0.9778\n",
      "iteration number: 3267\t training loss: 0.0484\tvalidation loss: 0.0676\t validation accuracy: 0.9800\n",
      "iteration number: 3268\t training loss: 0.0485\tvalidation loss: 0.0675\t validation accuracy: 0.9800\n",
      "iteration number: 3269\t training loss: 0.0484\tvalidation loss: 0.0675\t validation accuracy: 0.9822\n",
      "iteration number: 3270\t training loss: 0.0482\tvalidation loss: 0.0675\t validation accuracy: 0.9822\n",
      "iteration number: 3271\t training loss: 0.0504\tvalidation loss: 0.0691\t validation accuracy: 0.9778\n",
      "iteration number: 3272\t training loss: 0.0504\tvalidation loss: 0.0691\t validation accuracy: 0.9778\n",
      "iteration number: 3273\t training loss: 0.0513\tvalidation loss: 0.0704\t validation accuracy: 0.9778\n",
      "iteration number: 3274\t training loss: 0.0487\tvalidation loss: 0.0691\t validation accuracy: 0.9822\n",
      "iteration number: 3275\t training loss: 0.0484\tvalidation loss: 0.0689\t validation accuracy: 0.9822\n",
      "iteration number: 3276\t training loss: 0.0485\tvalidation loss: 0.0690\t validation accuracy: 0.9800\n",
      "iteration number: 3277\t training loss: 0.0486\tvalidation loss: 0.0692\t validation accuracy: 0.9800\n",
      "iteration number: 3278\t training loss: 0.0494\tvalidation loss: 0.0705\t validation accuracy: 0.9800\n",
      "iteration number: 3279\t training loss: 0.0491\tvalidation loss: 0.0696\t validation accuracy: 0.9800\n",
      "iteration number: 3280\t training loss: 0.0488\tvalidation loss: 0.0690\t validation accuracy: 0.9822\n",
      "iteration number: 3281\t training loss: 0.0480\tvalidation loss: 0.0693\t validation accuracy: 0.9800\n",
      "iteration number: 3282\t training loss: 0.0482\tvalidation loss: 0.0682\t validation accuracy: 0.9822\n",
      "iteration number: 3283\t training loss: 0.0488\tvalidation loss: 0.0691\t validation accuracy: 0.9822\n",
      "iteration number: 3284\t training loss: 0.0486\tvalidation loss: 0.0689\t validation accuracy: 0.9822\n",
      "iteration number: 3285\t training loss: 0.0483\tvalidation loss: 0.0684\t validation accuracy: 0.9822\n",
      "iteration number: 3286\t training loss: 0.0482\tvalidation loss: 0.0689\t validation accuracy: 0.9822\n",
      "iteration number: 3287\t training loss: 0.0477\tvalidation loss: 0.0682\t validation accuracy: 0.9822\n",
      "iteration number: 3288\t training loss: 0.0479\tvalidation loss: 0.0685\t validation accuracy: 0.9822\n",
      "iteration number: 3289\t training loss: 0.0476\tvalidation loss: 0.0681\t validation accuracy: 0.9822\n",
      "iteration number: 3290\t training loss: 0.0480\tvalidation loss: 0.0682\t validation accuracy: 0.9822\n",
      "iteration number: 3291\t training loss: 0.0489\tvalidation loss: 0.0695\t validation accuracy: 0.9800\n",
      "iteration number: 3292\t training loss: 0.0493\tvalidation loss: 0.0691\t validation accuracy: 0.9822\n",
      "iteration number: 3293\t training loss: 0.0490\tvalidation loss: 0.0688\t validation accuracy: 0.9800\n",
      "iteration number: 3294\t training loss: 0.0492\tvalidation loss: 0.0688\t validation accuracy: 0.9800\n",
      "iteration number: 3295\t training loss: 0.0483\tvalidation loss: 0.0682\t validation accuracy: 0.9822\n",
      "iteration number: 3296\t training loss: 0.0481\tvalidation loss: 0.0686\t validation accuracy: 0.9822\n",
      "iteration number: 3297\t training loss: 0.0482\tvalidation loss: 0.0683\t validation accuracy: 0.9822\n",
      "iteration number: 3298\t training loss: 0.0485\tvalidation loss: 0.0685\t validation accuracy: 0.9800\n",
      "iteration number: 3299\t training loss: 0.0479\tvalidation loss: 0.0689\t validation accuracy: 0.9822\n",
      "iteration number: 3300\t training loss: 0.0491\tvalidation loss: 0.0711\t validation accuracy: 0.9756\n",
      "iteration number: 3301\t training loss: 0.0500\tvalidation loss: 0.0707\t validation accuracy: 0.9756\n",
      "iteration number: 3302\t training loss: 0.0494\tvalidation loss: 0.0701\t validation accuracy: 0.9800\n",
      "iteration number: 3303\t training loss: 0.0487\tvalidation loss: 0.0701\t validation accuracy: 0.9800\n",
      "iteration number: 3304\t training loss: 0.0491\tvalidation loss: 0.0709\t validation accuracy: 0.9778\n",
      "iteration number: 3305\t training loss: 0.0489\tvalidation loss: 0.0712\t validation accuracy: 0.9778\n",
      "iteration number: 3306\t training loss: 0.0484\tvalidation loss: 0.0710\t validation accuracy: 0.9756\n",
      "iteration number: 3307\t training loss: 0.0484\tvalidation loss: 0.0708\t validation accuracy: 0.9756\n",
      "iteration number: 3308\t training loss: 0.0495\tvalidation loss: 0.0703\t validation accuracy: 0.9778\n",
      "iteration number: 3309\t training loss: 0.0486\tvalidation loss: 0.0694\t validation accuracy: 0.9778\n",
      "iteration number: 3310\t training loss: 0.0497\tvalidation loss: 0.0703\t validation accuracy: 0.9778\n",
      "iteration number: 3311\t training loss: 0.0480\tvalidation loss: 0.0689\t validation accuracy: 0.9800\n",
      "iteration number: 3312\t training loss: 0.0481\tvalidation loss: 0.0688\t validation accuracy: 0.9800\n",
      "iteration number: 3313\t training loss: 0.0474\tvalidation loss: 0.0682\t validation accuracy: 0.9800\n",
      "iteration number: 3314\t training loss: 0.0477\tvalidation loss: 0.0682\t validation accuracy: 0.9778\n",
      "iteration number: 3315\t training loss: 0.0484\tvalidation loss: 0.0692\t validation accuracy: 0.9800\n",
      "iteration number: 3316\t training loss: 0.0491\tvalidation loss: 0.0696\t validation accuracy: 0.9800\n",
      "iteration number: 3317\t training loss: 0.0483\tvalidation loss: 0.0685\t validation accuracy: 0.9800\n",
      "iteration number: 3318\t training loss: 0.0481\tvalidation loss: 0.0678\t validation accuracy: 0.9822\n",
      "iteration number: 3319\t training loss: 0.0480\tvalidation loss: 0.0682\t validation accuracy: 0.9822\n",
      "iteration number: 3320\t training loss: 0.0501\tvalidation loss: 0.0693\t validation accuracy: 0.9800\n",
      "iteration number: 3321\t training loss: 0.0498\tvalidation loss: 0.0691\t validation accuracy: 0.9800\n",
      "iteration number: 3322\t training loss: 0.0504\tvalidation loss: 0.0710\t validation accuracy: 0.9778\n",
      "iteration number: 3323\t training loss: 0.0502\tvalidation loss: 0.0707\t validation accuracy: 0.9800\n",
      "iteration number: 3324\t training loss: 0.0497\tvalidation loss: 0.0700\t validation accuracy: 0.9800\n",
      "iteration number: 3325\t training loss: 0.0486\tvalidation loss: 0.0682\t validation accuracy: 0.9800\n",
      "iteration number: 3326\t training loss: 0.0491\tvalidation loss: 0.0682\t validation accuracy: 0.9822\n",
      "iteration number: 3327\t training loss: 0.0486\tvalidation loss: 0.0681\t validation accuracy: 0.9822\n",
      "iteration number: 3328\t training loss: 0.0482\tvalidation loss: 0.0677\t validation accuracy: 0.9800\n",
      "iteration number: 3329\t training loss: 0.0481\tvalidation loss: 0.0675\t validation accuracy: 0.9800\n",
      "iteration number: 3330\t training loss: 0.0474\tvalidation loss: 0.0671\t validation accuracy: 0.9822\n",
      "iteration number: 3331\t training loss: 0.0498\tvalidation loss: 0.0689\t validation accuracy: 0.9822\n",
      "iteration number: 3332\t training loss: 0.0489\tvalidation loss: 0.0685\t validation accuracy: 0.9822\n",
      "iteration number: 3333\t training loss: 0.0484\tvalidation loss: 0.0673\t validation accuracy: 0.9822\n",
      "iteration number: 3334\t training loss: 0.0470\tvalidation loss: 0.0679\t validation accuracy: 0.9800\n",
      "iteration number: 3335\t training loss: 0.0468\tvalidation loss: 0.0675\t validation accuracy: 0.9800\n",
      "iteration number: 3336\t training loss: 0.0469\tvalidation loss: 0.0675\t validation accuracy: 0.9800\n",
      "iteration number: 3337\t training loss: 0.0471\tvalidation loss: 0.0675\t validation accuracy: 0.9822\n",
      "iteration number: 3338\t training loss: 0.0474\tvalidation loss: 0.0683\t validation accuracy: 0.9822\n",
      "iteration number: 3339\t training loss: 0.0474\tvalidation loss: 0.0678\t validation accuracy: 0.9822\n",
      "iteration number: 3340\t training loss: 0.0482\tvalidation loss: 0.0695\t validation accuracy: 0.9822\n",
      "iteration number: 3341\t training loss: 0.0473\tvalidation loss: 0.0686\t validation accuracy: 0.9822\n",
      "iteration number: 3342\t training loss: 0.0472\tvalidation loss: 0.0679\t validation accuracy: 0.9822\n",
      "iteration number: 3343\t training loss: 0.0473\tvalidation loss: 0.0680\t validation accuracy: 0.9822\n",
      "iteration number: 3344\t training loss: 0.0473\tvalidation loss: 0.0681\t validation accuracy: 0.9800\n",
      "iteration number: 3345\t training loss: 0.0471\tvalidation loss: 0.0678\t validation accuracy: 0.9822\n",
      "iteration number: 3346\t training loss: 0.0475\tvalidation loss: 0.0692\t validation accuracy: 0.9822\n",
      "iteration number: 3347\t training loss: 0.0475\tvalidation loss: 0.0690\t validation accuracy: 0.9800\n",
      "iteration number: 3348\t training loss: 0.0479\tvalidation loss: 0.0697\t validation accuracy: 0.9822\n",
      "iteration number: 3349\t training loss: 0.0483\tvalidation loss: 0.0702\t validation accuracy: 0.9800\n",
      "iteration number: 3350\t training loss: 0.0498\tvalidation loss: 0.0714\t validation accuracy: 0.9800\n",
      "iteration number: 3351\t training loss: 0.0481\tvalidation loss: 0.0690\t validation accuracy: 0.9800\n",
      "iteration number: 3352\t training loss: 0.0488\tvalidation loss: 0.0692\t validation accuracy: 0.9800\n",
      "iteration number: 3353\t training loss: 0.0484\tvalidation loss: 0.0689\t validation accuracy: 0.9844\n",
      "iteration number: 3354\t training loss: 0.0484\tvalidation loss: 0.0689\t validation accuracy: 0.9822\n",
      "iteration number: 3355\t training loss: 0.0483\tvalidation loss: 0.0690\t validation accuracy: 0.9800\n",
      "iteration number: 3356\t training loss: 0.0484\tvalidation loss: 0.0689\t validation accuracy: 0.9800\n",
      "iteration number: 3357\t training loss: 0.0477\tvalidation loss: 0.0686\t validation accuracy: 0.9800\n",
      "iteration number: 3358\t training loss: 0.0472\tvalidation loss: 0.0680\t validation accuracy: 0.9800\n",
      "iteration number: 3359\t training loss: 0.0472\tvalidation loss: 0.0669\t validation accuracy: 0.9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3360\t training loss: 0.0469\tvalidation loss: 0.0669\t validation accuracy: 0.9822\n",
      "iteration number: 3361\t training loss: 0.0475\tvalidation loss: 0.0669\t validation accuracy: 0.9822\n",
      "iteration number: 3362\t training loss: 0.0475\tvalidation loss: 0.0668\t validation accuracy: 0.9822\n",
      "iteration number: 3363\t training loss: 0.0470\tvalidation loss: 0.0670\t validation accuracy: 0.9822\n",
      "iteration number: 3364\t training loss: 0.0472\tvalidation loss: 0.0675\t validation accuracy: 0.9822\n",
      "iteration number: 3365\t training loss: 0.0471\tvalidation loss: 0.0675\t validation accuracy: 0.9844\n",
      "iteration number: 3366\t training loss: 0.0476\tvalidation loss: 0.0678\t validation accuracy: 0.9844\n",
      "iteration number: 3367\t training loss: 0.0473\tvalidation loss: 0.0673\t validation accuracy: 0.9800\n",
      "iteration number: 3368\t training loss: 0.0466\tvalidation loss: 0.0682\t validation accuracy: 0.9800\n",
      "iteration number: 3369\t training loss: 0.0468\tvalidation loss: 0.0686\t validation accuracy: 0.9800\n",
      "iteration number: 3370\t training loss: 0.0467\tvalidation loss: 0.0683\t validation accuracy: 0.9822\n",
      "iteration number: 3371\t training loss: 0.0473\tvalidation loss: 0.0686\t validation accuracy: 0.9778\n",
      "iteration number: 3372\t training loss: 0.0465\tvalidation loss: 0.0682\t validation accuracy: 0.9822\n",
      "iteration number: 3373\t training loss: 0.0470\tvalidation loss: 0.0680\t validation accuracy: 0.9822\n",
      "iteration number: 3374\t training loss: 0.0470\tvalidation loss: 0.0680\t validation accuracy: 0.9778\n",
      "iteration number: 3375\t training loss: 0.0477\tvalidation loss: 0.0698\t validation accuracy: 0.9756\n",
      "iteration number: 3376\t training loss: 0.0468\tvalidation loss: 0.0690\t validation accuracy: 0.9778\n",
      "iteration number: 3377\t training loss: 0.0473\tvalidation loss: 0.0696\t validation accuracy: 0.9778\n",
      "iteration number: 3378\t training loss: 0.0466\tvalidation loss: 0.0686\t validation accuracy: 0.9800\n",
      "iteration number: 3379\t training loss: 0.0477\tvalidation loss: 0.0688\t validation accuracy: 0.9756\n",
      "iteration number: 3380\t training loss: 0.0471\tvalidation loss: 0.0680\t validation accuracy: 0.9756\n",
      "iteration number: 3381\t training loss: 0.0462\tvalidation loss: 0.0673\t validation accuracy: 0.9822\n",
      "iteration number: 3382\t training loss: 0.0468\tvalidation loss: 0.0672\t validation accuracy: 0.9800\n",
      "iteration number: 3383\t training loss: 0.0468\tvalidation loss: 0.0676\t validation accuracy: 0.9822\n",
      "iteration number: 3384\t training loss: 0.0469\tvalidation loss: 0.0684\t validation accuracy: 0.9800\n",
      "iteration number: 3385\t training loss: 0.0470\tvalidation loss: 0.0682\t validation accuracy: 0.9778\n",
      "iteration number: 3386\t training loss: 0.0467\tvalidation loss: 0.0675\t validation accuracy: 0.9778\n",
      "iteration number: 3387\t training loss: 0.0467\tvalidation loss: 0.0674\t validation accuracy: 0.9822\n",
      "iteration number: 3388\t training loss: 0.0465\tvalidation loss: 0.0670\t validation accuracy: 0.9822\n",
      "iteration number: 3389\t training loss: 0.0464\tvalidation loss: 0.0664\t validation accuracy: 0.9822\n",
      "iteration number: 3390\t training loss: 0.0469\tvalidation loss: 0.0683\t validation accuracy: 0.9800\n",
      "iteration number: 3391\t training loss: 0.0467\tvalidation loss: 0.0688\t validation accuracy: 0.9778\n",
      "iteration number: 3392\t training loss: 0.0464\tvalidation loss: 0.0686\t validation accuracy: 0.9800\n",
      "iteration number: 3393\t training loss: 0.0466\tvalidation loss: 0.0683\t validation accuracy: 0.9800\n",
      "iteration number: 3394\t training loss: 0.0461\tvalidation loss: 0.0682\t validation accuracy: 0.9778\n",
      "iteration number: 3395\t training loss: 0.0462\tvalidation loss: 0.0686\t validation accuracy: 0.9778\n",
      "iteration number: 3396\t training loss: 0.0464\tvalidation loss: 0.0691\t validation accuracy: 0.9778\n",
      "iteration number: 3397\t training loss: 0.0461\tvalidation loss: 0.0683\t validation accuracy: 0.9778\n",
      "iteration number: 3398\t training loss: 0.0461\tvalidation loss: 0.0681\t validation accuracy: 0.9800\n",
      "iteration number: 3399\t training loss: 0.0460\tvalidation loss: 0.0677\t validation accuracy: 0.9800\n",
      "iteration number: 3400\t training loss: 0.0460\tvalidation loss: 0.0671\t validation accuracy: 0.9800\n",
      "iteration number: 3401\t training loss: 0.0465\tvalidation loss: 0.0693\t validation accuracy: 0.9778\n",
      "iteration number: 3402\t training loss: 0.0469\tvalidation loss: 0.0702\t validation accuracy: 0.9733\n",
      "iteration number: 3403\t training loss: 0.0478\tvalidation loss: 0.0705\t validation accuracy: 0.9756\n",
      "iteration number: 3404\t training loss: 0.0468\tvalidation loss: 0.0694\t validation accuracy: 0.9800\n",
      "iteration number: 3405\t training loss: 0.0475\tvalidation loss: 0.0702\t validation accuracy: 0.9800\n",
      "iteration number: 3406\t training loss: 0.0475\tvalidation loss: 0.0702\t validation accuracy: 0.9800\n",
      "iteration number: 3407\t training loss: 0.0477\tvalidation loss: 0.0697\t validation accuracy: 0.9800\n",
      "iteration number: 3408\t training loss: 0.0473\tvalidation loss: 0.0688\t validation accuracy: 0.9800\n",
      "iteration number: 3409\t training loss: 0.0477\tvalidation loss: 0.0689\t validation accuracy: 0.9800\n",
      "iteration number: 3410\t training loss: 0.0476\tvalidation loss: 0.0695\t validation accuracy: 0.9778\n",
      "iteration number: 3411\t training loss: 0.0469\tvalidation loss: 0.0686\t validation accuracy: 0.9800\n",
      "iteration number: 3412\t training loss: 0.0475\tvalidation loss: 0.0680\t validation accuracy: 0.9822\n",
      "iteration number: 3413\t training loss: 0.0466\tvalidation loss: 0.0675\t validation accuracy: 0.9800\n",
      "iteration number: 3414\t training loss: 0.0474\tvalidation loss: 0.0679\t validation accuracy: 0.9822\n",
      "iteration number: 3415\t training loss: 0.0473\tvalidation loss: 0.0687\t validation accuracy: 0.9800\n",
      "iteration number: 3416\t training loss: 0.0474\tvalidation loss: 0.0685\t validation accuracy: 0.9822\n",
      "iteration number: 3417\t training loss: 0.0474\tvalidation loss: 0.0686\t validation accuracy: 0.9822\n",
      "iteration number: 3418\t training loss: 0.0474\tvalidation loss: 0.0691\t validation accuracy: 0.9800\n",
      "iteration number: 3419\t training loss: 0.0469\tvalidation loss: 0.0693\t validation accuracy: 0.9800\n",
      "iteration number: 3420\t training loss: 0.0468\tvalidation loss: 0.0709\t validation accuracy: 0.9778\n",
      "iteration number: 3421\t training loss: 0.0464\tvalidation loss: 0.0703\t validation accuracy: 0.9800\n",
      "iteration number: 3422\t training loss: 0.0459\tvalidation loss: 0.0679\t validation accuracy: 0.9800\n",
      "iteration number: 3423\t training loss: 0.0456\tvalidation loss: 0.0679\t validation accuracy: 0.9800\n",
      "iteration number: 3424\t training loss: 0.0460\tvalidation loss: 0.0673\t validation accuracy: 0.9800\n",
      "iteration number: 3425\t training loss: 0.0457\tvalidation loss: 0.0673\t validation accuracy: 0.9800\n",
      "iteration number: 3426\t training loss: 0.0468\tvalidation loss: 0.0678\t validation accuracy: 0.9800\n",
      "iteration number: 3427\t training loss: 0.0467\tvalidation loss: 0.0678\t validation accuracy: 0.9822\n",
      "iteration number: 3428\t training loss: 0.0462\tvalidation loss: 0.0682\t validation accuracy: 0.9822\n",
      "iteration number: 3429\t training loss: 0.0457\tvalidation loss: 0.0688\t validation accuracy: 0.9822\n",
      "iteration number: 3430\t training loss: 0.0458\tvalidation loss: 0.0689\t validation accuracy: 0.9822\n",
      "iteration number: 3431\t training loss: 0.0455\tvalidation loss: 0.0680\t validation accuracy: 0.9822\n",
      "iteration number: 3432\t training loss: 0.0456\tvalidation loss: 0.0679\t validation accuracy: 0.9800\n",
      "iteration number: 3433\t training loss: 0.0459\tvalidation loss: 0.0681\t validation accuracy: 0.9822\n",
      "iteration number: 3434\t training loss: 0.0457\tvalidation loss: 0.0676\t validation accuracy: 0.9822\n",
      "iteration number: 3435\t training loss: 0.0458\tvalidation loss: 0.0680\t validation accuracy: 0.9822\n",
      "iteration number: 3436\t training loss: 0.0453\tvalidation loss: 0.0674\t validation accuracy: 0.9822\n",
      "iteration number: 3437\t training loss: 0.0453\tvalidation loss: 0.0676\t validation accuracy: 0.9822\n",
      "iteration number: 3438\t training loss: 0.0458\tvalidation loss: 0.0676\t validation accuracy: 0.9822\n",
      "iteration number: 3439\t training loss: 0.0453\tvalidation loss: 0.0669\t validation accuracy: 0.9800\n",
      "iteration number: 3440\t training loss: 0.0455\tvalidation loss: 0.0683\t validation accuracy: 0.9778\n",
      "iteration number: 3441\t training loss: 0.0456\tvalidation loss: 0.0685\t validation accuracy: 0.9778\n",
      "iteration number: 3442\t training loss: 0.0458\tvalidation loss: 0.0694\t validation accuracy: 0.9778\n",
      "iteration number: 3443\t training loss: 0.0462\tvalidation loss: 0.0698\t validation accuracy: 0.9756\n",
      "iteration number: 3444\t training loss: 0.0459\tvalidation loss: 0.0685\t validation accuracy: 0.9778\n",
      "iteration number: 3445\t training loss: 0.0467\tvalidation loss: 0.0687\t validation accuracy: 0.9778\n",
      "iteration number: 3446\t training loss: 0.0462\tvalidation loss: 0.0674\t validation accuracy: 0.9800\n",
      "iteration number: 3447\t training loss: 0.0455\tvalidation loss: 0.0674\t validation accuracy: 0.9800\n",
      "iteration number: 3448\t training loss: 0.0455\tvalidation loss: 0.0671\t validation accuracy: 0.9800\n",
      "iteration number: 3449\t training loss: 0.0460\tvalidation loss: 0.0683\t validation accuracy: 0.9800\n",
      "iteration number: 3450\t training loss: 0.0454\tvalidation loss: 0.0678\t validation accuracy: 0.9800\n",
      "iteration number: 3451\t training loss: 0.0452\tvalidation loss: 0.0668\t validation accuracy: 0.9800\n",
      "iteration number: 3452\t training loss: 0.0451\tvalidation loss: 0.0675\t validation accuracy: 0.9800\n",
      "iteration number: 3453\t training loss: 0.0460\tvalidation loss: 0.0677\t validation accuracy: 0.9800\n",
      "iteration number: 3454\t training loss: 0.0452\tvalidation loss: 0.0675\t validation accuracy: 0.9800\n",
      "iteration number: 3455\t training loss: 0.0450\tvalidation loss: 0.0673\t validation accuracy: 0.9800\n",
      "iteration number: 3456\t training loss: 0.0451\tvalidation loss: 0.0675\t validation accuracy: 0.9800\n",
      "iteration number: 3457\t training loss: 0.0453\tvalidation loss: 0.0682\t validation accuracy: 0.9800\n",
      "iteration number: 3458\t training loss: 0.0456\tvalidation loss: 0.0685\t validation accuracy: 0.9800\n",
      "iteration number: 3459\t training loss: 0.0465\tvalidation loss: 0.0691\t validation accuracy: 0.9800\n",
      "iteration number: 3460\t training loss: 0.0461\tvalidation loss: 0.0685\t validation accuracy: 0.9822\n",
      "iteration number: 3461\t training loss: 0.0455\tvalidation loss: 0.0682\t validation accuracy: 0.9800\n",
      "iteration number: 3462\t training loss: 0.0461\tvalidation loss: 0.0684\t validation accuracy: 0.9800\n",
      "iteration number: 3463\t training loss: 0.0454\tvalidation loss: 0.0682\t validation accuracy: 0.9800\n",
      "iteration number: 3464\t training loss: 0.0468\tvalidation loss: 0.0698\t validation accuracy: 0.9778\n",
      "iteration number: 3465\t training loss: 0.0467\tvalidation loss: 0.0702\t validation accuracy: 0.9778\n",
      "iteration number: 3466\t training loss: 0.0451\tvalidation loss: 0.0683\t validation accuracy: 0.9800\n",
      "iteration number: 3467\t training loss: 0.0464\tvalidation loss: 0.0688\t validation accuracy: 0.9800\n",
      "iteration number: 3468\t training loss: 0.0481\tvalidation loss: 0.0700\t validation accuracy: 0.9800\n",
      "iteration number: 3469\t training loss: 0.0462\tvalidation loss: 0.0698\t validation accuracy: 0.9822\n",
      "iteration number: 3470\t training loss: 0.0461\tvalidation loss: 0.0695\t validation accuracy: 0.9822\n",
      "iteration number: 3471\t training loss: 0.0458\tvalidation loss: 0.0699\t validation accuracy: 0.9822\n",
      "iteration number: 3472\t training loss: 0.0465\tvalidation loss: 0.0713\t validation accuracy: 0.9756\n",
      "iteration number: 3473\t training loss: 0.0465\tvalidation loss: 0.0714\t validation accuracy: 0.9756\n",
      "iteration number: 3474\t training loss: 0.0461\tvalidation loss: 0.0704\t validation accuracy: 0.9822\n",
      "iteration number: 3475\t training loss: 0.0461\tvalidation loss: 0.0700\t validation accuracy: 0.9822\n",
      "iteration number: 3476\t training loss: 0.0459\tvalidation loss: 0.0704\t validation accuracy: 0.9778\n",
      "iteration number: 3477\t training loss: 0.0459\tvalidation loss: 0.0702\t validation accuracy: 0.9800\n",
      "iteration number: 3478\t training loss: 0.0453\tvalidation loss: 0.0695\t validation accuracy: 0.9800\n",
      "iteration number: 3479\t training loss: 0.0459\tvalidation loss: 0.0700\t validation accuracy: 0.9778\n",
      "iteration number: 3480\t training loss: 0.0459\tvalidation loss: 0.0701\t validation accuracy: 0.9756\n",
      "iteration number: 3481\t training loss: 0.0469\tvalidation loss: 0.0722\t validation accuracy: 0.9756\n",
      "iteration number: 3482\t training loss: 0.0463\tvalidation loss: 0.0714\t validation accuracy: 0.9756\n",
      "iteration number: 3483\t training loss: 0.0460\tvalidation loss: 0.0707\t validation accuracy: 0.9756\n",
      "iteration number: 3484\t training loss: 0.0465\tvalidation loss: 0.0713\t validation accuracy: 0.9756\n",
      "iteration number: 3485\t training loss: 0.0461\tvalidation loss: 0.0716\t validation accuracy: 0.9756\n",
      "iteration number: 3486\t training loss: 0.0462\tvalidation loss: 0.0715\t validation accuracy: 0.9756\n",
      "iteration number: 3487\t training loss: 0.0461\tvalidation loss: 0.0708\t validation accuracy: 0.9778\n",
      "iteration number: 3488\t training loss: 0.0460\tvalidation loss: 0.0701\t validation accuracy: 0.9778\n",
      "iteration number: 3489\t training loss: 0.0461\tvalidation loss: 0.0702\t validation accuracy: 0.9800\n",
      "iteration number: 3490\t training loss: 0.0460\tvalidation loss: 0.0697\t validation accuracy: 0.9778\n",
      "iteration number: 3491\t training loss: 0.0473\tvalidation loss: 0.0727\t validation accuracy: 0.9756\n",
      "iteration number: 3492\t training loss: 0.0464\tvalidation loss: 0.0720\t validation accuracy: 0.9756\n",
      "iteration number: 3493\t training loss: 0.0469\tvalidation loss: 0.0729\t validation accuracy: 0.9711\n",
      "iteration number: 3494\t training loss: 0.0461\tvalidation loss: 0.0716\t validation accuracy: 0.9711\n",
      "iteration number: 3495\t training loss: 0.0466\tvalidation loss: 0.0721\t validation accuracy: 0.9733\n",
      "iteration number: 3496\t training loss: 0.0468\tvalidation loss: 0.0727\t validation accuracy: 0.9711\n",
      "iteration number: 3497\t training loss: 0.0464\tvalidation loss: 0.0717\t validation accuracy: 0.9711\n",
      "iteration number: 3498\t training loss: 0.0461\tvalidation loss: 0.0718\t validation accuracy: 0.9733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3499\t training loss: 0.0456\tvalidation loss: 0.0707\t validation accuracy: 0.9756\n",
      "iteration number: 3500\t training loss: 0.0456\tvalidation loss: 0.0710\t validation accuracy: 0.9756\n",
      "iteration number: 3501\t training loss: 0.0447\tvalidation loss: 0.0690\t validation accuracy: 0.9800\n",
      "iteration number: 3502\t training loss: 0.0457\tvalidation loss: 0.0686\t validation accuracy: 0.9778\n",
      "iteration number: 3503\t training loss: 0.0464\tvalidation loss: 0.0686\t validation accuracy: 0.9778\n",
      "iteration number: 3504\t training loss: 0.0450\tvalidation loss: 0.0683\t validation accuracy: 0.9800\n",
      "iteration number: 3505\t training loss: 0.0454\tvalidation loss: 0.0690\t validation accuracy: 0.9800\n",
      "iteration number: 3506\t training loss: 0.0454\tvalidation loss: 0.0679\t validation accuracy: 0.9800\n",
      "iteration number: 3507\t training loss: 0.0459\tvalidation loss: 0.0676\t validation accuracy: 0.9800\n",
      "iteration number: 3508\t training loss: 0.0452\tvalidation loss: 0.0677\t validation accuracy: 0.9800\n",
      "iteration number: 3509\t training loss: 0.0451\tvalidation loss: 0.0674\t validation accuracy: 0.9800\n",
      "iteration number: 3510\t training loss: 0.0453\tvalidation loss: 0.0674\t validation accuracy: 0.9800\n",
      "iteration number: 3511\t training loss: 0.0448\tvalidation loss: 0.0668\t validation accuracy: 0.9778\n",
      "iteration number: 3512\t training loss: 0.0447\tvalidation loss: 0.0666\t validation accuracy: 0.9800\n",
      "iteration number: 3513\t training loss: 0.0453\tvalidation loss: 0.0670\t validation accuracy: 0.9800\n",
      "iteration number: 3514\t training loss: 0.0452\tvalidation loss: 0.0671\t validation accuracy: 0.9800\n",
      "iteration number: 3515\t training loss: 0.0445\tvalidation loss: 0.0673\t validation accuracy: 0.9800\n",
      "iteration number: 3516\t training loss: 0.0453\tvalidation loss: 0.0675\t validation accuracy: 0.9778\n",
      "iteration number: 3517\t training loss: 0.0450\tvalidation loss: 0.0670\t validation accuracy: 0.9822\n",
      "iteration number: 3518\t training loss: 0.0449\tvalidation loss: 0.0671\t validation accuracy: 0.9822\n",
      "iteration number: 3519\t training loss: 0.0446\tvalidation loss: 0.0675\t validation accuracy: 0.9800\n",
      "iteration number: 3520\t training loss: 0.0453\tvalidation loss: 0.0693\t validation accuracy: 0.9778\n",
      "iteration number: 3521\t training loss: 0.0449\tvalidation loss: 0.0692\t validation accuracy: 0.9756\n",
      "iteration number: 3522\t training loss: 0.0448\tvalidation loss: 0.0692\t validation accuracy: 0.9756\n",
      "iteration number: 3523\t training loss: 0.0455\tvalidation loss: 0.0694\t validation accuracy: 0.9778\n",
      "iteration number: 3524\t training loss: 0.0451\tvalidation loss: 0.0691\t validation accuracy: 0.9756\n",
      "iteration number: 3525\t training loss: 0.0449\tvalidation loss: 0.0687\t validation accuracy: 0.9756\n",
      "iteration number: 3526\t training loss: 0.0450\tvalidation loss: 0.0681\t validation accuracy: 0.9778\n",
      "iteration number: 3527\t training loss: 0.0447\tvalidation loss: 0.0680\t validation accuracy: 0.9778\n",
      "iteration number: 3528\t training loss: 0.0448\tvalidation loss: 0.0682\t validation accuracy: 0.9778\n",
      "iteration number: 3529\t training loss: 0.0451\tvalidation loss: 0.0682\t validation accuracy: 0.9800\n",
      "iteration number: 3530\t training loss: 0.0450\tvalidation loss: 0.0688\t validation accuracy: 0.9800\n",
      "iteration number: 3531\t training loss: 0.0453\tvalidation loss: 0.0691\t validation accuracy: 0.9822\n",
      "iteration number: 3532\t training loss: 0.0454\tvalidation loss: 0.0690\t validation accuracy: 0.9822\n",
      "iteration number: 3533\t training loss: 0.0449\tvalidation loss: 0.0676\t validation accuracy: 0.9778\n",
      "iteration number: 3534\t training loss: 0.0446\tvalidation loss: 0.0678\t validation accuracy: 0.9778\n",
      "iteration number: 3535\t training loss: 0.0446\tvalidation loss: 0.0678\t validation accuracy: 0.9778\n",
      "iteration number: 3536\t training loss: 0.0452\tvalidation loss: 0.0678\t validation accuracy: 0.9822\n",
      "iteration number: 3537\t training loss: 0.0445\tvalidation loss: 0.0676\t validation accuracy: 0.9778\n",
      "iteration number: 3538\t training loss: 0.0445\tvalidation loss: 0.0680\t validation accuracy: 0.9756\n",
      "iteration number: 3539\t training loss: 0.0448\tvalidation loss: 0.0684\t validation accuracy: 0.9756\n",
      "iteration number: 3540\t training loss: 0.0443\tvalidation loss: 0.0687\t validation accuracy: 0.9756\n",
      "iteration number: 3541\t training loss: 0.0453\tvalidation loss: 0.0702\t validation accuracy: 0.9733\n",
      "iteration number: 3542\t training loss: 0.0453\tvalidation loss: 0.0697\t validation accuracy: 0.9778\n",
      "iteration number: 3543\t training loss: 0.0449\tvalidation loss: 0.0690\t validation accuracy: 0.9778\n",
      "iteration number: 3544\t training loss: 0.0447\tvalidation loss: 0.0685\t validation accuracy: 0.9800\n",
      "iteration number: 3545\t training loss: 0.0447\tvalidation loss: 0.0683\t validation accuracy: 0.9778\n",
      "iteration number: 3546\t training loss: 0.0444\tvalidation loss: 0.0679\t validation accuracy: 0.9800\n",
      "iteration number: 3547\t training loss: 0.0443\tvalidation loss: 0.0673\t validation accuracy: 0.9800\n",
      "iteration number: 3548\t training loss: 0.0445\tvalidation loss: 0.0683\t validation accuracy: 0.9800\n",
      "iteration number: 3549\t training loss: 0.0443\tvalidation loss: 0.0678\t validation accuracy: 0.9800\n",
      "iteration number: 3550\t training loss: 0.0444\tvalidation loss: 0.0684\t validation accuracy: 0.9800\n",
      "iteration number: 3551\t training loss: 0.0442\tvalidation loss: 0.0677\t validation accuracy: 0.9800\n",
      "iteration number: 3552\t training loss: 0.0438\tvalidation loss: 0.0674\t validation accuracy: 0.9800\n",
      "iteration number: 3553\t training loss: 0.0438\tvalidation loss: 0.0671\t validation accuracy: 0.9800\n",
      "iteration number: 3554\t training loss: 0.0442\tvalidation loss: 0.0666\t validation accuracy: 0.9800\n",
      "iteration number: 3555\t training loss: 0.0439\tvalidation loss: 0.0665\t validation accuracy: 0.9800\n",
      "iteration number: 3556\t training loss: 0.0438\tvalidation loss: 0.0661\t validation accuracy: 0.9800\n",
      "iteration number: 3557\t training loss: 0.0445\tvalidation loss: 0.0662\t validation accuracy: 0.9800\n",
      "iteration number: 3558\t training loss: 0.0444\tvalidation loss: 0.0657\t validation accuracy: 0.9800\n",
      "iteration number: 3559\t training loss: 0.0444\tvalidation loss: 0.0663\t validation accuracy: 0.9800\n",
      "iteration number: 3560\t training loss: 0.0443\tvalidation loss: 0.0670\t validation accuracy: 0.9800\n",
      "iteration number: 3561\t training loss: 0.0448\tvalidation loss: 0.0670\t validation accuracy: 0.9800\n",
      "iteration number: 3562\t training loss: 0.0452\tvalidation loss: 0.0676\t validation accuracy: 0.9800\n",
      "iteration number: 3563\t training loss: 0.0455\tvalidation loss: 0.0682\t validation accuracy: 0.9822\n",
      "iteration number: 3564\t training loss: 0.0450\tvalidation loss: 0.0676\t validation accuracy: 0.9822\n",
      "iteration number: 3565\t training loss: 0.0444\tvalidation loss: 0.0683\t validation accuracy: 0.9800\n",
      "iteration number: 3566\t training loss: 0.0449\tvalidation loss: 0.0685\t validation accuracy: 0.9778\n",
      "iteration number: 3567\t training loss: 0.0451\tvalidation loss: 0.0688\t validation accuracy: 0.9778\n",
      "iteration number: 3568\t training loss: 0.0453\tvalidation loss: 0.0692\t validation accuracy: 0.9778\n",
      "iteration number: 3569\t training loss: 0.0444\tvalidation loss: 0.0679\t validation accuracy: 0.9800\n",
      "iteration number: 3570\t training loss: 0.0446\tvalidation loss: 0.0680\t validation accuracy: 0.9800\n",
      "iteration number: 3571\t training loss: 0.0442\tvalidation loss: 0.0679\t validation accuracy: 0.9800\n",
      "iteration number: 3572\t training loss: 0.0441\tvalidation loss: 0.0677\t validation accuracy: 0.9800\n",
      "iteration number: 3573\t training loss: 0.0442\tvalidation loss: 0.0684\t validation accuracy: 0.9800\n",
      "iteration number: 3574\t training loss: 0.0448\tvalidation loss: 0.0687\t validation accuracy: 0.9800\n",
      "iteration number: 3575\t training loss: 0.0450\tvalidation loss: 0.0683\t validation accuracy: 0.9800\n",
      "iteration number: 3576\t training loss: 0.0449\tvalidation loss: 0.0681\t validation accuracy: 0.9800\n",
      "iteration number: 3577\t training loss: 0.0442\tvalidation loss: 0.0679\t validation accuracy: 0.9800\n",
      "iteration number: 3578\t training loss: 0.0449\tvalidation loss: 0.0681\t validation accuracy: 0.9800\n",
      "iteration number: 3579\t training loss: 0.0451\tvalidation loss: 0.0683\t validation accuracy: 0.9800\n",
      "iteration number: 3580\t training loss: 0.0443\tvalidation loss: 0.0669\t validation accuracy: 0.9800\n",
      "iteration number: 3581\t training loss: 0.0440\tvalidation loss: 0.0667\t validation accuracy: 0.9800\n",
      "iteration number: 3582\t training loss: 0.0439\tvalidation loss: 0.0664\t validation accuracy: 0.9778\n",
      "iteration number: 3583\t training loss: 0.0439\tvalidation loss: 0.0660\t validation accuracy: 0.9800\n",
      "iteration number: 3584\t training loss: 0.0440\tvalidation loss: 0.0664\t validation accuracy: 0.9800\n",
      "iteration number: 3585\t training loss: 0.0439\tvalidation loss: 0.0658\t validation accuracy: 0.9800\n",
      "iteration number: 3586\t training loss: 0.0439\tvalidation loss: 0.0660\t validation accuracy: 0.9822\n",
      "iteration number: 3587\t training loss: 0.0439\tvalidation loss: 0.0661\t validation accuracy: 0.9800\n",
      "iteration number: 3588\t training loss: 0.0443\tvalidation loss: 0.0665\t validation accuracy: 0.9800\n",
      "iteration number: 3589\t training loss: 0.0435\tvalidation loss: 0.0662\t validation accuracy: 0.9800\n",
      "iteration number: 3590\t training loss: 0.0437\tvalidation loss: 0.0664\t validation accuracy: 0.9800\n",
      "iteration number: 3591\t training loss: 0.0438\tvalidation loss: 0.0665\t validation accuracy: 0.9800\n",
      "iteration number: 3592\t training loss: 0.0432\tvalidation loss: 0.0656\t validation accuracy: 0.9800\n",
      "iteration number: 3593\t training loss: 0.0444\tvalidation loss: 0.0661\t validation accuracy: 0.9800\n",
      "iteration number: 3594\t training loss: 0.0436\tvalidation loss: 0.0664\t validation accuracy: 0.9800\n",
      "iteration number: 3595\t training loss: 0.0436\tvalidation loss: 0.0664\t validation accuracy: 0.9778\n",
      "iteration number: 3596\t training loss: 0.0434\tvalidation loss: 0.0665\t validation accuracy: 0.9778\n",
      "iteration number: 3597\t training loss: 0.0439\tvalidation loss: 0.0673\t validation accuracy: 0.9800\n",
      "iteration number: 3598\t training loss: 0.0443\tvalidation loss: 0.0696\t validation accuracy: 0.9733\n",
      "iteration number: 3599\t training loss: 0.0444\tvalidation loss: 0.0699\t validation accuracy: 0.9756\n",
      "iteration number: 3600\t training loss: 0.0438\tvalidation loss: 0.0680\t validation accuracy: 0.9800\n",
      "iteration number: 3601\t training loss: 0.0450\tvalidation loss: 0.0692\t validation accuracy: 0.9778\n",
      "iteration number: 3602\t training loss: 0.0435\tvalidation loss: 0.0676\t validation accuracy: 0.9778\n",
      "iteration number: 3603\t training loss: 0.0435\tvalidation loss: 0.0677\t validation accuracy: 0.9800\n",
      "iteration number: 3604\t training loss: 0.0441\tvalidation loss: 0.0682\t validation accuracy: 0.9822\n",
      "iteration number: 3605\t training loss: 0.0441\tvalidation loss: 0.0682\t validation accuracy: 0.9822\n",
      "iteration number: 3606\t training loss: 0.0435\tvalidation loss: 0.0671\t validation accuracy: 0.9822\n",
      "iteration number: 3607\t training loss: 0.0429\tvalidation loss: 0.0655\t validation accuracy: 0.9800\n",
      "iteration number: 3608\t training loss: 0.0436\tvalidation loss: 0.0655\t validation accuracy: 0.9822\n",
      "iteration number: 3609\t training loss: 0.0438\tvalidation loss: 0.0665\t validation accuracy: 0.9800\n",
      "iteration number: 3610\t training loss: 0.0437\tvalidation loss: 0.0665\t validation accuracy: 0.9800\n",
      "iteration number: 3611\t training loss: 0.0430\tvalidation loss: 0.0652\t validation accuracy: 0.9822\n",
      "iteration number: 3612\t training loss: 0.0431\tvalidation loss: 0.0652\t validation accuracy: 0.9822\n",
      "iteration number: 3613\t training loss: 0.0432\tvalidation loss: 0.0653\t validation accuracy: 0.9822\n",
      "iteration number: 3614\t training loss: 0.0433\tvalidation loss: 0.0655\t validation accuracy: 0.9822\n",
      "iteration number: 3615\t training loss: 0.0433\tvalidation loss: 0.0647\t validation accuracy: 0.9844\n",
      "iteration number: 3616\t training loss: 0.0432\tvalidation loss: 0.0644\t validation accuracy: 0.9822\n",
      "iteration number: 3617\t training loss: 0.0445\tvalidation loss: 0.0649\t validation accuracy: 0.9822\n",
      "iteration number: 3618\t training loss: 0.0438\tvalidation loss: 0.0646\t validation accuracy: 0.9844\n",
      "iteration number: 3619\t training loss: 0.0429\tvalidation loss: 0.0645\t validation accuracy: 0.9822\n",
      "iteration number: 3620\t training loss: 0.0429\tvalidation loss: 0.0652\t validation accuracy: 0.9822\n",
      "iteration number: 3621\t training loss: 0.0430\tvalidation loss: 0.0654\t validation accuracy: 0.9822\n",
      "iteration number: 3622\t training loss: 0.0455\tvalidation loss: 0.0671\t validation accuracy: 0.9778\n",
      "iteration number: 3623\t training loss: 0.0452\tvalidation loss: 0.0665\t validation accuracy: 0.9778\n",
      "iteration number: 3624\t training loss: 0.0438\tvalidation loss: 0.0657\t validation accuracy: 0.9800\n",
      "iteration number: 3625\t training loss: 0.0428\tvalidation loss: 0.0641\t validation accuracy: 0.9822\n",
      "iteration number: 3626\t training loss: 0.0427\tvalidation loss: 0.0645\t validation accuracy: 0.9822\n",
      "iteration number: 3627\t training loss: 0.0428\tvalidation loss: 0.0643\t validation accuracy: 0.9822\n",
      "iteration number: 3628\t training loss: 0.0429\tvalidation loss: 0.0643\t validation accuracy: 0.9822\n",
      "iteration number: 3629\t training loss: 0.0431\tvalidation loss: 0.0647\t validation accuracy: 0.9822\n",
      "iteration number: 3630\t training loss: 0.0430\tvalidation loss: 0.0647\t validation accuracy: 0.9822\n",
      "iteration number: 3631\t training loss: 0.0434\tvalidation loss: 0.0656\t validation accuracy: 0.9822\n",
      "iteration number: 3632\t training loss: 0.0432\tvalidation loss: 0.0651\t validation accuracy: 0.9822\n",
      "iteration number: 3633\t training loss: 0.0431\tvalidation loss: 0.0653\t validation accuracy: 0.9822\n",
      "iteration number: 3634\t training loss: 0.0434\tvalidation loss: 0.0652\t validation accuracy: 0.9800\n",
      "iteration number: 3635\t training loss: 0.0438\tvalidation loss: 0.0656\t validation accuracy: 0.9800\n",
      "iteration number: 3636\t training loss: 0.0436\tvalidation loss: 0.0655\t validation accuracy: 0.9800\n",
      "iteration number: 3637\t training loss: 0.0432\tvalidation loss: 0.0654\t validation accuracy: 0.9800\n",
      "iteration number: 3638\t training loss: 0.0436\tvalidation loss: 0.0655\t validation accuracy: 0.9800\n",
      "iteration number: 3639\t training loss: 0.0431\tvalidation loss: 0.0653\t validation accuracy: 0.9822\n",
      "iteration number: 3640\t training loss: 0.0432\tvalidation loss: 0.0655\t validation accuracy: 0.9822\n",
      "iteration number: 3641\t training loss: 0.0435\tvalidation loss: 0.0646\t validation accuracy: 0.9822\n",
      "iteration number: 3642\t training loss: 0.0437\tvalidation loss: 0.0646\t validation accuracy: 0.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3643\t training loss: 0.0436\tvalidation loss: 0.0646\t validation accuracy: 0.9800\n",
      "iteration number: 3644\t training loss: 0.0440\tvalidation loss: 0.0644\t validation accuracy: 0.9844\n",
      "iteration number: 3645\t training loss: 0.0430\tvalidation loss: 0.0647\t validation accuracy: 0.9822\n",
      "iteration number: 3646\t training loss: 0.0427\tvalidation loss: 0.0646\t validation accuracy: 0.9822\n",
      "iteration number: 3647\t training loss: 0.0427\tvalidation loss: 0.0649\t validation accuracy: 0.9822\n",
      "iteration number: 3648\t training loss: 0.0427\tvalidation loss: 0.0646\t validation accuracy: 0.9800\n",
      "iteration number: 3649\t training loss: 0.0425\tvalidation loss: 0.0648\t validation accuracy: 0.9822\n",
      "iteration number: 3650\t training loss: 0.0430\tvalidation loss: 0.0668\t validation accuracy: 0.9822\n",
      "iteration number: 3651\t training loss: 0.0424\tvalidation loss: 0.0653\t validation accuracy: 0.9822\n",
      "iteration number: 3652\t training loss: 0.0432\tvalidation loss: 0.0661\t validation accuracy: 0.9800\n",
      "iteration number: 3653\t training loss: 0.0425\tvalidation loss: 0.0654\t validation accuracy: 0.9822\n",
      "iteration number: 3654\t training loss: 0.0429\tvalidation loss: 0.0660\t validation accuracy: 0.9800\n",
      "iteration number: 3655\t training loss: 0.0439\tvalidation loss: 0.0661\t validation accuracy: 0.9778\n",
      "iteration number: 3656\t training loss: 0.0438\tvalidation loss: 0.0663\t validation accuracy: 0.9778\n",
      "iteration number: 3657\t training loss: 0.0433\tvalidation loss: 0.0663\t validation accuracy: 0.9778\n",
      "iteration number: 3658\t training loss: 0.0430\tvalidation loss: 0.0657\t validation accuracy: 0.9800\n",
      "iteration number: 3659\t training loss: 0.0441\tvalidation loss: 0.0666\t validation accuracy: 0.9800\n",
      "iteration number: 3660\t training loss: 0.0436\tvalidation loss: 0.0662\t validation accuracy: 0.9800\n",
      "iteration number: 3661\t training loss: 0.0437\tvalidation loss: 0.0662\t validation accuracy: 0.9822\n",
      "iteration number: 3662\t training loss: 0.0456\tvalidation loss: 0.0668\t validation accuracy: 0.9822\n",
      "iteration number: 3663\t training loss: 0.0448\tvalidation loss: 0.0664\t validation accuracy: 0.9822\n",
      "iteration number: 3664\t training loss: 0.0447\tvalidation loss: 0.0661\t validation accuracy: 0.9822\n",
      "iteration number: 3665\t training loss: 0.0435\tvalidation loss: 0.0659\t validation accuracy: 0.9822\n",
      "iteration number: 3666\t training loss: 0.0441\tvalidation loss: 0.0656\t validation accuracy: 0.9822\n",
      "iteration number: 3667\t training loss: 0.0457\tvalidation loss: 0.0670\t validation accuracy: 0.9800\n",
      "iteration number: 3668\t training loss: 0.0441\tvalidation loss: 0.0653\t validation accuracy: 0.9844\n",
      "iteration number: 3669\t training loss: 0.0426\tvalidation loss: 0.0641\t validation accuracy: 0.9822\n",
      "iteration number: 3670\t training loss: 0.0442\tvalidation loss: 0.0648\t validation accuracy: 0.9822\n",
      "iteration number: 3671\t training loss: 0.0439\tvalidation loss: 0.0640\t validation accuracy: 0.9822\n",
      "iteration number: 3672\t training loss: 0.0436\tvalidation loss: 0.0638\t validation accuracy: 0.9822\n",
      "iteration number: 3673\t training loss: 0.0428\tvalidation loss: 0.0639\t validation accuracy: 0.9844\n",
      "iteration number: 3674\t training loss: 0.0427\tvalidation loss: 0.0638\t validation accuracy: 0.9822\n",
      "iteration number: 3675\t training loss: 0.0427\tvalidation loss: 0.0644\t validation accuracy: 0.9844\n",
      "iteration number: 3676\t training loss: 0.0427\tvalidation loss: 0.0642\t validation accuracy: 0.9844\n",
      "iteration number: 3677\t training loss: 0.0437\tvalidation loss: 0.0658\t validation accuracy: 0.9822\n",
      "iteration number: 3678\t training loss: 0.0428\tvalidation loss: 0.0656\t validation accuracy: 0.9800\n",
      "iteration number: 3679\t training loss: 0.0430\tvalidation loss: 0.0665\t validation accuracy: 0.9778\n",
      "iteration number: 3680\t training loss: 0.0430\tvalidation loss: 0.0660\t validation accuracy: 0.9800\n",
      "iteration number: 3681\t training loss: 0.0438\tvalidation loss: 0.0670\t validation accuracy: 0.9800\n",
      "iteration number: 3682\t training loss: 0.0441\tvalidation loss: 0.0665\t validation accuracy: 0.9800\n",
      "iteration number: 3683\t training loss: 0.0430\tvalidation loss: 0.0653\t validation accuracy: 0.9822\n",
      "iteration number: 3684\t training loss: 0.0447\tvalidation loss: 0.0672\t validation accuracy: 0.9778\n",
      "iteration number: 3685\t training loss: 0.0427\tvalidation loss: 0.0652\t validation accuracy: 0.9822\n",
      "iteration number: 3686\t training loss: 0.0425\tvalidation loss: 0.0649\t validation accuracy: 0.9822\n",
      "iteration number: 3687\t training loss: 0.0422\tvalidation loss: 0.0648\t validation accuracy: 0.9800\n",
      "iteration number: 3688\t training loss: 0.0425\tvalidation loss: 0.0652\t validation accuracy: 0.9822\n",
      "iteration number: 3689\t training loss: 0.0423\tvalidation loss: 0.0650\t validation accuracy: 0.9822\n",
      "iteration number: 3690\t training loss: 0.0421\tvalidation loss: 0.0651\t validation accuracy: 0.9800\n",
      "iteration number: 3691\t training loss: 0.0423\tvalidation loss: 0.0651\t validation accuracy: 0.9822\n",
      "iteration number: 3692\t training loss: 0.0425\tvalidation loss: 0.0648\t validation accuracy: 0.9822\n",
      "iteration number: 3693\t training loss: 0.0427\tvalidation loss: 0.0646\t validation accuracy: 0.9822\n",
      "iteration number: 3694\t training loss: 0.0426\tvalidation loss: 0.0639\t validation accuracy: 0.9844\n",
      "iteration number: 3695\t training loss: 0.0427\tvalidation loss: 0.0639\t validation accuracy: 0.9844\n",
      "iteration number: 3696\t training loss: 0.0426\tvalidation loss: 0.0637\t validation accuracy: 0.9844\n",
      "iteration number: 3697\t training loss: 0.0423\tvalidation loss: 0.0641\t validation accuracy: 0.9822\n",
      "iteration number: 3698\t training loss: 0.0426\tvalidation loss: 0.0643\t validation accuracy: 0.9822\n",
      "iteration number: 3699\t training loss: 0.0426\tvalidation loss: 0.0641\t validation accuracy: 0.9822\n",
      "iteration number: 3700\t training loss: 0.0438\tvalidation loss: 0.0642\t validation accuracy: 0.9844\n",
      "iteration number: 3701\t training loss: 0.0428\tvalidation loss: 0.0641\t validation accuracy: 0.9844\n",
      "iteration number: 3702\t training loss: 0.0427\tvalidation loss: 0.0640\t validation accuracy: 0.9844\n",
      "iteration number: 3703\t training loss: 0.0427\tvalidation loss: 0.0637\t validation accuracy: 0.9822\n",
      "iteration number: 3704\t training loss: 0.0423\tvalidation loss: 0.0642\t validation accuracy: 0.9800\n",
      "iteration number: 3705\t training loss: 0.0417\tvalidation loss: 0.0648\t validation accuracy: 0.9822\n",
      "iteration number: 3706\t training loss: 0.0420\tvalidation loss: 0.0652\t validation accuracy: 0.9822\n",
      "iteration number: 3707\t training loss: 0.0419\tvalidation loss: 0.0648\t validation accuracy: 0.9822\n",
      "iteration number: 3708\t training loss: 0.0419\tvalidation loss: 0.0650\t validation accuracy: 0.9844\n",
      "iteration number: 3709\t training loss: 0.0420\tvalidation loss: 0.0658\t validation accuracy: 0.9844\n",
      "iteration number: 3710\t training loss: 0.0421\tvalidation loss: 0.0654\t validation accuracy: 0.9822\n",
      "iteration number: 3711\t training loss: 0.0419\tvalidation loss: 0.0652\t validation accuracy: 0.9844\n",
      "iteration number: 3712\t training loss: 0.0420\tvalidation loss: 0.0647\t validation accuracy: 0.9844\n",
      "iteration number: 3713\t training loss: 0.0419\tvalidation loss: 0.0648\t validation accuracy: 0.9822\n",
      "iteration number: 3714\t training loss: 0.0424\tvalidation loss: 0.0650\t validation accuracy: 0.9822\n",
      "iteration number: 3715\t training loss: 0.0421\tvalidation loss: 0.0646\t validation accuracy: 0.9822\n",
      "iteration number: 3716\t training loss: 0.0428\tvalidation loss: 0.0658\t validation accuracy: 0.9822\n",
      "iteration number: 3717\t training loss: 0.0426\tvalidation loss: 0.0655\t validation accuracy: 0.9822\n",
      "iteration number: 3718\t training loss: 0.0422\tvalidation loss: 0.0655\t validation accuracy: 0.9822\n",
      "iteration number: 3719\t training loss: 0.0432\tvalidation loss: 0.0685\t validation accuracy: 0.9822\n",
      "iteration number: 3720\t training loss: 0.0432\tvalidation loss: 0.0683\t validation accuracy: 0.9822\n",
      "iteration number: 3721\t training loss: 0.0428\tvalidation loss: 0.0678\t validation accuracy: 0.9822\n",
      "iteration number: 3722\t training loss: 0.0426\tvalidation loss: 0.0670\t validation accuracy: 0.9822\n",
      "iteration number: 3723\t training loss: 0.0426\tvalidation loss: 0.0668\t validation accuracy: 0.9822\n",
      "iteration number: 3724\t training loss: 0.0428\tvalidation loss: 0.0669\t validation accuracy: 0.9822\n",
      "iteration number: 3725\t training loss: 0.0424\tvalidation loss: 0.0670\t validation accuracy: 0.9800\n",
      "iteration number: 3726\t training loss: 0.0426\tvalidation loss: 0.0675\t validation accuracy: 0.9800\n",
      "iteration number: 3727\t training loss: 0.0427\tvalidation loss: 0.0669\t validation accuracy: 0.9800\n",
      "iteration number: 3728\t training loss: 0.0417\tvalidation loss: 0.0677\t validation accuracy: 0.9800\n",
      "iteration number: 3729\t training loss: 0.0418\tvalidation loss: 0.0676\t validation accuracy: 0.9778\n",
      "iteration number: 3730\t training loss: 0.0426\tvalidation loss: 0.0676\t validation accuracy: 0.9800\n",
      "iteration number: 3731\t training loss: 0.0421\tvalidation loss: 0.0670\t validation accuracy: 0.9800\n",
      "iteration number: 3732\t training loss: 0.0418\tvalidation loss: 0.0672\t validation accuracy: 0.9800\n",
      "iteration number: 3733\t training loss: 0.0422\tvalidation loss: 0.0667\t validation accuracy: 0.9778\n",
      "iteration number: 3734\t training loss: 0.0418\tvalidation loss: 0.0665\t validation accuracy: 0.9778\n",
      "iteration number: 3735\t training loss: 0.0413\tvalidation loss: 0.0666\t validation accuracy: 0.9778\n",
      "iteration number: 3736\t training loss: 0.0415\tvalidation loss: 0.0667\t validation accuracy: 0.9822\n",
      "iteration number: 3737\t training loss: 0.0414\tvalidation loss: 0.0664\t validation accuracy: 0.9800\n",
      "iteration number: 3738\t training loss: 0.0417\tvalidation loss: 0.0673\t validation accuracy: 0.9800\n",
      "iteration number: 3739\t training loss: 0.0416\tvalidation loss: 0.0671\t validation accuracy: 0.9800\n",
      "iteration number: 3740\t training loss: 0.0414\tvalidation loss: 0.0660\t validation accuracy: 0.9822\n",
      "iteration number: 3741\t training loss: 0.0412\tvalidation loss: 0.0662\t validation accuracy: 0.9800\n",
      "iteration number: 3742\t training loss: 0.0411\tvalidation loss: 0.0665\t validation accuracy: 0.9800\n",
      "iteration number: 3743\t training loss: 0.0414\tvalidation loss: 0.0663\t validation accuracy: 0.9822\n",
      "iteration number: 3744\t training loss: 0.0414\tvalidation loss: 0.0665\t validation accuracy: 0.9822\n",
      "iteration number: 3745\t training loss: 0.0416\tvalidation loss: 0.0662\t validation accuracy: 0.9800\n",
      "iteration number: 3746\t training loss: 0.0417\tvalidation loss: 0.0667\t validation accuracy: 0.9778\n",
      "iteration number: 3747\t training loss: 0.0413\tvalidation loss: 0.0667\t validation accuracy: 0.9778\n",
      "iteration number: 3748\t training loss: 0.0417\tvalidation loss: 0.0679\t validation accuracy: 0.9756\n",
      "iteration number: 3749\t training loss: 0.0422\tvalidation loss: 0.0684\t validation accuracy: 0.9778\n",
      "iteration number: 3750\t training loss: 0.0430\tvalidation loss: 0.0687\t validation accuracy: 0.9778\n",
      "iteration number: 3751\t training loss: 0.0430\tvalidation loss: 0.0688\t validation accuracy: 0.9778\n",
      "iteration number: 3752\t training loss: 0.0412\tvalidation loss: 0.0660\t validation accuracy: 0.9800\n",
      "iteration number: 3753\t training loss: 0.0412\tvalidation loss: 0.0665\t validation accuracy: 0.9756\n",
      "iteration number: 3754\t training loss: 0.0419\tvalidation loss: 0.0677\t validation accuracy: 0.9778\n",
      "iteration number: 3755\t training loss: 0.0420\tvalidation loss: 0.0672\t validation accuracy: 0.9756\n",
      "iteration number: 3756\t training loss: 0.0417\tvalidation loss: 0.0675\t validation accuracy: 0.9756\n",
      "iteration number: 3757\t training loss: 0.0419\tvalidation loss: 0.0672\t validation accuracy: 0.9756\n",
      "iteration number: 3758\t training loss: 0.0413\tvalidation loss: 0.0656\t validation accuracy: 0.9778\n",
      "iteration number: 3759\t training loss: 0.0412\tvalidation loss: 0.0659\t validation accuracy: 0.9778\n",
      "iteration number: 3760\t training loss: 0.0410\tvalidation loss: 0.0652\t validation accuracy: 0.9800\n",
      "iteration number: 3761\t training loss: 0.0408\tvalidation loss: 0.0649\t validation accuracy: 0.9800\n",
      "iteration number: 3762\t training loss: 0.0417\tvalidation loss: 0.0655\t validation accuracy: 0.9800\n",
      "iteration number: 3763\t training loss: 0.0416\tvalidation loss: 0.0660\t validation accuracy: 0.9778\n",
      "iteration number: 3764\t training loss: 0.0412\tvalidation loss: 0.0653\t validation accuracy: 0.9800\n",
      "iteration number: 3765\t training loss: 0.0411\tvalidation loss: 0.0662\t validation accuracy: 0.9778\n",
      "iteration number: 3766\t training loss: 0.0409\tvalidation loss: 0.0653\t validation accuracy: 0.9822\n",
      "iteration number: 3767\t training loss: 0.0407\tvalidation loss: 0.0651\t validation accuracy: 0.9800\n",
      "iteration number: 3768\t training loss: 0.0407\tvalidation loss: 0.0655\t validation accuracy: 0.9800\n",
      "iteration number: 3769\t training loss: 0.0409\tvalidation loss: 0.0653\t validation accuracy: 0.9800\n",
      "iteration number: 3770\t training loss: 0.0409\tvalidation loss: 0.0661\t validation accuracy: 0.9800\n",
      "iteration number: 3771\t training loss: 0.0412\tvalidation loss: 0.0666\t validation accuracy: 0.9756\n",
      "iteration number: 3772\t training loss: 0.0412\tvalidation loss: 0.0668\t validation accuracy: 0.9756\n",
      "iteration number: 3773\t training loss: 0.0417\tvalidation loss: 0.0679\t validation accuracy: 0.9756\n",
      "iteration number: 3774\t training loss: 0.0413\tvalidation loss: 0.0673\t validation accuracy: 0.9778\n",
      "iteration number: 3775\t training loss: 0.0416\tvalidation loss: 0.0675\t validation accuracy: 0.9756\n",
      "iteration number: 3776\t training loss: 0.0420\tvalidation loss: 0.0679\t validation accuracy: 0.9800\n",
      "iteration number: 3777\t training loss: 0.0416\tvalidation loss: 0.0671\t validation accuracy: 0.9822\n",
      "iteration number: 3778\t training loss: 0.0416\tvalidation loss: 0.0665\t validation accuracy: 0.9822\n",
      "iteration number: 3779\t training loss: 0.0414\tvalidation loss: 0.0665\t validation accuracy: 0.9822\n",
      "iteration number: 3780\t training loss: 0.0417\tvalidation loss: 0.0673\t validation accuracy: 0.9822\n",
      "iteration number: 3781\t training loss: 0.0417\tvalidation loss: 0.0679\t validation accuracy: 0.9778\n",
      "iteration number: 3782\t training loss: 0.0418\tvalidation loss: 0.0678\t validation accuracy: 0.9800\n",
      "iteration number: 3783\t training loss: 0.0407\tvalidation loss: 0.0657\t validation accuracy: 0.9800\n",
      "iteration number: 3784\t training loss: 0.0407\tvalidation loss: 0.0651\t validation accuracy: 0.9822\n",
      "iteration number: 3785\t training loss: 0.0408\tvalidation loss: 0.0650\t validation accuracy: 0.9822\n",
      "iteration number: 3786\t training loss: 0.0408\tvalidation loss: 0.0651\t validation accuracy: 0.9800\n",
      "iteration number: 3787\t training loss: 0.0409\tvalidation loss: 0.0645\t validation accuracy: 0.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3788\t training loss: 0.0420\tvalidation loss: 0.0646\t validation accuracy: 0.9800\n",
      "iteration number: 3789\t training loss: 0.0413\tvalidation loss: 0.0648\t validation accuracy: 0.9800\n",
      "iteration number: 3790\t training loss: 0.0410\tvalidation loss: 0.0647\t validation accuracy: 0.9778\n",
      "iteration number: 3791\t training loss: 0.0407\tvalidation loss: 0.0642\t validation accuracy: 0.9822\n",
      "iteration number: 3792\t training loss: 0.0407\tvalidation loss: 0.0645\t validation accuracy: 0.9800\n",
      "iteration number: 3793\t training loss: 0.0407\tvalidation loss: 0.0654\t validation accuracy: 0.9800\n",
      "iteration number: 3794\t training loss: 0.0413\tvalidation loss: 0.0661\t validation accuracy: 0.9778\n",
      "iteration number: 3795\t training loss: 0.0410\tvalidation loss: 0.0657\t validation accuracy: 0.9778\n",
      "iteration number: 3796\t training loss: 0.0410\tvalidation loss: 0.0660\t validation accuracy: 0.9778\n",
      "iteration number: 3797\t training loss: 0.0410\tvalidation loss: 0.0657\t validation accuracy: 0.9800\n",
      "iteration number: 3798\t training loss: 0.0405\tvalidation loss: 0.0648\t validation accuracy: 0.9800\n",
      "iteration number: 3799\t training loss: 0.0402\tvalidation loss: 0.0648\t validation accuracy: 0.9822\n",
      "iteration number: 3800\t training loss: 0.0404\tvalidation loss: 0.0647\t validation accuracy: 0.9800\n",
      "iteration number: 3801\t training loss: 0.0404\tvalidation loss: 0.0655\t validation accuracy: 0.9800\n",
      "iteration number: 3802\t training loss: 0.0403\tvalidation loss: 0.0652\t validation accuracy: 0.9800\n",
      "iteration number: 3803\t training loss: 0.0411\tvalidation loss: 0.0669\t validation accuracy: 0.9778\n",
      "iteration number: 3804\t training loss: 0.0403\tvalidation loss: 0.0649\t validation accuracy: 0.9800\n",
      "iteration number: 3805\t training loss: 0.0403\tvalidation loss: 0.0644\t validation accuracy: 0.9822\n",
      "iteration number: 3806\t training loss: 0.0404\tvalidation loss: 0.0643\t validation accuracy: 0.9822\n",
      "iteration number: 3807\t training loss: 0.0404\tvalidation loss: 0.0639\t validation accuracy: 0.9822\n",
      "iteration number: 3808\t training loss: 0.0404\tvalidation loss: 0.0640\t validation accuracy: 0.9822\n",
      "iteration number: 3809\t training loss: 0.0411\tvalidation loss: 0.0659\t validation accuracy: 0.9800\n",
      "iteration number: 3810\t training loss: 0.0414\tvalidation loss: 0.0663\t validation accuracy: 0.9800\n",
      "iteration number: 3811\t training loss: 0.0409\tvalidation loss: 0.0653\t validation accuracy: 0.9800\n",
      "iteration number: 3812\t training loss: 0.0411\tvalidation loss: 0.0659\t validation accuracy: 0.9822\n",
      "iteration number: 3813\t training loss: 0.0410\tvalidation loss: 0.0657\t validation accuracy: 0.9800\n",
      "iteration number: 3814\t training loss: 0.0426\tvalidation loss: 0.0677\t validation accuracy: 0.9778\n",
      "iteration number: 3815\t training loss: 0.0415\tvalidation loss: 0.0663\t validation accuracy: 0.9800\n",
      "iteration number: 3816\t training loss: 0.0410\tvalidation loss: 0.0660\t validation accuracy: 0.9822\n",
      "iteration number: 3817\t training loss: 0.0409\tvalidation loss: 0.0655\t validation accuracy: 0.9822\n",
      "iteration number: 3818\t training loss: 0.0405\tvalidation loss: 0.0651\t validation accuracy: 0.9822\n",
      "iteration number: 3819\t training loss: 0.0401\tvalidation loss: 0.0646\t validation accuracy: 0.9822\n",
      "iteration number: 3820\t training loss: 0.0401\tvalidation loss: 0.0648\t validation accuracy: 0.9822\n",
      "iteration number: 3821\t training loss: 0.0407\tvalidation loss: 0.0654\t validation accuracy: 0.9778\n",
      "iteration number: 3822\t training loss: 0.0403\tvalidation loss: 0.0648\t validation accuracy: 0.9778\n",
      "iteration number: 3823\t training loss: 0.0399\tvalidation loss: 0.0642\t validation accuracy: 0.9822\n",
      "iteration number: 3824\t training loss: 0.0400\tvalidation loss: 0.0644\t validation accuracy: 0.9822\n",
      "iteration number: 3825\t training loss: 0.0400\tvalidation loss: 0.0638\t validation accuracy: 0.9800\n",
      "iteration number: 3826\t training loss: 0.0400\tvalidation loss: 0.0632\t validation accuracy: 0.9822\n",
      "iteration number: 3827\t training loss: 0.0400\tvalidation loss: 0.0631\t validation accuracy: 0.9822\n",
      "iteration number: 3828\t training loss: 0.0400\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 3829\t training loss: 0.0399\tvalidation loss: 0.0625\t validation accuracy: 0.9822\n",
      "iteration number: 3830\t training loss: 0.0399\tvalidation loss: 0.0626\t validation accuracy: 0.9822\n",
      "iteration number: 3831\t training loss: 0.0400\tvalidation loss: 0.0630\t validation accuracy: 0.9822\n",
      "iteration number: 3832\t training loss: 0.0410\tvalidation loss: 0.0634\t validation accuracy: 0.9822\n",
      "iteration number: 3833\t training loss: 0.0408\tvalidation loss: 0.0631\t validation accuracy: 0.9822\n",
      "iteration number: 3834\t training loss: 0.0406\tvalidation loss: 0.0630\t validation accuracy: 0.9844\n",
      "iteration number: 3835\t training loss: 0.0406\tvalidation loss: 0.0628\t validation accuracy: 0.9844\n",
      "iteration number: 3836\t training loss: 0.0405\tvalidation loss: 0.0624\t validation accuracy: 0.9844\n",
      "iteration number: 3837\t training loss: 0.0402\tvalidation loss: 0.0627\t validation accuracy: 0.9844\n",
      "iteration number: 3838\t training loss: 0.0414\tvalidation loss: 0.0639\t validation accuracy: 0.9800\n",
      "iteration number: 3839\t training loss: 0.0412\tvalidation loss: 0.0637\t validation accuracy: 0.9822\n",
      "iteration number: 3840\t training loss: 0.0405\tvalidation loss: 0.0633\t validation accuracy: 0.9844\n",
      "iteration number: 3841\t training loss: 0.0402\tvalidation loss: 0.0620\t validation accuracy: 0.9844\n",
      "iteration number: 3842\t training loss: 0.0403\tvalidation loss: 0.0622\t validation accuracy: 0.9844\n",
      "iteration number: 3843\t training loss: 0.0404\tvalidation loss: 0.0618\t validation accuracy: 0.9844\n",
      "iteration number: 3844\t training loss: 0.0410\tvalidation loss: 0.0626\t validation accuracy: 0.9822\n",
      "iteration number: 3845\t training loss: 0.0404\tvalidation loss: 0.0622\t validation accuracy: 0.9844\n",
      "iteration number: 3846\t training loss: 0.0404\tvalidation loss: 0.0620\t validation accuracy: 0.9867\n",
      "iteration number: 3847\t training loss: 0.0400\tvalidation loss: 0.0620\t validation accuracy: 0.9844\n",
      "iteration number: 3848\t training loss: 0.0399\tvalidation loss: 0.0621\t validation accuracy: 0.9844\n",
      "iteration number: 3849\t training loss: 0.0401\tvalidation loss: 0.0621\t validation accuracy: 0.9844\n",
      "iteration number: 3850\t training loss: 0.0400\tvalidation loss: 0.0621\t validation accuracy: 0.9844\n",
      "iteration number: 3851\t training loss: 0.0400\tvalidation loss: 0.0622\t validation accuracy: 0.9844\n",
      "iteration number: 3852\t training loss: 0.0399\tvalidation loss: 0.0622\t validation accuracy: 0.9844\n",
      "iteration number: 3853\t training loss: 0.0400\tvalidation loss: 0.0623\t validation accuracy: 0.9844\n",
      "iteration number: 3854\t training loss: 0.0400\tvalidation loss: 0.0620\t validation accuracy: 0.9844\n",
      "iteration number: 3855\t training loss: 0.0404\tvalidation loss: 0.0622\t validation accuracy: 0.9844\n",
      "iteration number: 3856\t training loss: 0.0398\tvalidation loss: 0.0624\t validation accuracy: 0.9844\n",
      "iteration number: 3857\t training loss: 0.0396\tvalidation loss: 0.0621\t validation accuracy: 0.9844\n",
      "iteration number: 3858\t training loss: 0.0401\tvalidation loss: 0.0631\t validation accuracy: 0.9800\n",
      "iteration number: 3859\t training loss: 0.0401\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 3860\t training loss: 0.0402\tvalidation loss: 0.0640\t validation accuracy: 0.9800\n",
      "iteration number: 3861\t training loss: 0.0406\tvalidation loss: 0.0647\t validation accuracy: 0.9822\n",
      "iteration number: 3862\t training loss: 0.0396\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 3863\t training loss: 0.0402\tvalidation loss: 0.0642\t validation accuracy: 0.9822\n",
      "iteration number: 3864\t training loss: 0.0406\tvalidation loss: 0.0645\t validation accuracy: 0.9822\n",
      "iteration number: 3865\t training loss: 0.0406\tvalidation loss: 0.0649\t validation accuracy: 0.9822\n",
      "iteration number: 3866\t training loss: 0.0405\tvalidation loss: 0.0641\t validation accuracy: 0.9822\n",
      "iteration number: 3867\t training loss: 0.0402\tvalidation loss: 0.0641\t validation accuracy: 0.9822\n",
      "iteration number: 3868\t training loss: 0.0403\tvalidation loss: 0.0645\t validation accuracy: 0.9822\n",
      "iteration number: 3869\t training loss: 0.0402\tvalidation loss: 0.0648\t validation accuracy: 0.9844\n",
      "iteration number: 3870\t training loss: 0.0401\tvalidation loss: 0.0647\t validation accuracy: 0.9844\n",
      "iteration number: 3871\t training loss: 0.0402\tvalidation loss: 0.0644\t validation accuracy: 0.9822\n",
      "iteration number: 3872\t training loss: 0.0412\tvalidation loss: 0.0651\t validation accuracy: 0.9800\n",
      "iteration number: 3873\t training loss: 0.0416\tvalidation loss: 0.0669\t validation accuracy: 0.9778\n",
      "iteration number: 3874\t training loss: 0.0417\tvalidation loss: 0.0669\t validation accuracy: 0.9778\n",
      "iteration number: 3875\t training loss: 0.0420\tvalidation loss: 0.0677\t validation accuracy: 0.9756\n",
      "iteration number: 3876\t training loss: 0.0447\tvalidation loss: 0.0716\t validation accuracy: 0.9733\n",
      "iteration number: 3877\t training loss: 0.0419\tvalidation loss: 0.0680\t validation accuracy: 0.9800\n",
      "iteration number: 3878\t training loss: 0.0421\tvalidation loss: 0.0677\t validation accuracy: 0.9778\n",
      "iteration number: 3879\t training loss: 0.0412\tvalidation loss: 0.0664\t validation accuracy: 0.9800\n",
      "iteration number: 3880\t training loss: 0.0409\tvalidation loss: 0.0664\t validation accuracy: 0.9800\n",
      "iteration number: 3881\t training loss: 0.0400\tvalidation loss: 0.0644\t validation accuracy: 0.9800\n",
      "iteration number: 3882\t training loss: 0.0392\tvalidation loss: 0.0634\t validation accuracy: 0.9822\n",
      "iteration number: 3883\t training loss: 0.0390\tvalidation loss: 0.0630\t validation accuracy: 0.9822\n",
      "iteration number: 3884\t training loss: 0.0403\tvalidation loss: 0.0646\t validation accuracy: 0.9800\n",
      "iteration number: 3885\t training loss: 0.0400\tvalidation loss: 0.0640\t validation accuracy: 0.9800\n",
      "iteration number: 3886\t training loss: 0.0401\tvalidation loss: 0.0646\t validation accuracy: 0.9800\n",
      "iteration number: 3887\t training loss: 0.0397\tvalidation loss: 0.0642\t validation accuracy: 0.9800\n",
      "iteration number: 3888\t training loss: 0.0394\tvalidation loss: 0.0640\t validation accuracy: 0.9844\n",
      "iteration number: 3889\t training loss: 0.0396\tvalidation loss: 0.0638\t validation accuracy: 0.9822\n",
      "iteration number: 3890\t training loss: 0.0395\tvalidation loss: 0.0644\t validation accuracy: 0.9822\n",
      "iteration number: 3891\t training loss: 0.0392\tvalidation loss: 0.0635\t validation accuracy: 0.9800\n",
      "iteration number: 3892\t training loss: 0.0394\tvalidation loss: 0.0637\t validation accuracy: 0.9800\n",
      "iteration number: 3893\t training loss: 0.0390\tvalidation loss: 0.0633\t validation accuracy: 0.9844\n",
      "iteration number: 3894\t training loss: 0.0401\tvalidation loss: 0.0644\t validation accuracy: 0.9800\n",
      "iteration number: 3895\t training loss: 0.0402\tvalidation loss: 0.0651\t validation accuracy: 0.9778\n",
      "iteration number: 3896\t training loss: 0.0409\tvalidation loss: 0.0661\t validation accuracy: 0.9800\n",
      "iteration number: 3897\t training loss: 0.0398\tvalidation loss: 0.0645\t validation accuracy: 0.9822\n",
      "iteration number: 3898\t training loss: 0.0396\tvalidation loss: 0.0648\t validation accuracy: 0.9822\n",
      "iteration number: 3899\t training loss: 0.0395\tvalidation loss: 0.0644\t validation accuracy: 0.9800\n",
      "iteration number: 3900\t training loss: 0.0411\tvalidation loss: 0.0646\t validation accuracy: 0.9822\n",
      "iteration number: 3901\t training loss: 0.0402\tvalidation loss: 0.0631\t validation accuracy: 0.9822\n",
      "iteration number: 3902\t training loss: 0.0398\tvalidation loss: 0.0627\t validation accuracy: 0.9822\n",
      "iteration number: 3903\t training loss: 0.0410\tvalidation loss: 0.0640\t validation accuracy: 0.9778\n",
      "iteration number: 3904\t training loss: 0.0411\tvalidation loss: 0.0641\t validation accuracy: 0.9778\n",
      "iteration number: 3905\t training loss: 0.0397\tvalidation loss: 0.0638\t validation accuracy: 0.9800\n",
      "iteration number: 3906\t training loss: 0.0395\tvalidation loss: 0.0628\t validation accuracy: 0.9844\n",
      "iteration number: 3907\t training loss: 0.0395\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 3908\t training loss: 0.0393\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 3909\t training loss: 0.0392\tvalidation loss: 0.0628\t validation accuracy: 0.9844\n",
      "iteration number: 3910\t training loss: 0.0392\tvalidation loss: 0.0630\t validation accuracy: 0.9844\n",
      "iteration number: 3911\t training loss: 0.0397\tvalidation loss: 0.0634\t validation accuracy: 0.9844\n",
      "iteration number: 3912\t training loss: 0.0397\tvalidation loss: 0.0630\t validation accuracy: 0.9844\n",
      "iteration number: 3913\t training loss: 0.0393\tvalidation loss: 0.0631\t validation accuracy: 0.9844\n",
      "iteration number: 3914\t training loss: 0.0392\tvalidation loss: 0.0635\t validation accuracy: 0.9844\n",
      "iteration number: 3915\t training loss: 0.0393\tvalidation loss: 0.0632\t validation accuracy: 0.9822\n",
      "iteration number: 3916\t training loss: 0.0391\tvalidation loss: 0.0628\t validation accuracy: 0.9844\n",
      "iteration number: 3917\t training loss: 0.0393\tvalidation loss: 0.0634\t validation accuracy: 0.9844\n",
      "iteration number: 3918\t training loss: 0.0390\tvalidation loss: 0.0633\t validation accuracy: 0.9844\n",
      "iteration number: 3919\t training loss: 0.0389\tvalidation loss: 0.0632\t validation accuracy: 0.9844\n",
      "iteration number: 3920\t training loss: 0.0388\tvalidation loss: 0.0625\t validation accuracy: 0.9844\n",
      "iteration number: 3921\t training loss: 0.0387\tvalidation loss: 0.0621\t validation accuracy: 0.9822\n",
      "iteration number: 3922\t training loss: 0.0389\tvalidation loss: 0.0621\t validation accuracy: 0.9822\n",
      "iteration number: 3923\t training loss: 0.0395\tvalidation loss: 0.0623\t validation accuracy: 0.9822\n",
      "iteration number: 3924\t training loss: 0.0391\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 3925\t training loss: 0.0387\tvalidation loss: 0.0626\t validation accuracy: 0.9822\n",
      "iteration number: 3926\t training loss: 0.0390\tvalidation loss: 0.0628\t validation accuracy: 0.9844\n",
      "iteration number: 3927\t training loss: 0.0393\tvalidation loss: 0.0631\t validation accuracy: 0.9844\n",
      "iteration number: 3928\t training loss: 0.0393\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 3929\t training loss: 0.0391\tvalidation loss: 0.0627\t validation accuracy: 0.9822\n",
      "iteration number: 3930\t training loss: 0.0391\tvalidation loss: 0.0630\t validation accuracy: 0.9822\n",
      "iteration number: 3931\t training loss: 0.0388\tvalidation loss: 0.0625\t validation accuracy: 0.9822\n",
      "iteration number: 3932\t training loss: 0.0393\tvalidation loss: 0.0631\t validation accuracy: 0.9822\n",
      "iteration number: 3933\t training loss: 0.0395\tvalidation loss: 0.0633\t validation accuracy: 0.9822\n",
      "iteration number: 3934\t training loss: 0.0392\tvalidation loss: 0.0633\t validation accuracy: 0.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3935\t training loss: 0.0387\tvalidation loss: 0.0627\t validation accuracy: 0.9822\n",
      "iteration number: 3936\t training loss: 0.0392\tvalidation loss: 0.0633\t validation accuracy: 0.9822\n",
      "iteration number: 3937\t training loss: 0.0393\tvalidation loss: 0.0637\t validation accuracy: 0.9822\n",
      "iteration number: 3938\t training loss: 0.0391\tvalidation loss: 0.0628\t validation accuracy: 0.9844\n",
      "iteration number: 3939\t training loss: 0.0390\tvalidation loss: 0.0630\t validation accuracy: 0.9844\n",
      "iteration number: 3940\t training loss: 0.0397\tvalidation loss: 0.0635\t validation accuracy: 0.9822\n",
      "iteration number: 3941\t training loss: 0.0394\tvalidation loss: 0.0627\t validation accuracy: 0.9822\n",
      "iteration number: 3942\t training loss: 0.0394\tvalidation loss: 0.0627\t validation accuracy: 0.9822\n",
      "iteration number: 3943\t training loss: 0.0399\tvalidation loss: 0.0627\t validation accuracy: 0.9822\n",
      "iteration number: 3944\t training loss: 0.0393\tvalidation loss: 0.0623\t validation accuracy: 0.9800\n",
      "iteration number: 3945\t training loss: 0.0388\tvalidation loss: 0.0621\t validation accuracy: 0.9822\n",
      "iteration number: 3946\t training loss: 0.0387\tvalidation loss: 0.0623\t validation accuracy: 0.9822\n",
      "iteration number: 3947\t training loss: 0.0387\tvalidation loss: 0.0617\t validation accuracy: 0.9822\n",
      "iteration number: 3948\t training loss: 0.0387\tvalidation loss: 0.0618\t validation accuracy: 0.9822\n",
      "iteration number: 3949\t training loss: 0.0384\tvalidation loss: 0.0624\t validation accuracy: 0.9822\n",
      "iteration number: 3950\t training loss: 0.0387\tvalidation loss: 0.0632\t validation accuracy: 0.9822\n",
      "iteration number: 3951\t training loss: 0.0392\tvalidation loss: 0.0637\t validation accuracy: 0.9844\n",
      "iteration number: 3952\t training loss: 0.0388\tvalidation loss: 0.0625\t validation accuracy: 0.9844\n",
      "iteration number: 3953\t training loss: 0.0383\tvalidation loss: 0.0617\t validation accuracy: 0.9822\n",
      "iteration number: 3954\t training loss: 0.0383\tvalidation loss: 0.0618\t validation accuracy: 0.9822\n",
      "iteration number: 3955\t training loss: 0.0383\tvalidation loss: 0.0620\t validation accuracy: 0.9822\n",
      "iteration number: 3956\t training loss: 0.0387\tvalidation loss: 0.0625\t validation accuracy: 0.9822\n",
      "iteration number: 3957\t training loss: 0.0392\tvalidation loss: 0.0626\t validation accuracy: 0.9822\n",
      "iteration number: 3958\t training loss: 0.0386\tvalidation loss: 0.0629\t validation accuracy: 0.9867\n",
      "iteration number: 3959\t training loss: 0.0394\tvalidation loss: 0.0636\t validation accuracy: 0.9844\n",
      "iteration number: 3960\t training loss: 0.0395\tvalidation loss: 0.0642\t validation accuracy: 0.9844\n",
      "iteration number: 3961\t training loss: 0.0398\tvalidation loss: 0.0651\t validation accuracy: 0.9778\n",
      "iteration number: 3962\t training loss: 0.0398\tvalidation loss: 0.0648\t validation accuracy: 0.9822\n",
      "iteration number: 3963\t training loss: 0.0385\tvalidation loss: 0.0627\t validation accuracy: 0.9822\n",
      "iteration number: 3964\t training loss: 0.0394\tvalidation loss: 0.0642\t validation accuracy: 0.9822\n",
      "iteration number: 3965\t training loss: 0.0400\tvalidation loss: 0.0645\t validation accuracy: 0.9822\n",
      "iteration number: 3966\t training loss: 0.0393\tvalidation loss: 0.0638\t validation accuracy: 0.9822\n",
      "iteration number: 3967\t training loss: 0.0391\tvalidation loss: 0.0635\t validation accuracy: 0.9800\n",
      "iteration number: 3968\t training loss: 0.0392\tvalidation loss: 0.0638\t validation accuracy: 0.9822\n",
      "iteration number: 3969\t training loss: 0.0389\tvalidation loss: 0.0637\t validation accuracy: 0.9822\n",
      "iteration number: 3970\t training loss: 0.0388\tvalidation loss: 0.0637\t validation accuracy: 0.9822\n",
      "iteration number: 3971\t training loss: 0.0400\tvalidation loss: 0.0646\t validation accuracy: 0.9778\n",
      "iteration number: 3972\t training loss: 0.0394\tvalidation loss: 0.0635\t validation accuracy: 0.9822\n",
      "iteration number: 3973\t training loss: 0.0397\tvalidation loss: 0.0637\t validation accuracy: 0.9822\n",
      "iteration number: 3974\t training loss: 0.0398\tvalidation loss: 0.0635\t validation accuracy: 0.9822\n",
      "iteration number: 3975\t training loss: 0.0404\tvalidation loss: 0.0639\t validation accuracy: 0.9822\n",
      "iteration number: 3976\t training loss: 0.0392\tvalidation loss: 0.0633\t validation accuracy: 0.9822\n",
      "iteration number: 3977\t training loss: 0.0389\tvalidation loss: 0.0634\t validation accuracy: 0.9822\n",
      "iteration number: 3978\t training loss: 0.0388\tvalidation loss: 0.0635\t validation accuracy: 0.9822\n",
      "iteration number: 3979\t training loss: 0.0389\tvalidation loss: 0.0639\t validation accuracy: 0.9822\n",
      "iteration number: 3980\t training loss: 0.0388\tvalidation loss: 0.0644\t validation accuracy: 0.9822\n",
      "iteration number: 3981\t training loss: 0.0397\tvalidation loss: 0.0650\t validation accuracy: 0.9822\n",
      "iteration number: 3982\t training loss: 0.0397\tvalidation loss: 0.0650\t validation accuracy: 0.9822\n",
      "iteration number: 3983\t training loss: 0.0394\tvalidation loss: 0.0649\t validation accuracy: 0.9822\n",
      "iteration number: 3984\t training loss: 0.0384\tvalidation loss: 0.0638\t validation accuracy: 0.9800\n",
      "iteration number: 3985\t training loss: 0.0399\tvalidation loss: 0.0665\t validation accuracy: 0.9800\n",
      "iteration number: 3986\t training loss: 0.0387\tvalidation loss: 0.0646\t validation accuracy: 0.9822\n",
      "iteration number: 3987\t training loss: 0.0387\tvalidation loss: 0.0645\t validation accuracy: 0.9822\n",
      "iteration number: 3988\t training loss: 0.0391\tvalidation loss: 0.0648\t validation accuracy: 0.9800\n",
      "iteration number: 3989\t training loss: 0.0388\tvalidation loss: 0.0645\t validation accuracy: 0.9822\n",
      "iteration number: 3990\t training loss: 0.0385\tvalidation loss: 0.0640\t validation accuracy: 0.9822\n",
      "iteration number: 3991\t training loss: 0.0395\tvalidation loss: 0.0639\t validation accuracy: 0.9822\n",
      "iteration number: 3992\t training loss: 0.0391\tvalidation loss: 0.0631\t validation accuracy: 0.9822\n",
      "iteration number: 3993\t training loss: 0.0386\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 3994\t training loss: 0.0381\tvalidation loss: 0.0627\t validation accuracy: 0.9822\n",
      "iteration number: 3995\t training loss: 0.0382\tvalidation loss: 0.0631\t validation accuracy: 0.9822\n",
      "iteration number: 3996\t training loss: 0.0382\tvalidation loss: 0.0626\t validation accuracy: 0.9822\n",
      "iteration number: 3997\t training loss: 0.0382\tvalidation loss: 0.0630\t validation accuracy: 0.9822\n",
      "iteration number: 3998\t training loss: 0.0380\tvalidation loss: 0.0634\t validation accuracy: 0.9822\n",
      "iteration number: 3999\t training loss: 0.0379\tvalidation loss: 0.0625\t validation accuracy: 0.9822\n",
      "iteration number: 4000\t training loss: 0.0382\tvalidation loss: 0.0627\t validation accuracy: 0.9822\n",
      "iteration number: 4001\t training loss: 0.0384\tvalidation loss: 0.0635\t validation accuracy: 0.9822\n",
      "iteration number: 4002\t training loss: 0.0384\tvalidation loss: 0.0622\t validation accuracy: 0.9844\n",
      "iteration number: 4003\t training loss: 0.0385\tvalidation loss: 0.0632\t validation accuracy: 0.9800\n",
      "iteration number: 4004\t training loss: 0.0383\tvalidation loss: 0.0628\t validation accuracy: 0.9800\n",
      "iteration number: 4005\t training loss: 0.0384\tvalidation loss: 0.0630\t validation accuracy: 0.9822\n",
      "iteration number: 4006\t training loss: 0.0381\tvalidation loss: 0.0633\t validation accuracy: 0.9822\n",
      "iteration number: 4007\t training loss: 0.0381\tvalidation loss: 0.0637\t validation accuracy: 0.9800\n",
      "iteration number: 4008\t training loss: 0.0382\tvalidation loss: 0.0637\t validation accuracy: 0.9800\n",
      "iteration number: 4009\t training loss: 0.0381\tvalidation loss: 0.0628\t validation accuracy: 0.9822\n",
      "iteration number: 4010\t training loss: 0.0388\tvalidation loss: 0.0637\t validation accuracy: 0.9800\n",
      "iteration number: 4011\t training loss: 0.0387\tvalidation loss: 0.0635\t validation accuracy: 0.9800\n",
      "iteration number: 4012\t training loss: 0.0384\tvalidation loss: 0.0633\t validation accuracy: 0.9800\n",
      "iteration number: 4013\t training loss: 0.0384\tvalidation loss: 0.0633\t validation accuracy: 0.9800\n",
      "iteration number: 4014\t training loss: 0.0381\tvalidation loss: 0.0621\t validation accuracy: 0.9822\n",
      "iteration number: 4015\t training loss: 0.0385\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 4016\t training loss: 0.0387\tvalidation loss: 0.0632\t validation accuracy: 0.9822\n",
      "iteration number: 4017\t training loss: 0.0388\tvalidation loss: 0.0644\t validation accuracy: 0.9800\n",
      "iteration number: 4018\t training loss: 0.0386\tvalidation loss: 0.0637\t validation accuracy: 0.9822\n",
      "iteration number: 4019\t training loss: 0.0385\tvalidation loss: 0.0632\t validation accuracy: 0.9822\n",
      "iteration number: 4020\t training loss: 0.0388\tvalidation loss: 0.0637\t validation accuracy: 0.9822\n",
      "iteration number: 4021\t training loss: 0.0386\tvalidation loss: 0.0636\t validation accuracy: 0.9800\n",
      "iteration number: 4022\t training loss: 0.0383\tvalidation loss: 0.0639\t validation accuracy: 0.9800\n",
      "iteration number: 4023\t training loss: 0.0381\tvalidation loss: 0.0640\t validation accuracy: 0.9800\n",
      "iteration number: 4024\t training loss: 0.0383\tvalidation loss: 0.0645\t validation accuracy: 0.9800\n",
      "iteration number: 4025\t training loss: 0.0383\tvalidation loss: 0.0646\t validation accuracy: 0.9800\n",
      "iteration number: 4026\t training loss: 0.0395\tvalidation loss: 0.0672\t validation accuracy: 0.9733\n",
      "iteration number: 4027\t training loss: 0.0392\tvalidation loss: 0.0665\t validation accuracy: 0.9756\n",
      "iteration number: 4028\t training loss: 0.0380\tvalidation loss: 0.0636\t validation accuracy: 0.9800\n",
      "iteration number: 4029\t training loss: 0.0380\tvalidation loss: 0.0632\t validation accuracy: 0.9822\n",
      "iteration number: 4030\t training loss: 0.0382\tvalidation loss: 0.0635\t validation accuracy: 0.9778\n",
      "iteration number: 4031\t training loss: 0.0382\tvalidation loss: 0.0636\t validation accuracy: 0.9822\n",
      "iteration number: 4032\t training loss: 0.0382\tvalidation loss: 0.0636\t validation accuracy: 0.9822\n",
      "iteration number: 4033\t training loss: 0.0383\tvalidation loss: 0.0638\t validation accuracy: 0.9822\n",
      "iteration number: 4034\t training loss: 0.0381\tvalidation loss: 0.0635\t validation accuracy: 0.9800\n",
      "iteration number: 4035\t training loss: 0.0380\tvalidation loss: 0.0631\t validation accuracy: 0.9822\n",
      "iteration number: 4036\t training loss: 0.0384\tvalidation loss: 0.0643\t validation accuracy: 0.9778\n",
      "iteration number: 4037\t training loss: 0.0380\tvalidation loss: 0.0638\t validation accuracy: 0.9800\n",
      "iteration number: 4038\t training loss: 0.0378\tvalidation loss: 0.0625\t validation accuracy: 0.9822\n",
      "iteration number: 4039\t training loss: 0.0380\tvalidation loss: 0.0624\t validation accuracy: 0.9822\n",
      "iteration number: 4040\t training loss: 0.0380\tvalidation loss: 0.0621\t validation accuracy: 0.9822\n",
      "iteration number: 4041\t training loss: 0.0384\tvalidation loss: 0.0624\t validation accuracy: 0.9822\n",
      "iteration number: 4042\t training loss: 0.0381\tvalidation loss: 0.0625\t validation accuracy: 0.9822\n",
      "iteration number: 4043\t training loss: 0.0377\tvalidation loss: 0.0621\t validation accuracy: 0.9822\n",
      "iteration number: 4044\t training loss: 0.0375\tvalidation loss: 0.0617\t validation accuracy: 0.9822\n",
      "iteration number: 4045\t training loss: 0.0378\tvalidation loss: 0.0618\t validation accuracy: 0.9822\n",
      "iteration number: 4046\t training loss: 0.0379\tvalidation loss: 0.0620\t validation accuracy: 0.9822\n",
      "iteration number: 4047\t training loss: 0.0383\tvalidation loss: 0.0619\t validation accuracy: 0.9822\n",
      "iteration number: 4048\t training loss: 0.0383\tvalidation loss: 0.0625\t validation accuracy: 0.9800\n",
      "iteration number: 4049\t training loss: 0.0381\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 4050\t training loss: 0.0380\tvalidation loss: 0.0632\t validation accuracy: 0.9800\n",
      "iteration number: 4051\t training loss: 0.0380\tvalidation loss: 0.0633\t validation accuracy: 0.9822\n",
      "iteration number: 4052\t training loss: 0.0380\tvalidation loss: 0.0629\t validation accuracy: 0.9800\n",
      "iteration number: 4053\t training loss: 0.0378\tvalidation loss: 0.0622\t validation accuracy: 0.9800\n",
      "iteration number: 4054\t training loss: 0.0380\tvalidation loss: 0.0625\t validation accuracy: 0.9800\n",
      "iteration number: 4055\t training loss: 0.0379\tvalidation loss: 0.0624\t validation accuracy: 0.9800\n",
      "iteration number: 4056\t training loss: 0.0381\tvalidation loss: 0.0621\t validation accuracy: 0.9800\n",
      "iteration number: 4057\t training loss: 0.0382\tvalidation loss: 0.0623\t validation accuracy: 0.9800\n",
      "iteration number: 4058\t training loss: 0.0383\tvalidation loss: 0.0621\t validation accuracy: 0.9800\n",
      "iteration number: 4059\t training loss: 0.0382\tvalidation loss: 0.0626\t validation accuracy: 0.9800\n",
      "iteration number: 4060\t training loss: 0.0381\tvalidation loss: 0.0628\t validation accuracy: 0.9800\n",
      "iteration number: 4061\t training loss: 0.0384\tvalidation loss: 0.0620\t validation accuracy: 0.9800\n",
      "iteration number: 4062\t training loss: 0.0383\tvalidation loss: 0.0623\t validation accuracy: 0.9822\n",
      "iteration number: 4063\t training loss: 0.0387\tvalidation loss: 0.0633\t validation accuracy: 0.9822\n",
      "iteration number: 4064\t training loss: 0.0384\tvalidation loss: 0.0625\t validation accuracy: 0.9822\n",
      "iteration number: 4065\t training loss: 0.0385\tvalidation loss: 0.0627\t validation accuracy: 0.9822\n",
      "iteration number: 4066\t training loss: 0.0394\tvalidation loss: 0.0645\t validation accuracy: 0.9822\n",
      "iteration number: 4067\t training loss: 0.0390\tvalidation loss: 0.0637\t validation accuracy: 0.9822\n",
      "iteration number: 4068\t training loss: 0.0398\tvalidation loss: 0.0650\t validation accuracy: 0.9822\n",
      "iteration number: 4069\t training loss: 0.0391\tvalidation loss: 0.0643\t validation accuracy: 0.9822\n",
      "iteration number: 4070\t training loss: 0.0387\tvalidation loss: 0.0639\t validation accuracy: 0.9800\n",
      "iteration number: 4071\t training loss: 0.0384\tvalidation loss: 0.0639\t validation accuracy: 0.9822\n",
      "iteration number: 4072\t training loss: 0.0380\tvalidation loss: 0.0635\t validation accuracy: 0.9822\n",
      "iteration number: 4073\t training loss: 0.0372\tvalidation loss: 0.0620\t validation accuracy: 0.9800\n",
      "iteration number: 4074\t training loss: 0.0374\tvalidation loss: 0.0625\t validation accuracy: 0.9800\n",
      "iteration number: 4075\t training loss: 0.0371\tvalidation loss: 0.0621\t validation accuracy: 0.9778\n",
      "iteration number: 4076\t training loss: 0.0381\tvalidation loss: 0.0644\t validation accuracy: 0.9800\n",
      "iteration number: 4077\t training loss: 0.0377\tvalidation loss: 0.0634\t validation accuracy: 0.9800\n",
      "iteration number: 4078\t training loss: 0.0380\tvalidation loss: 0.0638\t validation accuracy: 0.9800\n",
      "iteration number: 4079\t training loss: 0.0381\tvalidation loss: 0.0642\t validation accuracy: 0.9778\n",
      "iteration number: 4080\t training loss: 0.0372\tvalidation loss: 0.0622\t validation accuracy: 0.9800\n",
      "iteration number: 4081\t training loss: 0.0373\tvalidation loss: 0.0619\t validation accuracy: 0.9800\n",
      "iteration number: 4082\t training loss: 0.0372\tvalidation loss: 0.0613\t validation accuracy: 0.9844\n",
      "iteration number: 4083\t training loss: 0.0371\tvalidation loss: 0.0608\t validation accuracy: 0.9844\n",
      "iteration number: 4084\t training loss: 0.0370\tvalidation loss: 0.0610\t validation accuracy: 0.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4085\t training loss: 0.0372\tvalidation loss: 0.0610\t validation accuracy: 0.9822\n",
      "iteration number: 4086\t training loss: 0.0375\tvalidation loss: 0.0615\t validation accuracy: 0.9822\n",
      "iteration number: 4087\t training loss: 0.0381\tvalidation loss: 0.0622\t validation accuracy: 0.9800\n",
      "iteration number: 4088\t training loss: 0.0375\tvalidation loss: 0.0617\t validation accuracy: 0.9800\n",
      "iteration number: 4089\t training loss: 0.0392\tvalidation loss: 0.0628\t validation accuracy: 0.9800\n",
      "iteration number: 4090\t training loss: 0.0387\tvalidation loss: 0.0629\t validation accuracy: 0.9778\n",
      "iteration number: 4091\t training loss: 0.0385\tvalidation loss: 0.0626\t validation accuracy: 0.9822\n",
      "iteration number: 4092\t training loss: 0.0388\tvalidation loss: 0.0626\t validation accuracy: 0.9822\n",
      "iteration number: 4093\t training loss: 0.0379\tvalidation loss: 0.0625\t validation accuracy: 0.9822\n",
      "iteration number: 4094\t training loss: 0.0381\tvalidation loss: 0.0624\t validation accuracy: 0.9822\n",
      "iteration number: 4095\t training loss: 0.0377\tvalidation loss: 0.0631\t validation accuracy: 0.9778\n",
      "iteration number: 4096\t training loss: 0.0387\tvalidation loss: 0.0630\t validation accuracy: 0.9778\n",
      "iteration number: 4097\t training loss: 0.0378\tvalidation loss: 0.0620\t validation accuracy: 0.9800\n",
      "iteration number: 4098\t training loss: 0.0374\tvalidation loss: 0.0615\t validation accuracy: 0.9822\n",
      "iteration number: 4099\t training loss: 0.0374\tvalidation loss: 0.0615\t validation accuracy: 0.9822\n",
      "iteration number: 4100\t training loss: 0.0375\tvalidation loss: 0.0624\t validation accuracy: 0.9822\n",
      "iteration number: 4101\t training loss: 0.0374\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 4102\t training loss: 0.0375\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 4103\t training loss: 0.0375\tvalidation loss: 0.0625\t validation accuracy: 0.9822\n",
      "iteration number: 4104\t training loss: 0.0378\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 4105\t training loss: 0.0372\tvalidation loss: 0.0629\t validation accuracy: 0.9822\n",
      "iteration number: 4106\t training loss: 0.0375\tvalidation loss: 0.0628\t validation accuracy: 0.9822\n",
      "iteration number: 4107\t training loss: 0.0371\tvalidation loss: 0.0621\t validation accuracy: 0.9822\n",
      "iteration number: 4108\t training loss: 0.0380\tvalidation loss: 0.0622\t validation accuracy: 0.9822\n",
      "iteration number: 4109\t training loss: 0.0373\tvalidation loss: 0.0614\t validation accuracy: 0.9822\n",
      "iteration number: 4110\t training loss: 0.0371\tvalidation loss: 0.0612\t validation accuracy: 0.9822\n",
      "iteration number: 4111\t training loss: 0.0382\tvalidation loss: 0.0614\t validation accuracy: 0.9822\n",
      "iteration number: 4112\t training loss: 0.0375\tvalidation loss: 0.0625\t validation accuracy: 0.9800\n",
      "iteration number: 4113\t training loss: 0.0371\tvalidation loss: 0.0623\t validation accuracy: 0.9800\n",
      "iteration number: 4114\t training loss: 0.0367\tvalidation loss: 0.0619\t validation accuracy: 0.9800\n",
      "iteration number: 4115\t training loss: 0.0369\tvalidation loss: 0.0620\t validation accuracy: 0.9778\n",
      "iteration number: 4116\t training loss: 0.0367\tvalidation loss: 0.0617\t validation accuracy: 0.9800\n",
      "iteration number: 4117\t training loss: 0.0365\tvalidation loss: 0.0614\t validation accuracy: 0.9800\n",
      "iteration number: 4118\t training loss: 0.0369\tvalidation loss: 0.0625\t validation accuracy: 0.9800\n",
      "iteration number: 4119\t training loss: 0.0369\tvalidation loss: 0.0616\t validation accuracy: 0.9800\n",
      "iteration number: 4120\t training loss: 0.0368\tvalidation loss: 0.0615\t validation accuracy: 0.9800\n",
      "iteration number: 4121\t training loss: 0.0372\tvalidation loss: 0.0617\t validation accuracy: 0.9800\n",
      "iteration number: 4122\t training loss: 0.0368\tvalidation loss: 0.0619\t validation accuracy: 0.9800\n",
      "iteration number: 4123\t training loss: 0.0367\tvalidation loss: 0.0609\t validation accuracy: 0.9822\n",
      "iteration number: 4124\t training loss: 0.0367\tvalidation loss: 0.0608\t validation accuracy: 0.9822\n",
      "iteration number: 4125\t training loss: 0.0365\tvalidation loss: 0.0610\t validation accuracy: 0.9822\n",
      "iteration number: 4126\t training loss: 0.0367\tvalidation loss: 0.0618\t validation accuracy: 0.9800\n",
      "iteration number: 4127\t training loss: 0.0368\tvalidation loss: 0.0618\t validation accuracy: 0.9822\n",
      "iteration number: 4128\t training loss: 0.0366\tvalidation loss: 0.0620\t validation accuracy: 0.9800\n",
      "iteration number: 4129\t training loss: 0.0367\tvalidation loss: 0.0618\t validation accuracy: 0.9822\n",
      "iteration number: 4130\t training loss: 0.0368\tvalidation loss: 0.0620\t validation accuracy: 0.9800\n",
      "iteration number: 4131\t training loss: 0.0374\tvalidation loss: 0.0630\t validation accuracy: 0.9822\n",
      "iteration number: 4132\t training loss: 0.0372\tvalidation loss: 0.0629\t validation accuracy: 0.9800\n",
      "iteration number: 4133\t training loss: 0.0371\tvalidation loss: 0.0632\t validation accuracy: 0.9800\n",
      "iteration number: 4134\t training loss: 0.0366\tvalidation loss: 0.0616\t validation accuracy: 0.9822\n",
      "iteration number: 4135\t training loss: 0.0366\tvalidation loss: 0.0617\t validation accuracy: 0.9822\n",
      "iteration number: 4136\t training loss: 0.0365\tvalidation loss: 0.0624\t validation accuracy: 0.9822\n",
      "iteration number: 4137\t training loss: 0.0364\tvalidation loss: 0.0623\t validation accuracy: 0.9822\n",
      "iteration number: 4138\t training loss: 0.0364\tvalidation loss: 0.0623\t validation accuracy: 0.9800\n",
      "iteration number: 4139\t training loss: 0.0364\tvalidation loss: 0.0624\t validation accuracy: 0.9800\n",
      "iteration number: 4140\t training loss: 0.0365\tvalidation loss: 0.0622\t validation accuracy: 0.9822\n",
      "iteration number: 4141\t training loss: 0.0364\tvalidation loss: 0.0622\t validation accuracy: 0.9822\n",
      "iteration number: 4142\t training loss: 0.0362\tvalidation loss: 0.0618\t validation accuracy: 0.9800\n",
      "iteration number: 4143\t training loss: 0.0380\tvalidation loss: 0.0629\t validation accuracy: 0.9778\n",
      "iteration number: 4144\t training loss: 0.0376\tvalidation loss: 0.0620\t validation accuracy: 0.9822\n",
      "iteration number: 4145\t training loss: 0.0369\tvalidation loss: 0.0615\t validation accuracy: 0.9822\n",
      "iteration number: 4146\t training loss: 0.0366\tvalidation loss: 0.0613\t validation accuracy: 0.9822\n",
      "iteration number: 4147\t training loss: 0.0393\tvalidation loss: 0.0628\t validation accuracy: 0.9800\n",
      "iteration number: 4148\t training loss: 0.0367\tvalidation loss: 0.0616\t validation accuracy: 0.9800\n",
      "iteration number: 4149\t training loss: 0.0363\tvalidation loss: 0.0614\t validation accuracy: 0.9822\n",
      "iteration number: 4150\t training loss: 0.0367\tvalidation loss: 0.0620\t validation accuracy: 0.9822\n",
      "iteration number: 4151\t training loss: 0.0374\tvalidation loss: 0.0630\t validation accuracy: 0.9800\n",
      "iteration number: 4152\t training loss: 0.0373\tvalidation loss: 0.0630\t validation accuracy: 0.9800\n",
      "iteration number: 4153\t training loss: 0.0364\tvalidation loss: 0.0616\t validation accuracy: 0.9800\n",
      "iteration number: 4154\t training loss: 0.0372\tvalidation loss: 0.0621\t validation accuracy: 0.9800\n",
      "iteration number: 4155\t training loss: 0.0374\tvalidation loss: 0.0623\t validation accuracy: 0.9778\n",
      "iteration number: 4156\t training loss: 0.0374\tvalidation loss: 0.0625\t validation accuracy: 0.9756\n",
      "iteration number: 4157\t training loss: 0.0370\tvalidation loss: 0.0622\t validation accuracy: 0.9800\n",
      "iteration number: 4158\t training loss: 0.0374\tvalidation loss: 0.0625\t validation accuracy: 0.9778\n",
      "iteration number: 4159\t training loss: 0.0372\tvalidation loss: 0.0621\t validation accuracy: 0.9800\n",
      "iteration number: 4160\t training loss: 0.0367\tvalidation loss: 0.0619\t validation accuracy: 0.9800\n",
      "iteration number: 4161\t training loss: 0.0366\tvalidation loss: 0.0619\t validation accuracy: 0.9800\n",
      "iteration number: 4162\t training loss: 0.0365\tvalidation loss: 0.0608\t validation accuracy: 0.9822\n",
      "iteration number: 4163\t training loss: 0.0362\tvalidation loss: 0.0606\t validation accuracy: 0.9822\n",
      "iteration number: 4164\t training loss: 0.0368\tvalidation loss: 0.0605\t validation accuracy: 0.9822\n",
      "iteration number: 4165\t training loss: 0.0372\tvalidation loss: 0.0606\t validation accuracy: 0.9822\n",
      "iteration number: 4166\t training loss: 0.0371\tvalidation loss: 0.0608\t validation accuracy: 0.9844\n",
      "iteration number: 4167\t training loss: 0.0374\tvalidation loss: 0.0616\t validation accuracy: 0.9822\n",
      "iteration number: 4168\t training loss: 0.0379\tvalidation loss: 0.0615\t validation accuracy: 0.9822\n",
      "iteration number: 4169\t training loss: 0.0373\tvalidation loss: 0.0620\t validation accuracy: 0.9822\n",
      "iteration number: 4170\t training loss: 0.0363\tvalidation loss: 0.0617\t validation accuracy: 0.9844\n",
      "iteration number: 4171\t training loss: 0.0367\tvalidation loss: 0.0612\t validation accuracy: 0.9844\n",
      "iteration number: 4172\t training loss: 0.0373\tvalidation loss: 0.0612\t validation accuracy: 0.9822\n",
      "iteration number: 4173\t training loss: 0.0370\tvalidation loss: 0.0606\t validation accuracy: 0.9844\n",
      "iteration number: 4174\t training loss: 0.0376\tvalidation loss: 0.0614\t validation accuracy: 0.9844\n",
      "iteration number: 4175\t training loss: 0.0372\tvalidation loss: 0.0606\t validation accuracy: 0.9844\n",
      "iteration number: 4176\t training loss: 0.0368\tvalidation loss: 0.0604\t validation accuracy: 0.9844\n",
      "iteration number: 4177\t training loss: 0.0368\tvalidation loss: 0.0604\t validation accuracy: 0.9844\n",
      "iteration number: 4178\t training loss: 0.0366\tvalidation loss: 0.0604\t validation accuracy: 0.9867\n",
      "iteration number: 4179\t training loss: 0.0366\tvalidation loss: 0.0603\t validation accuracy: 0.9844\n",
      "iteration number: 4180\t training loss: 0.0366\tvalidation loss: 0.0604\t validation accuracy: 0.9867\n",
      "iteration number: 4181\t training loss: 0.0368\tvalidation loss: 0.0603\t validation accuracy: 0.9844\n",
      "iteration number: 4182\t training loss: 0.0362\tvalidation loss: 0.0605\t validation accuracy: 0.9867\n",
      "iteration number: 4183\t training loss: 0.0364\tvalidation loss: 0.0616\t validation accuracy: 0.9822\n",
      "iteration number: 4184\t training loss: 0.0365\tvalidation loss: 0.0616\t validation accuracy: 0.9822\n",
      "iteration number: 4185\t training loss: 0.0357\tvalidation loss: 0.0604\t validation accuracy: 0.9822\n",
      "iteration number: 4186\t training loss: 0.0361\tvalidation loss: 0.0623\t validation accuracy: 0.9800\n",
      "iteration number: 4187\t training loss: 0.0364\tvalidation loss: 0.0615\t validation accuracy: 0.9822\n",
      "iteration number: 4188\t training loss: 0.0359\tvalidation loss: 0.0612\t validation accuracy: 0.9800\n",
      "iteration number: 4189\t training loss: 0.0358\tvalidation loss: 0.0608\t validation accuracy: 0.9800\n",
      "iteration number: 4190\t training loss: 0.0361\tvalidation loss: 0.0602\t validation accuracy: 0.9800\n",
      "iteration number: 4191\t training loss: 0.0358\tvalidation loss: 0.0600\t validation accuracy: 0.9822\n",
      "iteration number: 4192\t training loss: 0.0359\tvalidation loss: 0.0609\t validation accuracy: 0.9822\n",
      "iteration number: 4193\t training loss: 0.0358\tvalidation loss: 0.0608\t validation accuracy: 0.9800\n",
      "iteration number: 4194\t training loss: 0.0362\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4195\t training loss: 0.0359\tvalidation loss: 0.0603\t validation accuracy: 0.9822\n",
      "iteration number: 4196\t training loss: 0.0357\tvalidation loss: 0.0604\t validation accuracy: 0.9822\n",
      "iteration number: 4197\t training loss: 0.0359\tvalidation loss: 0.0610\t validation accuracy: 0.9800\n",
      "iteration number: 4198\t training loss: 0.0356\tvalidation loss: 0.0600\t validation accuracy: 0.9800\n",
      "iteration number: 4199\t training loss: 0.0355\tvalidation loss: 0.0600\t validation accuracy: 0.9800\n",
      "iteration number: 4200\t training loss: 0.0356\tvalidation loss: 0.0600\t validation accuracy: 0.9822\n",
      "iteration number: 4201\t training loss: 0.0361\tvalidation loss: 0.0616\t validation accuracy: 0.9800\n",
      "iteration number: 4202\t training loss: 0.0360\tvalidation loss: 0.0619\t validation accuracy: 0.9800\n",
      "iteration number: 4203\t training loss: 0.0358\tvalidation loss: 0.0615\t validation accuracy: 0.9822\n",
      "iteration number: 4204\t training loss: 0.0359\tvalidation loss: 0.0620\t validation accuracy: 0.9822\n",
      "iteration number: 4205\t training loss: 0.0356\tvalidation loss: 0.0611\t validation accuracy: 0.9822\n",
      "iteration number: 4206\t training loss: 0.0356\tvalidation loss: 0.0611\t validation accuracy: 0.9822\n",
      "iteration number: 4207\t training loss: 0.0358\tvalidation loss: 0.0613\t validation accuracy: 0.9822\n",
      "iteration number: 4208\t training loss: 0.0363\tvalidation loss: 0.0614\t validation accuracy: 0.9822\n",
      "iteration number: 4209\t training loss: 0.0356\tvalidation loss: 0.0605\t validation accuracy: 0.9800\n",
      "iteration number: 4210\t training loss: 0.0356\tvalidation loss: 0.0602\t validation accuracy: 0.9800\n",
      "iteration number: 4211\t training loss: 0.0358\tvalidation loss: 0.0605\t validation accuracy: 0.9822\n",
      "iteration number: 4212\t training loss: 0.0365\tvalidation loss: 0.0611\t validation accuracy: 0.9822\n",
      "iteration number: 4213\t training loss: 0.0374\tvalidation loss: 0.0608\t validation accuracy: 0.9822\n",
      "iteration number: 4214\t training loss: 0.0362\tvalidation loss: 0.0605\t validation accuracy: 0.9800\n",
      "iteration number: 4215\t training loss: 0.0357\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4216\t training loss: 0.0357\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4217\t training loss: 0.0358\tvalidation loss: 0.0605\t validation accuracy: 0.9800\n",
      "iteration number: 4218\t training loss: 0.0366\tvalidation loss: 0.0618\t validation accuracy: 0.9844\n",
      "iteration number: 4219\t training loss: 0.0361\tvalidation loss: 0.0617\t validation accuracy: 0.9844\n",
      "iteration number: 4220\t training loss: 0.0364\tvalidation loss: 0.0616\t validation accuracy: 0.9822\n",
      "iteration number: 4221\t training loss: 0.0359\tvalidation loss: 0.0612\t validation accuracy: 0.9822\n",
      "iteration number: 4222\t training loss: 0.0363\tvalidation loss: 0.0614\t validation accuracy: 0.9822\n",
      "iteration number: 4223\t training loss: 0.0361\tvalidation loss: 0.0618\t validation accuracy: 0.9822\n",
      "iteration number: 4224\t training loss: 0.0363\tvalidation loss: 0.0624\t validation accuracy: 0.9800\n",
      "iteration number: 4225\t training loss: 0.0356\tvalidation loss: 0.0614\t validation accuracy: 0.9800\n",
      "iteration number: 4226\t training loss: 0.0353\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4227\t training loss: 0.0355\tvalidation loss: 0.0603\t validation accuracy: 0.9822\n",
      "iteration number: 4228\t training loss: 0.0353\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4229\t training loss: 0.0358\tvalidation loss: 0.0605\t validation accuracy: 0.9800\n",
      "iteration number: 4230\t training loss: 0.0358\tvalidation loss: 0.0601\t validation accuracy: 0.9844\n",
      "iteration number: 4231\t training loss: 0.0356\tvalidation loss: 0.0599\t validation accuracy: 0.9844\n",
      "iteration number: 4232\t training loss: 0.0361\tvalidation loss: 0.0606\t validation accuracy: 0.9822\n",
      "iteration number: 4233\t training loss: 0.0358\tvalidation loss: 0.0600\t validation accuracy: 0.9822\n",
      "iteration number: 4234\t training loss: 0.0361\tvalidation loss: 0.0619\t validation accuracy: 0.9800\n",
      "iteration number: 4235\t training loss: 0.0360\tvalidation loss: 0.0613\t validation accuracy: 0.9800\n",
      "iteration number: 4236\t training loss: 0.0365\tvalidation loss: 0.0616\t validation accuracy: 0.9822\n",
      "iteration number: 4237\t training loss: 0.0359\tvalidation loss: 0.0611\t validation accuracy: 0.9822\n",
      "iteration number: 4238\t training loss: 0.0357\tvalidation loss: 0.0612\t validation accuracy: 0.9822\n",
      "iteration number: 4239\t training loss: 0.0357\tvalidation loss: 0.0610\t validation accuracy: 0.9822\n",
      "iteration number: 4240\t training loss: 0.0365\tvalidation loss: 0.0620\t validation accuracy: 0.9822\n",
      "iteration number: 4241\t training loss: 0.0362\tvalidation loss: 0.0620\t validation accuracy: 0.9822\n",
      "iteration number: 4242\t training loss: 0.0357\tvalidation loss: 0.0616\t validation accuracy: 0.9800\n",
      "iteration number: 4243\t training loss: 0.0357\tvalidation loss: 0.0612\t validation accuracy: 0.9800\n",
      "iteration number: 4244\t training loss: 0.0353\tvalidation loss: 0.0603\t validation accuracy: 0.9800\n",
      "iteration number: 4245\t training loss: 0.0357\tvalidation loss: 0.0603\t validation accuracy: 0.9822\n",
      "iteration number: 4246\t training loss: 0.0362\tvalidation loss: 0.0613\t validation accuracy: 0.9800\n",
      "iteration number: 4247\t training loss: 0.0358\tvalidation loss: 0.0605\t validation accuracy: 0.9822\n",
      "iteration number: 4248\t training loss: 0.0360\tvalidation loss: 0.0613\t validation accuracy: 0.9822\n",
      "iteration number: 4249\t training loss: 0.0356\tvalidation loss: 0.0604\t validation accuracy: 0.9844\n",
      "iteration number: 4250\t training loss: 0.0362\tvalidation loss: 0.0617\t validation accuracy: 0.9800\n",
      "iteration number: 4251\t training loss: 0.0358\tvalidation loss: 0.0615\t validation accuracy: 0.9800\n",
      "iteration number: 4252\t training loss: 0.0362\tvalidation loss: 0.0620\t validation accuracy: 0.9822\n",
      "iteration number: 4253\t training loss: 0.0362\tvalidation loss: 0.0621\t validation accuracy: 0.9822\n",
      "iteration number: 4254\t training loss: 0.0364\tvalidation loss: 0.0623\t validation accuracy: 0.9822\n",
      "iteration number: 4255\t training loss: 0.0361\tvalidation loss: 0.0612\t validation accuracy: 0.9822\n",
      "iteration number: 4256\t training loss: 0.0359\tvalidation loss: 0.0616\t validation accuracy: 0.9822\n",
      "iteration number: 4257\t training loss: 0.0354\tvalidation loss: 0.0605\t validation accuracy: 0.9822\n",
      "iteration number: 4258\t training loss: 0.0359\tvalidation loss: 0.0604\t validation accuracy: 0.9822\n",
      "iteration number: 4259\t training loss: 0.0362\tvalidation loss: 0.0606\t validation accuracy: 0.9800\n",
      "iteration number: 4260\t training loss: 0.0356\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4261\t training loss: 0.0351\tvalidation loss: 0.0605\t validation accuracy: 0.9822\n",
      "iteration number: 4262\t training loss: 0.0351\tvalidation loss: 0.0602\t validation accuracy: 0.9822\n",
      "iteration number: 4263\t training loss: 0.0352\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4264\t training loss: 0.0353\tvalidation loss: 0.0610\t validation accuracy: 0.9800\n",
      "iteration number: 4265\t training loss: 0.0349\tvalidation loss: 0.0600\t validation accuracy: 0.9800\n",
      "iteration number: 4266\t training loss: 0.0349\tvalidation loss: 0.0602\t validation accuracy: 0.9800\n",
      "iteration number: 4267\t training loss: 0.0349\tvalidation loss: 0.0601\t validation accuracy: 0.9800\n",
      "iteration number: 4268\t training loss: 0.0350\tvalidation loss: 0.0602\t validation accuracy: 0.9800\n",
      "iteration number: 4269\t training loss: 0.0350\tvalidation loss: 0.0609\t validation accuracy: 0.9800\n",
      "iteration number: 4270\t training loss: 0.0352\tvalidation loss: 0.0606\t validation accuracy: 0.9822\n",
      "iteration number: 4271\t training loss: 0.0350\tvalidation loss: 0.0598\t validation accuracy: 0.9822\n",
      "iteration number: 4272\t training loss: 0.0355\tvalidation loss: 0.0602\t validation accuracy: 0.9822\n",
      "iteration number: 4273\t training loss: 0.0358\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4274\t training loss: 0.0357\tvalidation loss: 0.0597\t validation accuracy: 0.9822\n",
      "iteration number: 4275\t training loss: 0.0357\tvalidation loss: 0.0597\t validation accuracy: 0.9822\n",
      "iteration number: 4276\t training loss: 0.0358\tvalidation loss: 0.0593\t validation accuracy: 0.9822\n",
      "iteration number: 4277\t training loss: 0.0354\tvalidation loss: 0.0589\t validation accuracy: 0.9844\n",
      "iteration number: 4278\t training loss: 0.0358\tvalidation loss: 0.0589\t validation accuracy: 0.9844\n",
      "iteration number: 4279\t training loss: 0.0359\tvalidation loss: 0.0588\t validation accuracy: 0.9844\n",
      "iteration number: 4280\t training loss: 0.0358\tvalidation loss: 0.0589\t validation accuracy: 0.9844\n",
      "iteration number: 4281\t training loss: 0.0358\tvalidation loss: 0.0590\t validation accuracy: 0.9844\n",
      "iteration number: 4282\t training loss: 0.0352\tvalidation loss: 0.0596\t validation accuracy: 0.9844\n",
      "iteration number: 4283\t training loss: 0.0352\tvalidation loss: 0.0594\t validation accuracy: 0.9844\n",
      "iteration number: 4284\t training loss: 0.0358\tvalidation loss: 0.0594\t validation accuracy: 0.9822\n",
      "iteration number: 4285\t training loss: 0.0363\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4286\t training loss: 0.0371\tvalidation loss: 0.0609\t validation accuracy: 0.9822\n",
      "iteration number: 4287\t training loss: 0.0355\tvalidation loss: 0.0595\t validation accuracy: 0.9844\n",
      "iteration number: 4288\t training loss: 0.0353\tvalidation loss: 0.0591\t validation accuracy: 0.9844\n",
      "iteration number: 4289\t training loss: 0.0351\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4290\t training loss: 0.0352\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4291\t training loss: 0.0347\tvalidation loss: 0.0603\t validation accuracy: 0.9800\n",
      "iteration number: 4292\t training loss: 0.0347\tvalidation loss: 0.0603\t validation accuracy: 0.9800\n",
      "iteration number: 4293\t training loss: 0.0350\tvalidation loss: 0.0606\t validation accuracy: 0.9822\n",
      "iteration number: 4294\t training loss: 0.0350\tvalidation loss: 0.0605\t validation accuracy: 0.9822\n",
      "iteration number: 4295\t training loss: 0.0349\tvalidation loss: 0.0600\t validation accuracy: 0.9822\n",
      "iteration number: 4296\t training loss: 0.0353\tvalidation loss: 0.0605\t validation accuracy: 0.9822\n",
      "iteration number: 4297\t training loss: 0.0354\tvalidation loss: 0.0615\t validation accuracy: 0.9800\n",
      "iteration number: 4298\t training loss: 0.0350\tvalidation loss: 0.0603\t validation accuracy: 0.9800\n",
      "iteration number: 4299\t training loss: 0.0348\tvalidation loss: 0.0591\t validation accuracy: 0.9800\n",
      "iteration number: 4300\t training loss: 0.0347\tvalidation loss: 0.0592\t validation accuracy: 0.9800\n",
      "iteration number: 4301\t training loss: 0.0349\tvalidation loss: 0.0597\t validation accuracy: 0.9822\n",
      "iteration number: 4302\t training loss: 0.0350\tvalidation loss: 0.0594\t validation accuracy: 0.9822\n",
      "iteration number: 4303\t training loss: 0.0350\tvalidation loss: 0.0591\t validation accuracy: 0.9822\n",
      "iteration number: 4304\t training loss: 0.0354\tvalidation loss: 0.0600\t validation accuracy: 0.9822\n",
      "iteration number: 4305\t training loss: 0.0356\tvalidation loss: 0.0598\t validation accuracy: 0.9822\n",
      "iteration number: 4306\t training loss: 0.0353\tvalidation loss: 0.0589\t validation accuracy: 0.9844\n",
      "iteration number: 4307\t training loss: 0.0355\tvalidation loss: 0.0589\t validation accuracy: 0.9844\n",
      "iteration number: 4308\t training loss: 0.0360\tvalidation loss: 0.0590\t validation accuracy: 0.9844\n",
      "iteration number: 4309\t training loss: 0.0358\tvalidation loss: 0.0595\t validation accuracy: 0.9844\n",
      "iteration number: 4310\t training loss: 0.0363\tvalidation loss: 0.0590\t validation accuracy: 0.9844\n",
      "iteration number: 4311\t training loss: 0.0350\tvalidation loss: 0.0593\t validation accuracy: 0.9844\n",
      "iteration number: 4312\t training loss: 0.0348\tvalidation loss: 0.0595\t validation accuracy: 0.9844\n",
      "iteration number: 4313\t training loss: 0.0348\tvalidation loss: 0.0601\t validation accuracy: 0.9844\n",
      "iteration number: 4314\t training loss: 0.0348\tvalidation loss: 0.0603\t validation accuracy: 0.9822\n",
      "iteration number: 4315\t training loss: 0.0345\tvalidation loss: 0.0602\t validation accuracy: 0.9822\n",
      "iteration number: 4316\t training loss: 0.0349\tvalidation loss: 0.0604\t validation accuracy: 0.9822\n",
      "iteration number: 4317\t training loss: 0.0355\tvalidation loss: 0.0622\t validation accuracy: 0.9822\n",
      "iteration number: 4318\t training loss: 0.0358\tvalidation loss: 0.0624\t validation accuracy: 0.9800\n",
      "iteration number: 4319\t training loss: 0.0347\tvalidation loss: 0.0610\t validation accuracy: 0.9822\n",
      "iteration number: 4320\t training loss: 0.0346\tvalidation loss: 0.0609\t validation accuracy: 0.9822\n",
      "iteration number: 4321\t training loss: 0.0345\tvalidation loss: 0.0603\t validation accuracy: 0.9822\n",
      "iteration number: 4322\t training loss: 0.0344\tvalidation loss: 0.0605\t validation accuracy: 0.9822\n",
      "iteration number: 4323\t training loss: 0.0344\tvalidation loss: 0.0602\t validation accuracy: 0.9822\n",
      "iteration number: 4324\t training loss: 0.0349\tvalidation loss: 0.0618\t validation accuracy: 0.9822\n",
      "iteration number: 4325\t training loss: 0.0361\tvalidation loss: 0.0638\t validation accuracy: 0.9822\n",
      "iteration number: 4326\t training loss: 0.0354\tvalidation loss: 0.0625\t validation accuracy: 0.9822\n",
      "iteration number: 4327\t training loss: 0.0353\tvalidation loss: 0.0624\t validation accuracy: 0.9822\n",
      "iteration number: 4328\t training loss: 0.0351\tvalidation loss: 0.0625\t validation accuracy: 0.9822\n",
      "iteration number: 4329\t training loss: 0.0347\tvalidation loss: 0.0613\t validation accuracy: 0.9822\n",
      "iteration number: 4330\t training loss: 0.0347\tvalidation loss: 0.0617\t validation accuracy: 0.9844\n",
      "iteration number: 4331\t training loss: 0.0348\tvalidation loss: 0.0615\t validation accuracy: 0.9822\n",
      "iteration number: 4332\t training loss: 0.0346\tvalidation loss: 0.0608\t validation accuracy: 0.9844\n",
      "iteration number: 4333\t training loss: 0.0361\tvalidation loss: 0.0640\t validation accuracy: 0.9822\n",
      "iteration number: 4334\t training loss: 0.0359\tvalidation loss: 0.0635\t validation accuracy: 0.9822\n",
      "iteration number: 4335\t training loss: 0.0358\tvalidation loss: 0.0628\t validation accuracy: 0.9800\n",
      "iteration number: 4336\t training loss: 0.0355\tvalidation loss: 0.0625\t validation accuracy: 0.9844\n",
      "iteration number: 4337\t training loss: 0.0353\tvalidation loss: 0.0626\t validation accuracy: 0.9844\n",
      "iteration number: 4338\t training loss: 0.0349\tvalidation loss: 0.0618\t validation accuracy: 0.9844\n",
      "iteration number: 4339\t training loss: 0.0347\tvalidation loss: 0.0619\t validation accuracy: 0.9844\n",
      "iteration number: 4340\t training loss: 0.0348\tvalidation loss: 0.0621\t validation accuracy: 0.9844\n",
      "iteration number: 4341\t training loss: 0.0346\tvalidation loss: 0.0608\t validation accuracy: 0.9844\n",
      "iteration number: 4342\t training loss: 0.0346\tvalidation loss: 0.0606\t validation accuracy: 0.9867\n",
      "iteration number: 4343\t training loss: 0.0346\tvalidation loss: 0.0608\t validation accuracy: 0.9844\n",
      "iteration number: 4344\t training loss: 0.0355\tvalidation loss: 0.0626\t validation accuracy: 0.9844\n",
      "iteration number: 4345\t training loss: 0.0349\tvalidation loss: 0.0622\t validation accuracy: 0.9844\n",
      "iteration number: 4346\t training loss: 0.0348\tvalidation loss: 0.0621\t validation accuracy: 0.9844\n",
      "iteration number: 4347\t training loss: 0.0349\tvalidation loss: 0.0618\t validation accuracy: 0.9844\n",
      "iteration number: 4348\t training loss: 0.0349\tvalidation loss: 0.0618\t validation accuracy: 0.9844\n",
      "iteration number: 4349\t training loss: 0.0346\tvalidation loss: 0.0613\t validation accuracy: 0.9844\n",
      "iteration number: 4350\t training loss: 0.0349\tvalidation loss: 0.0620\t validation accuracy: 0.9844\n",
      "iteration number: 4351\t training loss: 0.0343\tvalidation loss: 0.0614\t validation accuracy: 0.9822\n",
      "iteration number: 4352\t training loss: 0.0355\tvalidation loss: 0.0623\t validation accuracy: 0.9822\n",
      "iteration number: 4353\t training loss: 0.0348\tvalidation loss: 0.0617\t validation accuracy: 0.9867\n",
      "iteration number: 4354\t training loss: 0.0344\tvalidation loss: 0.0610\t validation accuracy: 0.9844\n",
      "iteration number: 4355\t training loss: 0.0342\tvalidation loss: 0.0612\t validation accuracy: 0.9844\n",
      "iteration number: 4356\t training loss: 0.0344\tvalidation loss: 0.0614\t validation accuracy: 0.9844\n",
      "iteration number: 4357\t training loss: 0.0344\tvalidation loss: 0.0614\t validation accuracy: 0.9844\n",
      "iteration number: 4358\t training loss: 0.0344\tvalidation loss: 0.0613\t validation accuracy: 0.9844\n",
      "iteration number: 4359\t training loss: 0.0347\tvalidation loss: 0.0623\t validation accuracy: 0.9822\n",
      "iteration number: 4360\t training loss: 0.0342\tvalidation loss: 0.0612\t validation accuracy: 0.9822\n",
      "iteration number: 4361\t training loss: 0.0340\tvalidation loss: 0.0606\t validation accuracy: 0.9822\n",
      "iteration number: 4362\t training loss: 0.0343\tvalidation loss: 0.0613\t validation accuracy: 0.9822\n",
      "iteration number: 4363\t training loss: 0.0353\tvalidation loss: 0.0632\t validation accuracy: 0.9800\n",
      "iteration number: 4364\t training loss: 0.0352\tvalidation loss: 0.0630\t validation accuracy: 0.9822\n",
      "iteration number: 4365\t training loss: 0.0343\tvalidation loss: 0.0607\t validation accuracy: 0.9844\n",
      "iteration number: 4366\t training loss: 0.0359\tvalidation loss: 0.0639\t validation accuracy: 0.9778\n",
      "iteration number: 4367\t training loss: 0.0349\tvalidation loss: 0.0623\t validation accuracy: 0.9844\n",
      "iteration number: 4368\t training loss: 0.0348\tvalidation loss: 0.0622\t validation accuracy: 0.9822\n",
      "iteration number: 4369\t training loss: 0.0348\tvalidation loss: 0.0618\t validation accuracy: 0.9822\n",
      "iteration number: 4370\t training loss: 0.0344\tvalidation loss: 0.0609\t validation accuracy: 0.9822\n",
      "iteration number: 4371\t training loss: 0.0346\tvalidation loss: 0.0614\t validation accuracy: 0.9822\n",
      "iteration number: 4372\t training loss: 0.0344\tvalidation loss: 0.0608\t validation accuracy: 0.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4373\t training loss: 0.0344\tvalidation loss: 0.0608\t validation accuracy: 0.9822\n",
      "iteration number: 4374\t training loss: 0.0342\tvalidation loss: 0.0600\t validation accuracy: 0.9800\n",
      "iteration number: 4375\t training loss: 0.0342\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4376\t training loss: 0.0343\tvalidation loss: 0.0598\t validation accuracy: 0.9778\n",
      "iteration number: 4377\t training loss: 0.0347\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4378\t training loss: 0.0344\tvalidation loss: 0.0603\t validation accuracy: 0.9800\n",
      "iteration number: 4379\t training loss: 0.0349\tvalidation loss: 0.0619\t validation accuracy: 0.9800\n",
      "iteration number: 4380\t training loss: 0.0348\tvalidation loss: 0.0618\t validation accuracy: 0.9822\n",
      "iteration number: 4381\t training loss: 0.0347\tvalidation loss: 0.0619\t validation accuracy: 0.9844\n",
      "iteration number: 4382\t training loss: 0.0351\tvalidation loss: 0.0627\t validation accuracy: 0.9800\n",
      "iteration number: 4383\t training loss: 0.0346\tvalidation loss: 0.0613\t validation accuracy: 0.9800\n",
      "iteration number: 4384\t training loss: 0.0347\tvalidation loss: 0.0614\t validation accuracy: 0.9844\n",
      "iteration number: 4385\t training loss: 0.0337\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4386\t training loss: 0.0340\tvalidation loss: 0.0600\t validation accuracy: 0.9822\n",
      "iteration number: 4387\t training loss: 0.0341\tvalidation loss: 0.0597\t validation accuracy: 0.9822\n",
      "iteration number: 4388\t training loss: 0.0339\tvalidation loss: 0.0602\t validation accuracy: 0.9822\n",
      "iteration number: 4389\t training loss: 0.0348\tvalidation loss: 0.0611\t validation accuracy: 0.9800\n",
      "iteration number: 4390\t training loss: 0.0352\tvalidation loss: 0.0619\t validation accuracy: 0.9778\n",
      "iteration number: 4391\t training loss: 0.0349\tvalidation loss: 0.0609\t validation accuracy: 0.9822\n",
      "iteration number: 4392\t training loss: 0.0348\tvalidation loss: 0.0607\t validation accuracy: 0.9822\n",
      "iteration number: 4393\t training loss: 0.0350\tvalidation loss: 0.0611\t validation accuracy: 0.9822\n",
      "iteration number: 4394\t training loss: 0.0348\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4395\t training loss: 0.0345\tvalidation loss: 0.0593\t validation accuracy: 0.9822\n",
      "iteration number: 4396\t training loss: 0.0358\tvalidation loss: 0.0610\t validation accuracy: 0.9778\n",
      "iteration number: 4397\t training loss: 0.0355\tvalidation loss: 0.0609\t validation accuracy: 0.9800\n",
      "iteration number: 4398\t training loss: 0.0361\tvalidation loss: 0.0622\t validation accuracy: 0.9778\n",
      "iteration number: 4399\t training loss: 0.0346\tvalidation loss: 0.0592\t validation accuracy: 0.9822\n",
      "iteration number: 4400\t training loss: 0.0345\tvalidation loss: 0.0591\t validation accuracy: 0.9822\n",
      "iteration number: 4401\t training loss: 0.0345\tvalidation loss: 0.0592\t validation accuracy: 0.9822\n",
      "iteration number: 4402\t training loss: 0.0334\tvalidation loss: 0.0597\t validation accuracy: 0.9800\n",
      "iteration number: 4403\t training loss: 0.0346\tvalidation loss: 0.0612\t validation accuracy: 0.9800\n",
      "iteration number: 4404\t training loss: 0.0350\tvalidation loss: 0.0605\t validation accuracy: 0.9800\n",
      "iteration number: 4405\t training loss: 0.0345\tvalidation loss: 0.0601\t validation accuracy: 0.9800\n",
      "iteration number: 4406\t training loss: 0.0343\tvalidation loss: 0.0608\t validation accuracy: 0.9800\n",
      "iteration number: 4407\t training loss: 0.0359\tvalidation loss: 0.0620\t validation accuracy: 0.9800\n",
      "iteration number: 4408\t training loss: 0.0358\tvalidation loss: 0.0614\t validation accuracy: 0.9800\n",
      "iteration number: 4409\t training loss: 0.0352\tvalidation loss: 0.0611\t validation accuracy: 0.9800\n",
      "iteration number: 4410\t training loss: 0.0370\tvalidation loss: 0.0617\t validation accuracy: 0.9822\n",
      "iteration number: 4411\t training loss: 0.0363\tvalidation loss: 0.0612\t validation accuracy: 0.9778\n",
      "iteration number: 4412\t training loss: 0.0356\tvalidation loss: 0.0609\t validation accuracy: 0.9778\n",
      "iteration number: 4413\t training loss: 0.0368\tvalidation loss: 0.0613\t validation accuracy: 0.9800\n",
      "iteration number: 4414\t training loss: 0.0360\tvalidation loss: 0.0608\t validation accuracy: 0.9800\n",
      "iteration number: 4415\t training loss: 0.0350\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4416\t training loss: 0.0338\tvalidation loss: 0.0598\t validation accuracy: 0.9822\n",
      "iteration number: 4417\t training loss: 0.0333\tvalidation loss: 0.0593\t validation accuracy: 0.9822\n",
      "iteration number: 4418\t training loss: 0.0335\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4419\t training loss: 0.0333\tvalidation loss: 0.0591\t validation accuracy: 0.9822\n",
      "iteration number: 4420\t training loss: 0.0333\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4421\t training loss: 0.0334\tvalidation loss: 0.0588\t validation accuracy: 0.9822\n",
      "iteration number: 4422\t training loss: 0.0332\tvalidation loss: 0.0590\t validation accuracy: 0.9844\n",
      "iteration number: 4423\t training loss: 0.0334\tvalidation loss: 0.0585\t validation accuracy: 0.9844\n",
      "iteration number: 4424\t training loss: 0.0332\tvalidation loss: 0.0586\t validation accuracy: 0.9822\n",
      "iteration number: 4425\t training loss: 0.0333\tvalidation loss: 0.0586\t validation accuracy: 0.9844\n",
      "iteration number: 4426\t training loss: 0.0336\tvalidation loss: 0.0605\t validation accuracy: 0.9822\n",
      "iteration number: 4427\t training loss: 0.0336\tvalidation loss: 0.0605\t validation accuracy: 0.9844\n",
      "iteration number: 4428\t training loss: 0.0335\tvalidation loss: 0.0605\t validation accuracy: 0.9844\n",
      "iteration number: 4429\t training loss: 0.0332\tvalidation loss: 0.0598\t validation accuracy: 0.9800\n",
      "iteration number: 4430\t training loss: 0.0333\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4431\t training loss: 0.0334\tvalidation loss: 0.0605\t validation accuracy: 0.9800\n",
      "iteration number: 4432\t training loss: 0.0345\tvalidation loss: 0.0613\t validation accuracy: 0.9800\n",
      "iteration number: 4433\t training loss: 0.0343\tvalidation loss: 0.0613\t validation accuracy: 0.9800\n",
      "iteration number: 4434\t training loss: 0.0343\tvalidation loss: 0.0614\t validation accuracy: 0.9822\n",
      "iteration number: 4435\t training loss: 0.0335\tvalidation loss: 0.0597\t validation accuracy: 0.9844\n",
      "iteration number: 4436\t training loss: 0.0333\tvalidation loss: 0.0596\t validation accuracy: 0.9844\n",
      "iteration number: 4437\t training loss: 0.0334\tvalidation loss: 0.0598\t validation accuracy: 0.9844\n",
      "iteration number: 4438\t training loss: 0.0333\tvalidation loss: 0.0603\t validation accuracy: 0.9844\n",
      "iteration number: 4439\t training loss: 0.0340\tvalidation loss: 0.0601\t validation accuracy: 0.9800\n",
      "iteration number: 4440\t training loss: 0.0345\tvalidation loss: 0.0608\t validation accuracy: 0.9800\n",
      "iteration number: 4441\t training loss: 0.0343\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4442\t training loss: 0.0338\tvalidation loss: 0.0607\t validation accuracy: 0.9800\n",
      "iteration number: 4443\t training loss: 0.0334\tvalidation loss: 0.0607\t validation accuracy: 0.9800\n",
      "iteration number: 4444\t training loss: 0.0331\tvalidation loss: 0.0602\t validation accuracy: 0.9800\n",
      "iteration number: 4445\t training loss: 0.0332\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4446\t training loss: 0.0331\tvalidation loss: 0.0596\t validation accuracy: 0.9800\n",
      "iteration number: 4447\t training loss: 0.0331\tvalidation loss: 0.0596\t validation accuracy: 0.9800\n",
      "iteration number: 4448\t training loss: 0.0331\tvalidation loss: 0.0594\t validation accuracy: 0.9800\n",
      "iteration number: 4449\t training loss: 0.0331\tvalidation loss: 0.0593\t validation accuracy: 0.9800\n",
      "iteration number: 4450\t training loss: 0.0332\tvalidation loss: 0.0592\t validation accuracy: 0.9800\n",
      "iteration number: 4451\t training loss: 0.0333\tvalidation loss: 0.0591\t validation accuracy: 0.9822\n",
      "iteration number: 4452\t training loss: 0.0335\tvalidation loss: 0.0594\t validation accuracy: 0.9822\n",
      "iteration number: 4453\t training loss: 0.0332\tvalidation loss: 0.0586\t validation accuracy: 0.9822\n",
      "iteration number: 4454\t training loss: 0.0332\tvalidation loss: 0.0592\t validation accuracy: 0.9822\n",
      "iteration number: 4455\t training loss: 0.0329\tvalidation loss: 0.0589\t validation accuracy: 0.9800\n",
      "iteration number: 4456\t training loss: 0.0330\tvalidation loss: 0.0592\t validation accuracy: 0.9822\n",
      "iteration number: 4457\t training loss: 0.0329\tvalidation loss: 0.0588\t validation accuracy: 0.9800\n",
      "iteration number: 4458\t training loss: 0.0332\tvalidation loss: 0.0597\t validation accuracy: 0.9800\n",
      "iteration number: 4459\t training loss: 0.0329\tvalidation loss: 0.0588\t validation accuracy: 0.9800\n",
      "iteration number: 4460\t training loss: 0.0330\tvalidation loss: 0.0592\t validation accuracy: 0.9800\n",
      "iteration number: 4461\t training loss: 0.0330\tvalidation loss: 0.0588\t validation accuracy: 0.9800\n",
      "iteration number: 4462\t training loss: 0.0332\tvalidation loss: 0.0597\t validation accuracy: 0.9800\n",
      "iteration number: 4463\t training loss: 0.0333\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4464\t training loss: 0.0332\tvalidation loss: 0.0594\t validation accuracy: 0.9800\n",
      "iteration number: 4465\t training loss: 0.0330\tvalidation loss: 0.0593\t validation accuracy: 0.9800\n",
      "iteration number: 4466\t training loss: 0.0333\tvalidation loss: 0.0596\t validation accuracy: 0.9800\n",
      "iteration number: 4467\t training loss: 0.0332\tvalidation loss: 0.0593\t validation accuracy: 0.9822\n",
      "iteration number: 4468\t training loss: 0.0334\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4469\t training loss: 0.0335\tvalidation loss: 0.0601\t validation accuracy: 0.9822\n",
      "iteration number: 4470\t training loss: 0.0336\tvalidation loss: 0.0600\t validation accuracy: 0.9822\n",
      "iteration number: 4471\t training loss: 0.0343\tvalidation loss: 0.0617\t validation accuracy: 0.9778\n",
      "iteration number: 4472\t training loss: 0.0338\tvalidation loss: 0.0610\t validation accuracy: 0.9800\n",
      "iteration number: 4473\t training loss: 0.0336\tvalidation loss: 0.0619\t validation accuracy: 0.9800\n",
      "iteration number: 4474\t training loss: 0.0337\tvalidation loss: 0.0622\t validation accuracy: 0.9800\n",
      "iteration number: 4475\t training loss: 0.0337\tvalidation loss: 0.0619\t validation accuracy: 0.9778\n",
      "iteration number: 4476\t training loss: 0.0333\tvalidation loss: 0.0598\t validation accuracy: 0.9822\n",
      "iteration number: 4477\t training loss: 0.0334\tvalidation loss: 0.0604\t validation accuracy: 0.9822\n",
      "iteration number: 4478\t training loss: 0.0328\tvalidation loss: 0.0595\t validation accuracy: 0.9800\n",
      "iteration number: 4479\t training loss: 0.0332\tvalidation loss: 0.0585\t validation accuracy: 0.9822\n",
      "iteration number: 4480\t training loss: 0.0335\tvalidation loss: 0.0584\t validation accuracy: 0.9822\n",
      "iteration number: 4481\t training loss: 0.0328\tvalidation loss: 0.0588\t validation accuracy: 0.9822\n",
      "iteration number: 4482\t training loss: 0.0328\tvalidation loss: 0.0588\t validation accuracy: 0.9822\n",
      "iteration number: 4483\t training loss: 0.0327\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4484\t training loss: 0.0328\tvalidation loss: 0.0588\t validation accuracy: 0.9822\n",
      "iteration number: 4485\t training loss: 0.0329\tvalidation loss: 0.0584\t validation accuracy: 0.9822\n",
      "iteration number: 4486\t training loss: 0.0332\tvalidation loss: 0.0584\t validation accuracy: 0.9822\n",
      "iteration number: 4487\t training loss: 0.0327\tvalidation loss: 0.0592\t validation accuracy: 0.9800\n",
      "iteration number: 4488\t training loss: 0.0327\tvalidation loss: 0.0590\t validation accuracy: 0.9800\n",
      "iteration number: 4489\t training loss: 0.0328\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4490\t training loss: 0.0328\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4491\t training loss: 0.0330\tvalidation loss: 0.0598\t validation accuracy: 0.9800\n",
      "iteration number: 4492\t training loss: 0.0335\tvalidation loss: 0.0597\t validation accuracy: 0.9800\n",
      "iteration number: 4493\t training loss: 0.0333\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4494\t training loss: 0.0331\tvalidation loss: 0.0586\t validation accuracy: 0.9800\n",
      "iteration number: 4495\t training loss: 0.0326\tvalidation loss: 0.0586\t validation accuracy: 0.9800\n",
      "iteration number: 4496\t training loss: 0.0326\tvalidation loss: 0.0585\t validation accuracy: 0.9800\n",
      "iteration number: 4497\t training loss: 0.0327\tvalidation loss: 0.0587\t validation accuracy: 0.9800\n",
      "iteration number: 4498\t training loss: 0.0327\tvalidation loss: 0.0584\t validation accuracy: 0.9822\n",
      "iteration number: 4499\t training loss: 0.0326\tvalidation loss: 0.0583\t validation accuracy: 0.9800\n",
      "iteration number: 4500\t training loss: 0.0328\tvalidation loss: 0.0578\t validation accuracy: 0.9822\n",
      "iteration number: 4501\t training loss: 0.0329\tvalidation loss: 0.0578\t validation accuracy: 0.9822\n",
      "iteration number: 4502\t training loss: 0.0333\tvalidation loss: 0.0595\t validation accuracy: 0.9822\n",
      "iteration number: 4503\t training loss: 0.0341\tvalidation loss: 0.0612\t validation accuracy: 0.9800\n",
      "iteration number: 4504\t training loss: 0.0345\tvalidation loss: 0.0617\t validation accuracy: 0.9800\n",
      "iteration number: 4505\t training loss: 0.0341\tvalidation loss: 0.0611\t validation accuracy: 0.9822\n",
      "iteration number: 4506\t training loss: 0.0336\tvalidation loss: 0.0601\t validation accuracy: 0.9822\n",
      "iteration number: 4507\t training loss: 0.0339\tvalidation loss: 0.0613\t validation accuracy: 0.9822\n",
      "iteration number: 4508\t training loss: 0.0333\tvalidation loss: 0.0604\t validation accuracy: 0.9822\n",
      "iteration number: 4509\t training loss: 0.0331\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4510\t training loss: 0.0325\tvalidation loss: 0.0589\t validation accuracy: 0.9844\n",
      "iteration number: 4511\t training loss: 0.0327\tvalidation loss: 0.0597\t validation accuracy: 0.9822\n",
      "iteration number: 4512\t training loss: 0.0324\tvalidation loss: 0.0592\t validation accuracy: 0.9800\n",
      "iteration number: 4513\t training loss: 0.0327\tvalidation loss: 0.0595\t validation accuracy: 0.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4514\t training loss: 0.0331\tvalidation loss: 0.0601\t validation accuracy: 0.9800\n",
      "iteration number: 4515\t training loss: 0.0330\tvalidation loss: 0.0602\t validation accuracy: 0.9800\n",
      "iteration number: 4516\t training loss: 0.0329\tvalidation loss: 0.0597\t validation accuracy: 0.9800\n",
      "iteration number: 4517\t training loss: 0.0332\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4518\t training loss: 0.0333\tvalidation loss: 0.0598\t validation accuracy: 0.9800\n",
      "iteration number: 4519\t training loss: 0.0337\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4520\t training loss: 0.0346\tvalidation loss: 0.0603\t validation accuracy: 0.9800\n",
      "iteration number: 4521\t training loss: 0.0344\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4522\t training loss: 0.0335\tvalidation loss: 0.0600\t validation accuracy: 0.9800\n",
      "iteration number: 4523\t training loss: 0.0337\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4524\t training loss: 0.0334\tvalidation loss: 0.0593\t validation accuracy: 0.9800\n",
      "iteration number: 4525\t training loss: 0.0332\tvalidation loss: 0.0594\t validation accuracy: 0.9800\n",
      "iteration number: 4526\t training loss: 0.0337\tvalidation loss: 0.0595\t validation accuracy: 0.9800\n",
      "iteration number: 4527\t training loss: 0.0338\tvalidation loss: 0.0593\t validation accuracy: 0.9800\n",
      "iteration number: 4528\t training loss: 0.0339\tvalidation loss: 0.0593\t validation accuracy: 0.9800\n",
      "iteration number: 4529\t training loss: 0.0335\tvalidation loss: 0.0597\t validation accuracy: 0.9800\n",
      "iteration number: 4530\t training loss: 0.0339\tvalidation loss: 0.0601\t validation accuracy: 0.9800\n",
      "iteration number: 4531\t training loss: 0.0336\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4532\t training loss: 0.0325\tvalidation loss: 0.0597\t validation accuracy: 0.9800\n",
      "iteration number: 4533\t training loss: 0.0330\tvalidation loss: 0.0605\t validation accuracy: 0.9800\n",
      "iteration number: 4534\t training loss: 0.0329\tvalidation loss: 0.0609\t validation accuracy: 0.9800\n",
      "iteration number: 4535\t training loss: 0.0332\tvalidation loss: 0.0616\t validation accuracy: 0.9800\n",
      "iteration number: 4536\t training loss: 0.0328\tvalidation loss: 0.0610\t validation accuracy: 0.9800\n",
      "iteration number: 4537\t training loss: 0.0325\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4538\t training loss: 0.0326\tvalidation loss: 0.0608\t validation accuracy: 0.9800\n",
      "iteration number: 4539\t training loss: 0.0324\tvalidation loss: 0.0603\t validation accuracy: 0.9800\n",
      "iteration number: 4540\t training loss: 0.0323\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4541\t training loss: 0.0324\tvalidation loss: 0.0603\t validation accuracy: 0.9800\n",
      "iteration number: 4542\t training loss: 0.0324\tvalidation loss: 0.0605\t validation accuracy: 0.9800\n",
      "iteration number: 4543\t training loss: 0.0330\tvalidation loss: 0.0618\t validation accuracy: 0.9800\n",
      "iteration number: 4544\t training loss: 0.0328\tvalidation loss: 0.0614\t validation accuracy: 0.9800\n",
      "iteration number: 4545\t training loss: 0.0324\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4546\t training loss: 0.0329\tvalidation loss: 0.0612\t validation accuracy: 0.9800\n",
      "iteration number: 4547\t training loss: 0.0325\tvalidation loss: 0.0605\t validation accuracy: 0.9800\n",
      "iteration number: 4548\t training loss: 0.0330\tvalidation loss: 0.0609\t validation accuracy: 0.9800\n",
      "iteration number: 4549\t training loss: 0.0335\tvalidation loss: 0.0617\t validation accuracy: 0.9800\n",
      "iteration number: 4550\t training loss: 0.0325\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4551\t training loss: 0.0322\tvalidation loss: 0.0594\t validation accuracy: 0.9800\n",
      "iteration number: 4552\t training loss: 0.0323\tvalidation loss: 0.0594\t validation accuracy: 0.9800\n",
      "iteration number: 4553\t training loss: 0.0323\tvalidation loss: 0.0598\t validation accuracy: 0.9800\n",
      "iteration number: 4554\t training loss: 0.0322\tvalidation loss: 0.0600\t validation accuracy: 0.9800\n",
      "iteration number: 4555\t training loss: 0.0322\tvalidation loss: 0.0594\t validation accuracy: 0.9800\n",
      "iteration number: 4556\t training loss: 0.0327\tvalidation loss: 0.0610\t validation accuracy: 0.9800\n",
      "iteration number: 4557\t training loss: 0.0323\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4558\t training loss: 0.0323\tvalidation loss: 0.0597\t validation accuracy: 0.9800\n",
      "iteration number: 4559\t training loss: 0.0329\tvalidation loss: 0.0597\t validation accuracy: 0.9822\n",
      "iteration number: 4560\t training loss: 0.0328\tvalidation loss: 0.0602\t validation accuracy: 0.9800\n",
      "iteration number: 4561\t training loss: 0.0331\tvalidation loss: 0.0601\t validation accuracy: 0.9800\n",
      "iteration number: 4562\t training loss: 0.0335\tvalidation loss: 0.0615\t validation accuracy: 0.9778\n",
      "iteration number: 4563\t training loss: 0.0328\tvalidation loss: 0.0605\t validation accuracy: 0.9800\n",
      "iteration number: 4564\t training loss: 0.0332\tvalidation loss: 0.0610\t validation accuracy: 0.9800\n",
      "iteration number: 4565\t training loss: 0.0327\tvalidation loss: 0.0602\t validation accuracy: 0.9800\n",
      "iteration number: 4566\t training loss: 0.0323\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4567\t training loss: 0.0327\tvalidation loss: 0.0608\t validation accuracy: 0.9822\n",
      "iteration number: 4568\t training loss: 0.0324\tvalidation loss: 0.0606\t validation accuracy: 0.9822\n",
      "iteration number: 4569\t training loss: 0.0321\tvalidation loss: 0.0602\t validation accuracy: 0.9800\n",
      "iteration number: 4570\t training loss: 0.0321\tvalidation loss: 0.0600\t validation accuracy: 0.9800\n",
      "iteration number: 4571\t training loss: 0.0322\tvalidation loss: 0.0601\t validation accuracy: 0.9800\n",
      "iteration number: 4572\t training loss: 0.0322\tvalidation loss: 0.0606\t validation accuracy: 0.9800\n",
      "iteration number: 4573\t training loss: 0.0321\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4574\t training loss: 0.0322\tvalidation loss: 0.0608\t validation accuracy: 0.9800\n",
      "iteration number: 4575\t training loss: 0.0322\tvalidation loss: 0.0602\t validation accuracy: 0.9800\n",
      "iteration number: 4576\t training loss: 0.0323\tvalidation loss: 0.0606\t validation accuracy: 0.9822\n",
      "iteration number: 4577\t training loss: 0.0334\tvalidation loss: 0.0621\t validation accuracy: 0.9800\n",
      "iteration number: 4578\t training loss: 0.0332\tvalidation loss: 0.0626\t validation accuracy: 0.9800\n",
      "iteration number: 4579\t training loss: 0.0332\tvalidation loss: 0.0618\t validation accuracy: 0.9800\n",
      "iteration number: 4580\t training loss: 0.0328\tvalidation loss: 0.0611\t validation accuracy: 0.9822\n",
      "iteration number: 4581\t training loss: 0.0321\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4582\t training loss: 0.0323\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4583\t training loss: 0.0321\tvalidation loss: 0.0600\t validation accuracy: 0.9800\n",
      "iteration number: 4584\t training loss: 0.0319\tvalidation loss: 0.0592\t validation accuracy: 0.9800\n",
      "iteration number: 4585\t training loss: 0.0319\tvalidation loss: 0.0590\t validation accuracy: 0.9800\n",
      "iteration number: 4586\t training loss: 0.0320\tvalidation loss: 0.0592\t validation accuracy: 0.9800\n",
      "iteration number: 4587\t training loss: 0.0319\tvalidation loss: 0.0586\t validation accuracy: 0.9800\n",
      "iteration number: 4588\t training loss: 0.0321\tvalidation loss: 0.0592\t validation accuracy: 0.9800\n",
      "iteration number: 4589\t training loss: 0.0319\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4590\t training loss: 0.0318\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4591\t training loss: 0.0319\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4592\t training loss: 0.0324\tvalidation loss: 0.0595\t validation accuracy: 0.9822\n",
      "iteration number: 4593\t training loss: 0.0319\tvalidation loss: 0.0583\t validation accuracy: 0.9844\n",
      "iteration number: 4594\t training loss: 0.0323\tvalidation loss: 0.0592\t validation accuracy: 0.9822\n",
      "iteration number: 4595\t training loss: 0.0329\tvalidation loss: 0.0598\t validation accuracy: 0.9822\n",
      "iteration number: 4596\t training loss: 0.0327\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4597\t training loss: 0.0324\tvalidation loss: 0.0587\t validation accuracy: 0.9822\n",
      "iteration number: 4598\t training loss: 0.0322\tvalidation loss: 0.0586\t validation accuracy: 0.9822\n",
      "iteration number: 4599\t training loss: 0.0318\tvalidation loss: 0.0583\t validation accuracy: 0.9822\n",
      "iteration number: 4600\t training loss: 0.0318\tvalidation loss: 0.0585\t validation accuracy: 0.9822\n",
      "iteration number: 4601\t training loss: 0.0323\tvalidation loss: 0.0593\t validation accuracy: 0.9800\n",
      "iteration number: 4602\t training loss: 0.0323\tvalidation loss: 0.0592\t validation accuracy: 0.9822\n",
      "iteration number: 4603\t training loss: 0.0317\tvalidation loss: 0.0589\t validation accuracy: 0.9844\n",
      "iteration number: 4604\t training loss: 0.0317\tvalidation loss: 0.0587\t validation accuracy: 0.9822\n",
      "iteration number: 4605\t training loss: 0.0318\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4606\t training loss: 0.0319\tvalidation loss: 0.0594\t validation accuracy: 0.9822\n",
      "iteration number: 4607\t training loss: 0.0318\tvalidation loss: 0.0592\t validation accuracy: 0.9800\n",
      "iteration number: 4608\t training loss: 0.0317\tvalidation loss: 0.0590\t validation accuracy: 0.9800\n",
      "iteration number: 4609\t training loss: 0.0318\tvalidation loss: 0.0576\t validation accuracy: 0.9844\n",
      "iteration number: 4610\t training loss: 0.0318\tvalidation loss: 0.0578\t validation accuracy: 0.9822\n",
      "iteration number: 4611\t training loss: 0.0318\tvalidation loss: 0.0577\t validation accuracy: 0.9844\n",
      "iteration number: 4612\t training loss: 0.0317\tvalidation loss: 0.0575\t validation accuracy: 0.9844\n",
      "iteration number: 4613\t training loss: 0.0316\tvalidation loss: 0.0584\t validation accuracy: 0.9844\n",
      "iteration number: 4614\t training loss: 0.0317\tvalidation loss: 0.0585\t validation accuracy: 0.9822\n",
      "iteration number: 4615\t training loss: 0.0317\tvalidation loss: 0.0586\t validation accuracy: 0.9822\n",
      "iteration number: 4616\t training loss: 0.0319\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4617\t training loss: 0.0318\tvalidation loss: 0.0595\t validation accuracy: 0.9822\n",
      "iteration number: 4618\t training loss: 0.0317\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4619\t training loss: 0.0316\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4620\t training loss: 0.0316\tvalidation loss: 0.0593\t validation accuracy: 0.9822\n",
      "iteration number: 4621\t training loss: 0.0323\tvalidation loss: 0.0605\t validation accuracy: 0.9822\n",
      "iteration number: 4622\t training loss: 0.0326\tvalidation loss: 0.0621\t validation accuracy: 0.9822\n",
      "iteration number: 4623\t training loss: 0.0326\tvalidation loss: 0.0620\t validation accuracy: 0.9822\n",
      "iteration number: 4624\t training loss: 0.0330\tvalidation loss: 0.0604\t validation accuracy: 0.9778\n",
      "iteration number: 4625\t training loss: 0.0324\tvalidation loss: 0.0614\t validation accuracy: 0.9800\n",
      "iteration number: 4626\t training loss: 0.0322\tvalidation loss: 0.0609\t validation accuracy: 0.9822\n",
      "iteration number: 4627\t training loss: 0.0322\tvalidation loss: 0.0614\t validation accuracy: 0.9822\n",
      "iteration number: 4628\t training loss: 0.0322\tvalidation loss: 0.0606\t validation accuracy: 0.9844\n",
      "iteration number: 4629\t training loss: 0.0323\tvalidation loss: 0.0602\t validation accuracy: 0.9800\n",
      "iteration number: 4630\t training loss: 0.0325\tvalidation loss: 0.0606\t validation accuracy: 0.9844\n",
      "iteration number: 4631\t training loss: 0.0326\tvalidation loss: 0.0617\t validation accuracy: 0.9800\n",
      "iteration number: 4632\t training loss: 0.0326\tvalidation loss: 0.0617\t validation accuracy: 0.9822\n",
      "iteration number: 4633\t training loss: 0.0325\tvalidation loss: 0.0615\t validation accuracy: 0.9822\n",
      "iteration number: 4634\t training loss: 0.0322\tvalidation loss: 0.0607\t validation accuracy: 0.9822\n",
      "iteration number: 4635\t training loss: 0.0319\tvalidation loss: 0.0600\t validation accuracy: 0.9822\n",
      "iteration number: 4636\t training loss: 0.0322\tvalidation loss: 0.0604\t validation accuracy: 0.9822\n",
      "iteration number: 4637\t training loss: 0.0324\tvalidation loss: 0.0609\t validation accuracy: 0.9822\n",
      "iteration number: 4638\t training loss: 0.0325\tvalidation loss: 0.0609\t validation accuracy: 0.9800\n",
      "iteration number: 4639\t training loss: 0.0322\tvalidation loss: 0.0606\t validation accuracy: 0.9800\n",
      "iteration number: 4640\t training loss: 0.0324\tvalidation loss: 0.0610\t validation accuracy: 0.9800\n",
      "iteration number: 4641\t training loss: 0.0317\tvalidation loss: 0.0598\t validation accuracy: 0.9844\n",
      "iteration number: 4642\t training loss: 0.0316\tvalidation loss: 0.0596\t validation accuracy: 0.9800\n",
      "iteration number: 4643\t training loss: 0.0322\tvalidation loss: 0.0596\t validation accuracy: 0.9778\n",
      "iteration number: 4644\t training loss: 0.0320\tvalidation loss: 0.0589\t validation accuracy: 0.9800\n",
      "iteration number: 4645\t training loss: 0.0321\tvalidation loss: 0.0589\t validation accuracy: 0.9800\n",
      "iteration number: 4646\t training loss: 0.0314\tvalidation loss: 0.0585\t validation accuracy: 0.9822\n",
      "iteration number: 4647\t training loss: 0.0318\tvalidation loss: 0.0594\t validation accuracy: 0.9778\n",
      "iteration number: 4648\t training loss: 0.0317\tvalidation loss: 0.0593\t validation accuracy: 0.9800\n",
      "iteration number: 4649\t training loss: 0.0330\tvalidation loss: 0.0596\t validation accuracy: 0.9778\n",
      "iteration number: 4650\t training loss: 0.0328\tvalidation loss: 0.0593\t validation accuracy: 0.9822\n",
      "iteration number: 4651\t training loss: 0.0331\tvalidation loss: 0.0594\t validation accuracy: 0.9822\n",
      "iteration number: 4652\t training loss: 0.0329\tvalidation loss: 0.0597\t validation accuracy: 0.9800\n",
      "iteration number: 4653\t training loss: 0.0330\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4654\t training loss: 0.0318\tvalidation loss: 0.0594\t validation accuracy: 0.9844\n",
      "iteration number: 4655\t training loss: 0.0315\tvalidation loss: 0.0603\t validation accuracy: 0.9800\n",
      "iteration number: 4656\t training loss: 0.0317\tvalidation loss: 0.0608\t validation accuracy: 0.9800\n",
      "iteration number: 4657\t training loss: 0.0316\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4658\t training loss: 0.0313\tvalidation loss: 0.0592\t validation accuracy: 0.9822\n",
      "iteration number: 4659\t training loss: 0.0314\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4660\t training loss: 0.0315\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4661\t training loss: 0.0314\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4662\t training loss: 0.0319\tvalidation loss: 0.0584\t validation accuracy: 0.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4663\t training loss: 0.0318\tvalidation loss: 0.0594\t validation accuracy: 0.9822\n",
      "iteration number: 4664\t training loss: 0.0316\tvalidation loss: 0.0594\t validation accuracy: 0.9822\n",
      "iteration number: 4665\t training loss: 0.0322\tvalidation loss: 0.0597\t validation accuracy: 0.9800\n",
      "iteration number: 4666\t training loss: 0.0326\tvalidation loss: 0.0601\t validation accuracy: 0.9800\n",
      "iteration number: 4667\t training loss: 0.0324\tvalidation loss: 0.0600\t validation accuracy: 0.9800\n",
      "iteration number: 4668\t training loss: 0.0324\tvalidation loss: 0.0596\t validation accuracy: 0.9800\n",
      "iteration number: 4669\t training loss: 0.0316\tvalidation loss: 0.0586\t validation accuracy: 0.9822\n",
      "iteration number: 4670\t training loss: 0.0316\tvalidation loss: 0.0585\t validation accuracy: 0.9822\n",
      "iteration number: 4671\t training loss: 0.0320\tvalidation loss: 0.0582\t validation accuracy: 0.9822\n",
      "iteration number: 4672\t training loss: 0.0319\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4673\t training loss: 0.0316\tvalidation loss: 0.0581\t validation accuracy: 0.9867\n",
      "iteration number: 4674\t training loss: 0.0317\tvalidation loss: 0.0591\t validation accuracy: 0.9822\n",
      "iteration number: 4675\t training loss: 0.0315\tvalidation loss: 0.0586\t validation accuracy: 0.9800\n",
      "iteration number: 4676\t training loss: 0.0317\tvalidation loss: 0.0586\t validation accuracy: 0.9800\n",
      "iteration number: 4677\t training loss: 0.0321\tvalidation loss: 0.0592\t validation accuracy: 0.9800\n",
      "iteration number: 4678\t training loss: 0.0317\tvalidation loss: 0.0588\t validation accuracy: 0.9778\n",
      "iteration number: 4679\t training loss: 0.0312\tvalidation loss: 0.0584\t validation accuracy: 0.9844\n",
      "iteration number: 4680\t training loss: 0.0313\tvalidation loss: 0.0590\t validation accuracy: 0.9800\n",
      "iteration number: 4681\t training loss: 0.0315\tvalidation loss: 0.0590\t validation accuracy: 0.9800\n",
      "iteration number: 4682\t training loss: 0.0314\tvalidation loss: 0.0586\t validation accuracy: 0.9822\n",
      "iteration number: 4683\t training loss: 0.0315\tvalidation loss: 0.0584\t validation accuracy: 0.9822\n",
      "iteration number: 4684\t training loss: 0.0313\tvalidation loss: 0.0578\t validation accuracy: 0.9822\n",
      "iteration number: 4685\t training loss: 0.0311\tvalidation loss: 0.0573\t validation accuracy: 0.9844\n",
      "iteration number: 4686\t training loss: 0.0313\tvalidation loss: 0.0574\t validation accuracy: 0.9822\n",
      "iteration number: 4687\t training loss: 0.0318\tvalidation loss: 0.0571\t validation accuracy: 0.9844\n",
      "iteration number: 4688\t training loss: 0.0322\tvalidation loss: 0.0578\t validation accuracy: 0.9822\n",
      "iteration number: 4689\t training loss: 0.0318\tvalidation loss: 0.0577\t validation accuracy: 0.9822\n",
      "iteration number: 4690\t training loss: 0.0320\tvalidation loss: 0.0582\t validation accuracy: 0.9822\n",
      "iteration number: 4691\t training loss: 0.0323\tvalidation loss: 0.0590\t validation accuracy: 0.9844\n",
      "iteration number: 4692\t training loss: 0.0316\tvalidation loss: 0.0585\t validation accuracy: 0.9822\n",
      "iteration number: 4693\t training loss: 0.0319\tvalidation loss: 0.0588\t validation accuracy: 0.9800\n",
      "iteration number: 4694\t training loss: 0.0318\tvalidation loss: 0.0588\t validation accuracy: 0.9800\n",
      "iteration number: 4695\t training loss: 0.0313\tvalidation loss: 0.0591\t validation accuracy: 0.9800\n",
      "iteration number: 4696\t training loss: 0.0315\tvalidation loss: 0.0592\t validation accuracy: 0.9800\n",
      "iteration number: 4697\t training loss: 0.0313\tvalidation loss: 0.0587\t validation accuracy: 0.9867\n",
      "iteration number: 4698\t training loss: 0.0313\tvalidation loss: 0.0584\t validation accuracy: 0.9867\n",
      "iteration number: 4699\t training loss: 0.0321\tvalidation loss: 0.0596\t validation accuracy: 0.9822\n",
      "iteration number: 4700\t training loss: 0.0337\tvalidation loss: 0.0617\t validation accuracy: 0.9778\n",
      "iteration number: 4701\t training loss: 0.0336\tvalidation loss: 0.0612\t validation accuracy: 0.9778\n",
      "iteration number: 4702\t training loss: 0.0320\tvalidation loss: 0.0603\t validation accuracy: 0.9778\n",
      "iteration number: 4703\t training loss: 0.0319\tvalidation loss: 0.0604\t validation accuracy: 0.9800\n",
      "iteration number: 4704\t training loss: 0.0320\tvalidation loss: 0.0603\t validation accuracy: 0.9822\n",
      "iteration number: 4705\t training loss: 0.0319\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4706\t training loss: 0.0316\tvalidation loss: 0.0596\t validation accuracy: 0.9822\n",
      "iteration number: 4707\t training loss: 0.0317\tvalidation loss: 0.0594\t validation accuracy: 0.9844\n",
      "iteration number: 4708\t training loss: 0.0314\tvalidation loss: 0.0584\t validation accuracy: 0.9822\n",
      "iteration number: 4709\t training loss: 0.0310\tvalidation loss: 0.0583\t validation accuracy: 0.9844\n",
      "iteration number: 4710\t training loss: 0.0310\tvalidation loss: 0.0582\t validation accuracy: 0.9822\n",
      "iteration number: 4711\t training loss: 0.0308\tvalidation loss: 0.0584\t validation accuracy: 0.9844\n",
      "iteration number: 4712\t training loss: 0.0308\tvalidation loss: 0.0584\t validation accuracy: 0.9844\n",
      "iteration number: 4713\t training loss: 0.0307\tvalidation loss: 0.0584\t validation accuracy: 0.9844\n",
      "iteration number: 4714\t training loss: 0.0308\tvalidation loss: 0.0596\t validation accuracy: 0.9800\n",
      "iteration number: 4715\t training loss: 0.0311\tvalidation loss: 0.0602\t validation accuracy: 0.9822\n",
      "iteration number: 4716\t training loss: 0.0308\tvalidation loss: 0.0601\t validation accuracy: 0.9800\n",
      "iteration number: 4717\t training loss: 0.0318\tvalidation loss: 0.0619\t validation accuracy: 0.9778\n",
      "iteration number: 4718\t training loss: 0.0319\tvalidation loss: 0.0621\t validation accuracy: 0.9756\n",
      "iteration number: 4719\t training loss: 0.0312\tvalidation loss: 0.0605\t validation accuracy: 0.9778\n",
      "iteration number: 4720\t training loss: 0.0311\tvalidation loss: 0.0603\t validation accuracy: 0.9778\n",
      "iteration number: 4721\t training loss: 0.0314\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4722\t training loss: 0.0311\tvalidation loss: 0.0594\t validation accuracy: 0.9822\n",
      "iteration number: 4723\t training loss: 0.0307\tvalidation loss: 0.0584\t validation accuracy: 0.9844\n",
      "iteration number: 4724\t training loss: 0.0308\tvalidation loss: 0.0584\t validation accuracy: 0.9822\n",
      "iteration number: 4725\t training loss: 0.0310\tvalidation loss: 0.0589\t validation accuracy: 0.9800\n",
      "iteration number: 4726\t training loss: 0.0309\tvalidation loss: 0.0586\t validation accuracy: 0.9867\n",
      "iteration number: 4727\t training loss: 0.0307\tvalidation loss: 0.0586\t validation accuracy: 0.9867\n",
      "iteration number: 4728\t training loss: 0.0307\tvalidation loss: 0.0587\t validation accuracy: 0.9867\n",
      "iteration number: 4729\t training loss: 0.0317\tvalidation loss: 0.0591\t validation accuracy: 0.9822\n",
      "iteration number: 4730\t training loss: 0.0313\tvalidation loss: 0.0586\t validation accuracy: 0.9822\n",
      "iteration number: 4731\t training loss: 0.0312\tvalidation loss: 0.0584\t validation accuracy: 0.9844\n",
      "iteration number: 4732\t training loss: 0.0309\tvalidation loss: 0.0585\t validation accuracy: 0.9822\n",
      "iteration number: 4733\t training loss: 0.0307\tvalidation loss: 0.0579\t validation accuracy: 0.9844\n",
      "iteration number: 4734\t training loss: 0.0309\tvalidation loss: 0.0580\t validation accuracy: 0.9822\n",
      "iteration number: 4735\t training loss: 0.0306\tvalidation loss: 0.0578\t validation accuracy: 0.9844\n",
      "iteration number: 4736\t training loss: 0.0307\tvalidation loss: 0.0580\t validation accuracy: 0.9800\n",
      "iteration number: 4737\t training loss: 0.0313\tvalidation loss: 0.0585\t validation accuracy: 0.9844\n",
      "iteration number: 4738\t training loss: 0.0313\tvalidation loss: 0.0584\t validation accuracy: 0.9822\n",
      "iteration number: 4739\t training loss: 0.0313\tvalidation loss: 0.0595\t validation accuracy: 0.9822\n",
      "iteration number: 4740\t training loss: 0.0311\tvalidation loss: 0.0591\t validation accuracy: 0.9822\n",
      "iteration number: 4741\t training loss: 0.0316\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4742\t training loss: 0.0319\tvalidation loss: 0.0603\t validation accuracy: 0.9844\n",
      "iteration number: 4743\t training loss: 0.0318\tvalidation loss: 0.0601\t validation accuracy: 0.9844\n",
      "iteration number: 4744\t training loss: 0.0310\tvalidation loss: 0.0597\t validation accuracy: 0.9867\n",
      "iteration number: 4745\t training loss: 0.0304\tvalidation loss: 0.0586\t validation accuracy: 0.9844\n",
      "iteration number: 4746\t training loss: 0.0303\tvalidation loss: 0.0583\t validation accuracy: 0.9822\n",
      "iteration number: 4747\t training loss: 0.0304\tvalidation loss: 0.0590\t validation accuracy: 0.9844\n",
      "iteration number: 4748\t training loss: 0.0315\tvalidation loss: 0.0603\t validation accuracy: 0.9844\n",
      "iteration number: 4749\t training loss: 0.0306\tvalidation loss: 0.0594\t validation accuracy: 0.9844\n",
      "iteration number: 4750\t training loss: 0.0308\tvalidation loss: 0.0597\t validation accuracy: 0.9844\n",
      "iteration number: 4751\t training loss: 0.0317\tvalidation loss: 0.0619\t validation accuracy: 0.9800\n",
      "iteration number: 4752\t training loss: 0.0315\tvalidation loss: 0.0614\t validation accuracy: 0.9800\n",
      "iteration number: 4753\t training loss: 0.0319\tvalidation loss: 0.0610\t validation accuracy: 0.9778\n",
      "iteration number: 4754\t training loss: 0.0314\tvalidation loss: 0.0603\t validation accuracy: 0.9822\n",
      "iteration number: 4755\t training loss: 0.0313\tvalidation loss: 0.0604\t validation accuracy: 0.9778\n",
      "iteration number: 4756\t training loss: 0.0307\tvalidation loss: 0.0597\t validation accuracy: 0.9800\n",
      "iteration number: 4757\t training loss: 0.0306\tvalidation loss: 0.0595\t validation accuracy: 0.9800\n",
      "iteration number: 4758\t training loss: 0.0307\tvalidation loss: 0.0598\t validation accuracy: 0.9800\n",
      "iteration number: 4759\t training loss: 0.0307\tvalidation loss: 0.0599\t validation accuracy: 0.9778\n",
      "iteration number: 4760\t training loss: 0.0306\tvalidation loss: 0.0595\t validation accuracy: 0.9822\n",
      "iteration number: 4761\t training loss: 0.0307\tvalidation loss: 0.0595\t validation accuracy: 0.9844\n",
      "iteration number: 4762\t training loss: 0.0308\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4763\t training loss: 0.0308\tvalidation loss: 0.0597\t validation accuracy: 0.9822\n",
      "iteration number: 4764\t training loss: 0.0311\tvalidation loss: 0.0591\t validation accuracy: 0.9844\n",
      "iteration number: 4765\t training loss: 0.0310\tvalidation loss: 0.0588\t validation accuracy: 0.9867\n",
      "iteration number: 4766\t training loss: 0.0323\tvalidation loss: 0.0609\t validation accuracy: 0.9800\n",
      "iteration number: 4767\t training loss: 0.0317\tvalidation loss: 0.0611\t validation accuracy: 0.9778\n",
      "iteration number: 4768\t training loss: 0.0315\tvalidation loss: 0.0611\t validation accuracy: 0.9778\n",
      "iteration number: 4769\t training loss: 0.0312\tvalidation loss: 0.0611\t validation accuracy: 0.9844\n",
      "iteration number: 4770\t training loss: 0.0311\tvalidation loss: 0.0609\t validation accuracy: 0.9844\n",
      "iteration number: 4771\t training loss: 0.0329\tvalidation loss: 0.0633\t validation accuracy: 0.9756\n",
      "iteration number: 4772\t training loss: 0.0312\tvalidation loss: 0.0607\t validation accuracy: 0.9844\n",
      "iteration number: 4773\t training loss: 0.0308\tvalidation loss: 0.0603\t validation accuracy: 0.9844\n",
      "iteration number: 4774\t training loss: 0.0303\tvalidation loss: 0.0593\t validation accuracy: 0.9844\n",
      "iteration number: 4775\t training loss: 0.0304\tvalidation loss: 0.0594\t validation accuracy: 0.9844\n",
      "iteration number: 4776\t training loss: 0.0302\tvalidation loss: 0.0589\t validation accuracy: 0.9844\n",
      "iteration number: 4777\t training loss: 0.0316\tvalidation loss: 0.0613\t validation accuracy: 0.9778\n",
      "iteration number: 4778\t training loss: 0.0322\tvalidation loss: 0.0620\t validation accuracy: 0.9778\n",
      "iteration number: 4779\t training loss: 0.0310\tvalidation loss: 0.0601\t validation accuracy: 0.9844\n",
      "iteration number: 4780\t training loss: 0.0310\tvalidation loss: 0.0605\t validation accuracy: 0.9844\n",
      "iteration number: 4781\t training loss: 0.0308\tvalidation loss: 0.0599\t validation accuracy: 0.9844\n",
      "iteration number: 4782\t training loss: 0.0305\tvalidation loss: 0.0596\t validation accuracy: 0.9844\n",
      "iteration number: 4783\t training loss: 0.0303\tvalidation loss: 0.0597\t validation accuracy: 0.9844\n",
      "iteration number: 4784\t training loss: 0.0303\tvalidation loss: 0.0595\t validation accuracy: 0.9844\n",
      "iteration number: 4785\t training loss: 0.0303\tvalidation loss: 0.0591\t validation accuracy: 0.9844\n",
      "iteration number: 4786\t training loss: 0.0301\tvalidation loss: 0.0587\t validation accuracy: 0.9844\n",
      "iteration number: 4787\t training loss: 0.0302\tvalidation loss: 0.0581\t validation accuracy: 0.9844\n",
      "iteration number: 4788\t training loss: 0.0303\tvalidation loss: 0.0580\t validation accuracy: 0.9844\n",
      "iteration number: 4789\t training loss: 0.0305\tvalidation loss: 0.0575\t validation accuracy: 0.9867\n",
      "iteration number: 4790\t training loss: 0.0304\tvalidation loss: 0.0575\t validation accuracy: 0.9867\n",
      "iteration number: 4791\t training loss: 0.0304\tvalidation loss: 0.0574\t validation accuracy: 0.9867\n",
      "iteration number: 4792\t training loss: 0.0302\tvalidation loss: 0.0574\t validation accuracy: 0.9844\n",
      "iteration number: 4793\t training loss: 0.0304\tvalidation loss: 0.0575\t validation accuracy: 0.9867\n",
      "iteration number: 4794\t training loss: 0.0304\tvalidation loss: 0.0574\t validation accuracy: 0.9867\n",
      "iteration number: 4795\t training loss: 0.0300\tvalidation loss: 0.0575\t validation accuracy: 0.9844\n",
      "iteration number: 4796\t training loss: 0.0303\tvalidation loss: 0.0575\t validation accuracy: 0.9844\n",
      "iteration number: 4797\t training loss: 0.0306\tvalidation loss: 0.0597\t validation accuracy: 0.9844\n",
      "iteration number: 4798\t training loss: 0.0309\tvalidation loss: 0.0602\t validation accuracy: 0.9822\n",
      "iteration number: 4799\t training loss: 0.0310\tvalidation loss: 0.0596\t validation accuracy: 0.9822\n",
      "iteration number: 4800\t training loss: 0.0306\tvalidation loss: 0.0589\t validation accuracy: 0.9844\n",
      "iteration number: 4801\t training loss: 0.0307\tvalidation loss: 0.0588\t validation accuracy: 0.9822\n",
      "iteration number: 4802\t training loss: 0.0316\tvalidation loss: 0.0613\t validation accuracy: 0.9756\n",
      "iteration number: 4803\t training loss: 0.0307\tvalidation loss: 0.0599\t validation accuracy: 0.9800\n",
      "iteration number: 4804\t training loss: 0.0297\tvalidation loss: 0.0585\t validation accuracy: 0.9800\n",
      "iteration number: 4805\t training loss: 0.0298\tvalidation loss: 0.0586\t validation accuracy: 0.9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4806\t training loss: 0.0307\tvalidation loss: 0.0588\t validation accuracy: 0.9822\n",
      "iteration number: 4807\t training loss: 0.0305\tvalidation loss: 0.0584\t validation accuracy: 0.9822\n",
      "iteration number: 4808\t training loss: 0.0297\tvalidation loss: 0.0581\t validation accuracy: 0.9800\n",
      "iteration number: 4809\t training loss: 0.0297\tvalidation loss: 0.0583\t validation accuracy: 0.9800\n",
      "iteration number: 4810\t training loss: 0.0297\tvalidation loss: 0.0582\t validation accuracy: 0.9800\n",
      "iteration number: 4811\t training loss: 0.0297\tvalidation loss: 0.0580\t validation accuracy: 0.9800\n",
      "iteration number: 4812\t training loss: 0.0300\tvalidation loss: 0.0574\t validation accuracy: 0.9800\n",
      "iteration number: 4813\t training loss: 0.0298\tvalidation loss: 0.0574\t validation accuracy: 0.9800\n",
      "iteration number: 4814\t training loss: 0.0298\tvalidation loss: 0.0575\t validation accuracy: 0.9800\n",
      "iteration number: 4815\t training loss: 0.0301\tvalidation loss: 0.0580\t validation accuracy: 0.9844\n",
      "iteration number: 4816\t training loss: 0.0299\tvalidation loss: 0.0573\t validation accuracy: 0.9844\n",
      "iteration number: 4817\t training loss: 0.0299\tvalidation loss: 0.0571\t validation accuracy: 0.9844\n",
      "iteration number: 4818\t training loss: 0.0303\tvalidation loss: 0.0575\t validation accuracy: 0.9844\n",
      "iteration number: 4819\t training loss: 0.0305\tvalidation loss: 0.0581\t validation accuracy: 0.9822\n",
      "iteration number: 4820\t training loss: 0.0300\tvalidation loss: 0.0573\t validation accuracy: 0.9822\n",
      "iteration number: 4821\t training loss: 0.0299\tvalidation loss: 0.0576\t validation accuracy: 0.9822\n",
      "iteration number: 4822\t training loss: 0.0299\tvalidation loss: 0.0572\t validation accuracy: 0.9822\n",
      "iteration number: 4823\t training loss: 0.0299\tvalidation loss: 0.0568\t validation accuracy: 0.9822\n",
      "iteration number: 4824\t training loss: 0.0300\tvalidation loss: 0.0575\t validation accuracy: 0.9822\n",
      "iteration number: 4825\t training loss: 0.0301\tvalidation loss: 0.0579\t validation accuracy: 0.9822\n",
      "iteration number: 4826\t training loss: 0.0306\tvalidation loss: 0.0587\t validation accuracy: 0.9822\n",
      "iteration number: 4827\t training loss: 0.0306\tvalidation loss: 0.0593\t validation accuracy: 0.9822\n",
      "iteration number: 4828\t training loss: 0.0303\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4829\t training loss: 0.0301\tvalidation loss: 0.0586\t validation accuracy: 0.9822\n",
      "iteration number: 4830\t training loss: 0.0301\tvalidation loss: 0.0588\t validation accuracy: 0.9800\n",
      "iteration number: 4831\t training loss: 0.0300\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4832\t training loss: 0.0304\tvalidation loss: 0.0591\t validation accuracy: 0.9800\n",
      "iteration number: 4833\t training loss: 0.0298\tvalidation loss: 0.0590\t validation accuracy: 0.9800\n",
      "iteration number: 4834\t training loss: 0.0299\tvalidation loss: 0.0594\t validation accuracy: 0.9800\n",
      "iteration number: 4835\t training loss: 0.0297\tvalidation loss: 0.0591\t validation accuracy: 0.9800\n",
      "iteration number: 4836\t training loss: 0.0302\tvalidation loss: 0.0591\t validation accuracy: 0.9800\n",
      "iteration number: 4837\t training loss: 0.0298\tvalidation loss: 0.0588\t validation accuracy: 0.9822\n",
      "iteration number: 4838\t training loss: 0.0296\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4839\t training loss: 0.0297\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4840\t training loss: 0.0296\tvalidation loss: 0.0592\t validation accuracy: 0.9822\n",
      "iteration number: 4841\t training loss: 0.0296\tvalidation loss: 0.0587\t validation accuracy: 0.9822\n",
      "iteration number: 4842\t training loss: 0.0303\tvalidation loss: 0.0600\t validation accuracy: 0.9822\n",
      "iteration number: 4843\t training loss: 0.0303\tvalidation loss: 0.0598\t validation accuracy: 0.9844\n",
      "iteration number: 4844\t training loss: 0.0303\tvalidation loss: 0.0596\t validation accuracy: 0.9822\n",
      "iteration number: 4845\t training loss: 0.0302\tvalidation loss: 0.0599\t validation accuracy: 0.9778\n",
      "iteration number: 4846\t training loss: 0.0298\tvalidation loss: 0.0594\t validation accuracy: 0.9800\n",
      "iteration number: 4847\t training loss: 0.0295\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4848\t training loss: 0.0296\tvalidation loss: 0.0586\t validation accuracy: 0.9822\n",
      "iteration number: 4849\t training loss: 0.0297\tvalidation loss: 0.0587\t validation accuracy: 0.9822\n",
      "iteration number: 4850\t training loss: 0.0296\tvalidation loss: 0.0585\t validation accuracy: 0.9822\n",
      "iteration number: 4851\t training loss: 0.0299\tvalidation loss: 0.0586\t validation accuracy: 0.9844\n",
      "iteration number: 4852\t training loss: 0.0300\tvalidation loss: 0.0588\t validation accuracy: 0.9844\n",
      "iteration number: 4853\t training loss: 0.0303\tvalidation loss: 0.0594\t validation accuracy: 0.9822\n",
      "iteration number: 4854\t training loss: 0.0296\tvalidation loss: 0.0584\t validation accuracy: 0.9844\n",
      "iteration number: 4855\t training loss: 0.0297\tvalidation loss: 0.0582\t validation accuracy: 0.9822\n",
      "iteration number: 4856\t training loss: 0.0297\tvalidation loss: 0.0582\t validation accuracy: 0.9822\n",
      "iteration number: 4857\t training loss: 0.0294\tvalidation loss: 0.0581\t validation accuracy: 0.9867\n",
      "iteration number: 4858\t training loss: 0.0294\tvalidation loss: 0.0579\t validation accuracy: 0.9867\n",
      "iteration number: 4859\t training loss: 0.0295\tvalidation loss: 0.0585\t validation accuracy: 0.9844\n",
      "iteration number: 4860\t training loss: 0.0294\tvalidation loss: 0.0581\t validation accuracy: 0.9844\n",
      "iteration number: 4861\t training loss: 0.0300\tvalidation loss: 0.0585\t validation accuracy: 0.9844\n",
      "iteration number: 4862\t training loss: 0.0306\tvalidation loss: 0.0585\t validation accuracy: 0.9844\n",
      "iteration number: 4863\t training loss: 0.0320\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4864\t training loss: 0.0325\tvalidation loss: 0.0592\t validation accuracy: 0.9822\n",
      "iteration number: 4865\t training loss: 0.0308\tvalidation loss: 0.0582\t validation accuracy: 0.9822\n",
      "iteration number: 4866\t training loss: 0.0299\tvalidation loss: 0.0592\t validation accuracy: 0.9822\n",
      "iteration number: 4867\t training loss: 0.0294\tvalidation loss: 0.0587\t validation accuracy: 0.9844\n",
      "iteration number: 4868\t training loss: 0.0295\tvalidation loss: 0.0590\t validation accuracy: 0.9844\n",
      "iteration number: 4869\t training loss: 0.0294\tvalidation loss: 0.0592\t validation accuracy: 0.9844\n",
      "iteration number: 4870\t training loss: 0.0295\tvalidation loss: 0.0595\t validation accuracy: 0.9800\n",
      "iteration number: 4871\t training loss: 0.0296\tvalidation loss: 0.0597\t validation accuracy: 0.9800\n",
      "iteration number: 4872\t training loss: 0.0305\tvalidation loss: 0.0618\t validation accuracy: 0.9778\n",
      "iteration number: 4873\t training loss: 0.0300\tvalidation loss: 0.0611\t validation accuracy: 0.9800\n",
      "iteration number: 4874\t training loss: 0.0298\tvalidation loss: 0.0606\t validation accuracy: 0.9800\n",
      "iteration number: 4875\t training loss: 0.0301\tvalidation loss: 0.0605\t validation accuracy: 0.9800\n",
      "iteration number: 4876\t training loss: 0.0293\tvalidation loss: 0.0587\t validation accuracy: 0.9800\n",
      "iteration number: 4877\t training loss: 0.0296\tvalidation loss: 0.0588\t validation accuracy: 0.9822\n",
      "iteration number: 4878\t training loss: 0.0307\tvalidation loss: 0.0588\t validation accuracy: 0.9822\n",
      "iteration number: 4879\t training loss: 0.0299\tvalidation loss: 0.0581\t validation accuracy: 0.9822\n",
      "iteration number: 4880\t training loss: 0.0298\tvalidation loss: 0.0583\t validation accuracy: 0.9822\n",
      "iteration number: 4881\t training loss: 0.0294\tvalidation loss: 0.0584\t validation accuracy: 0.9822\n",
      "iteration number: 4882\t training loss: 0.0291\tvalidation loss: 0.0580\t validation accuracy: 0.9844\n",
      "iteration number: 4883\t training loss: 0.0294\tvalidation loss: 0.0583\t validation accuracy: 0.9844\n",
      "iteration number: 4884\t training loss: 0.0293\tvalidation loss: 0.0583\t validation accuracy: 0.9844\n",
      "iteration number: 4885\t training loss: 0.0296\tvalidation loss: 0.0578\t validation accuracy: 0.9822\n",
      "iteration number: 4886\t training loss: 0.0305\tvalidation loss: 0.0583\t validation accuracy: 0.9822\n",
      "iteration number: 4887\t training loss: 0.0292\tvalidation loss: 0.0579\t validation accuracy: 0.9844\n",
      "iteration number: 4888\t training loss: 0.0292\tvalidation loss: 0.0584\t validation accuracy: 0.9844\n",
      "iteration number: 4889\t training loss: 0.0292\tvalidation loss: 0.0587\t validation accuracy: 0.9867\n",
      "iteration number: 4890\t training loss: 0.0290\tvalidation loss: 0.0588\t validation accuracy: 0.9800\n",
      "iteration number: 4891\t training loss: 0.0290\tvalidation loss: 0.0578\t validation accuracy: 0.9822\n",
      "iteration number: 4892\t training loss: 0.0290\tvalidation loss: 0.0581\t validation accuracy: 0.9822\n",
      "iteration number: 4893\t training loss: 0.0292\tvalidation loss: 0.0582\t validation accuracy: 0.9800\n",
      "iteration number: 4894\t training loss: 0.0292\tvalidation loss: 0.0588\t validation accuracy: 0.9800\n",
      "iteration number: 4895\t training loss: 0.0291\tvalidation loss: 0.0585\t validation accuracy: 0.9800\n",
      "iteration number: 4896\t training loss: 0.0292\tvalidation loss: 0.0588\t validation accuracy: 0.9778\n",
      "iteration number: 4897\t training loss: 0.0293\tvalidation loss: 0.0583\t validation accuracy: 0.9800\n",
      "iteration number: 4898\t training loss: 0.0294\tvalidation loss: 0.0579\t validation accuracy: 0.9800\n",
      "iteration number: 4899\t training loss: 0.0292\tvalidation loss: 0.0577\t validation accuracy: 0.9822\n",
      "iteration number: 4900\t training loss: 0.0303\tvalidation loss: 0.0578\t validation accuracy: 0.9800\n",
      "iteration number: 4901\t training loss: 0.0304\tvalidation loss: 0.0581\t validation accuracy: 0.9800\n",
      "iteration number: 4902\t training loss: 0.0301\tvalidation loss: 0.0579\t validation accuracy: 0.9800\n",
      "iteration number: 4903\t training loss: 0.0298\tvalidation loss: 0.0581\t validation accuracy: 0.9800\n",
      "iteration number: 4904\t training loss: 0.0296\tvalidation loss: 0.0582\t validation accuracy: 0.9800\n",
      "iteration number: 4905\t training loss: 0.0294\tvalidation loss: 0.0577\t validation accuracy: 0.9822\n",
      "iteration number: 4906\t training loss: 0.0294\tvalidation loss: 0.0578\t validation accuracy: 0.9822\n",
      "iteration number: 4907\t training loss: 0.0295\tvalidation loss: 0.0585\t validation accuracy: 0.9822\n",
      "iteration number: 4908\t training loss: 0.0294\tvalidation loss: 0.0587\t validation accuracy: 0.9822\n",
      "iteration number: 4909\t training loss: 0.0308\tvalidation loss: 0.0614\t validation accuracy: 0.9756\n",
      "iteration number: 4910\t training loss: 0.0302\tvalidation loss: 0.0598\t validation accuracy: 0.9778\n",
      "iteration number: 4911\t training loss: 0.0308\tvalidation loss: 0.0608\t validation accuracy: 0.9778\n",
      "iteration number: 4912\t training loss: 0.0298\tvalidation loss: 0.0592\t validation accuracy: 0.9800\n",
      "iteration number: 4913\t training loss: 0.0292\tvalidation loss: 0.0580\t validation accuracy: 0.9800\n",
      "iteration number: 4914\t training loss: 0.0290\tvalidation loss: 0.0576\t validation accuracy: 0.9822\n",
      "iteration number: 4915\t training loss: 0.0288\tvalidation loss: 0.0583\t validation accuracy: 0.9844\n",
      "iteration number: 4916\t training loss: 0.0288\tvalidation loss: 0.0578\t validation accuracy: 0.9844\n",
      "iteration number: 4917\t training loss: 0.0289\tvalidation loss: 0.0584\t validation accuracy: 0.9844\n",
      "iteration number: 4918\t training loss: 0.0290\tvalidation loss: 0.0590\t validation accuracy: 0.9844\n",
      "iteration number: 4919\t training loss: 0.0291\tvalidation loss: 0.0591\t validation accuracy: 0.9844\n",
      "iteration number: 4920\t training loss: 0.0291\tvalidation loss: 0.0594\t validation accuracy: 0.9844\n",
      "iteration number: 4921\t training loss: 0.0289\tvalidation loss: 0.0584\t validation accuracy: 0.9844\n",
      "iteration number: 4922\t training loss: 0.0288\tvalidation loss: 0.0576\t validation accuracy: 0.9844\n",
      "iteration number: 4923\t training loss: 0.0288\tvalidation loss: 0.0574\t validation accuracy: 0.9844\n",
      "iteration number: 4924\t training loss: 0.0289\tvalidation loss: 0.0571\t validation accuracy: 0.9844\n",
      "iteration number: 4925\t training loss: 0.0289\tvalidation loss: 0.0567\t validation accuracy: 0.9867\n",
      "iteration number: 4926\t training loss: 0.0296\tvalidation loss: 0.0566\t validation accuracy: 0.9867\n",
      "iteration number: 4927\t training loss: 0.0295\tvalidation loss: 0.0564\t validation accuracy: 0.9867\n",
      "iteration number: 4928\t training loss: 0.0294\tvalidation loss: 0.0569\t validation accuracy: 0.9844\n",
      "iteration number: 4929\t training loss: 0.0293\tvalidation loss: 0.0572\t validation accuracy: 0.9844\n",
      "iteration number: 4930\t training loss: 0.0292\tvalidation loss: 0.0581\t validation accuracy: 0.9844\n",
      "iteration number: 4931\t training loss: 0.0293\tvalidation loss: 0.0588\t validation accuracy: 0.9822\n",
      "iteration number: 4932\t training loss: 0.0292\tvalidation loss: 0.0582\t validation accuracy: 0.9844\n",
      "iteration number: 4933\t training loss: 0.0291\tvalidation loss: 0.0579\t validation accuracy: 0.9844\n",
      "iteration number: 4934\t training loss: 0.0292\tvalidation loss: 0.0592\t validation accuracy: 0.9822\n",
      "iteration number: 4935\t training loss: 0.0291\tvalidation loss: 0.0585\t validation accuracy: 0.9844\n",
      "iteration number: 4936\t training loss: 0.0290\tvalidation loss: 0.0589\t validation accuracy: 0.9822\n",
      "iteration number: 4937\t training loss: 0.0294\tvalidation loss: 0.0582\t validation accuracy: 0.9844\n",
      "iteration number: 4938\t training loss: 0.0292\tvalidation loss: 0.0584\t validation accuracy: 0.9844\n",
      "iteration number: 4939\t training loss: 0.0290\tvalidation loss: 0.0581\t validation accuracy: 0.9844\n",
      "iteration number: 4940\t training loss: 0.0289\tvalidation loss: 0.0581\t validation accuracy: 0.9844\n",
      "iteration number: 4941\t training loss: 0.0290\tvalidation loss: 0.0585\t validation accuracy: 0.9844\n",
      "iteration number: 4942\t training loss: 0.0290\tvalidation loss: 0.0586\t validation accuracy: 0.9844\n",
      "iteration number: 4943\t training loss: 0.0290\tvalidation loss: 0.0585\t validation accuracy: 0.9844\n",
      "iteration number: 4944\t training loss: 0.0293\tvalidation loss: 0.0578\t validation accuracy: 0.9844\n",
      "iteration number: 4945\t training loss: 0.0293\tvalidation loss: 0.0586\t validation accuracy: 0.9844\n",
      "iteration number: 4946\t training loss: 0.0293\tvalidation loss: 0.0587\t validation accuracy: 0.9822\n",
      "iteration number: 4947\t training loss: 0.0295\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4948\t training loss: 0.0295\tvalidation loss: 0.0587\t validation accuracy: 0.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4949\t training loss: 0.0296\tvalidation loss: 0.0590\t validation accuracy: 0.9800\n",
      "iteration number: 4950\t training loss: 0.0297\tvalidation loss: 0.0591\t validation accuracy: 0.9822\n",
      "iteration number: 4951\t training loss: 0.0294\tvalidation loss: 0.0585\t validation accuracy: 0.9844\n",
      "iteration number: 4952\t training loss: 0.0292\tvalidation loss: 0.0579\t validation accuracy: 0.9822\n",
      "iteration number: 4953\t training loss: 0.0293\tvalidation loss: 0.0580\t validation accuracy: 0.9822\n",
      "iteration number: 4954\t training loss: 0.0292\tvalidation loss: 0.0580\t validation accuracy: 0.9822\n",
      "iteration number: 4955\t training loss: 0.0292\tvalidation loss: 0.0577\t validation accuracy: 0.9844\n",
      "iteration number: 4956\t training loss: 0.0295\tvalidation loss: 0.0579\t validation accuracy: 0.9844\n",
      "iteration number: 4957\t training loss: 0.0294\tvalidation loss: 0.0578\t validation accuracy: 0.9844\n",
      "iteration number: 4958\t training loss: 0.0294\tvalidation loss: 0.0579\t validation accuracy: 0.9844\n",
      "iteration number: 4959\t training loss: 0.0292\tvalidation loss: 0.0580\t validation accuracy: 0.9844\n",
      "iteration number: 4960\t training loss: 0.0298\tvalidation loss: 0.0592\t validation accuracy: 0.9844\n",
      "iteration number: 4961\t training loss: 0.0300\tvalidation loss: 0.0604\t validation accuracy: 0.9822\n",
      "iteration number: 4962\t training loss: 0.0293\tvalidation loss: 0.0591\t validation accuracy: 0.9822\n",
      "iteration number: 4963\t training loss: 0.0300\tvalidation loss: 0.0606\t validation accuracy: 0.9800\n",
      "iteration number: 4964\t training loss: 0.0301\tvalidation loss: 0.0607\t validation accuracy: 0.9800\n",
      "iteration number: 4965\t training loss: 0.0297\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4966\t training loss: 0.0299\tvalidation loss: 0.0603\t validation accuracy: 0.9822\n",
      "iteration number: 4967\t training loss: 0.0306\tvalidation loss: 0.0609\t validation accuracy: 0.9800\n",
      "iteration number: 4968\t training loss: 0.0289\tvalidation loss: 0.0591\t validation accuracy: 0.9844\n",
      "iteration number: 4969\t training loss: 0.0288\tvalidation loss: 0.0593\t validation accuracy: 0.9822\n",
      "iteration number: 4970\t training loss: 0.0287\tvalidation loss: 0.0590\t validation accuracy: 0.9822\n",
      "iteration number: 4971\t training loss: 0.0293\tvalidation loss: 0.0599\t validation accuracy: 0.9822\n",
      "iteration number: 4972\t training loss: 0.0288\tvalidation loss: 0.0590\t validation accuracy: 0.9867\n",
      "iteration number: 4973\t training loss: 0.0289\tvalidation loss: 0.0590\t validation accuracy: 0.9867\n",
      "iteration number: 4974\t training loss: 0.0286\tvalidation loss: 0.0584\t validation accuracy: 0.9867\n",
      "iteration number: 4975\t training loss: 0.0287\tvalidation loss: 0.0586\t validation accuracy: 0.9867\n",
      "iteration number: 4976\t training loss: 0.0284\tvalidation loss: 0.0575\t validation accuracy: 0.9867\n",
      "iteration number: 4977\t training loss: 0.0284\tvalidation loss: 0.0579\t validation accuracy: 0.9867\n",
      "iteration number: 4978\t training loss: 0.0284\tvalidation loss: 0.0577\t validation accuracy: 0.9867\n",
      "iteration number: 4979\t training loss: 0.0284\tvalidation loss: 0.0578\t validation accuracy: 0.9867\n",
      "iteration number: 4980\t training loss: 0.0283\tvalidation loss: 0.0582\t validation accuracy: 0.9844\n",
      "iteration number: 4981\t training loss: 0.0286\tvalidation loss: 0.0574\t validation accuracy: 0.9867\n",
      "iteration number: 4982\t training loss: 0.0284\tvalidation loss: 0.0577\t validation accuracy: 0.9844\n",
      "iteration number: 4983\t training loss: 0.0284\tvalidation loss: 0.0578\t validation accuracy: 0.9844\n",
      "iteration number: 4984\t training loss: 0.0285\tvalidation loss: 0.0582\t validation accuracy: 0.9867\n",
      "iteration number: 4985\t training loss: 0.0289\tvalidation loss: 0.0600\t validation accuracy: 0.9844\n",
      "iteration number: 4986\t training loss: 0.0289\tvalidation loss: 0.0600\t validation accuracy: 0.9844\n",
      "iteration number: 4987\t training loss: 0.0290\tvalidation loss: 0.0606\t validation accuracy: 0.9822\n",
      "iteration number: 4988\t training loss: 0.0291\tvalidation loss: 0.0607\t validation accuracy: 0.9800\n",
      "iteration number: 4989\t training loss: 0.0305\tvalidation loss: 0.0635\t validation accuracy: 0.9778\n",
      "iteration number: 4990\t training loss: 0.0308\tvalidation loss: 0.0633\t validation accuracy: 0.9778\n",
      "iteration number: 4991\t training loss: 0.0301\tvalidation loss: 0.0621\t validation accuracy: 0.9822\n",
      "iteration number: 4992\t training loss: 0.0302\tvalidation loss: 0.0621\t validation accuracy: 0.9800\n",
      "iteration number: 4993\t training loss: 0.0288\tvalidation loss: 0.0597\t validation accuracy: 0.9844\n",
      "iteration number: 4994\t training loss: 0.0295\tvalidation loss: 0.0606\t validation accuracy: 0.9844\n",
      "iteration number: 4995\t training loss: 0.0298\tvalidation loss: 0.0607\t validation accuracy: 0.9844\n",
      "iteration number: 4996\t training loss: 0.0293\tvalidation loss: 0.0597\t validation accuracy: 0.9867\n",
      "iteration number: 4997\t training loss: 0.0293\tvalidation loss: 0.0591\t validation accuracy: 0.9867\n",
      "iteration number: 4998\t training loss: 0.0291\tvalidation loss: 0.0582\t validation accuracy: 0.9867\n",
      "iteration number: 4999\t training loss: 0.0291\tvalidation loss: 0.0577\t validation accuracy: 0.9867\n",
      "iteration number: 5000\t training loss: 0.0292\tvalidation loss: 0.0581\t validation accuracy: 0.9867\n"
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptron(X, Y, hidden_size=50, activation='relu')\n",
    "mlp.train(vectorized=True, early_stopping=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "#### - Did you succeed to train the MLP and get a high validation accuracy? <br> Display available metrics (training and validation accuracies, training and validation losses)\n",
    "#### - Plot the prediction for a given validation sample. Is it accurate?\n",
    "#### - Compare the full gradient descent with the SGD.\n",
    "#### - Play with the hyper parameters you have: the hidden size, the activation function, the initial step and the batch size. <br> Comment. Don't hesitate to visualize results.\n",
    "#### - Once properly implemented, compare the training using early stopping, dropout, or both of them. <br> Why are these methods useful here?\n",
    "<span style=\"color:green\">\n",
    "Early stopping is useful to make training time shorter and returning a model that performs well on validation set. In case no improvement on validation set has been noticed for many iterations, a reasonable assumption is that any more iteration will only worsen the model on validation set. Thus traning is stopped and model returned as in its last state.\n",
    "</span>\n",
    "\n",
    "#### - Once properly implemented, compare the training using momentum.\n",
    "<span style=\"color:green\">\n",
    "The optimizer converges much faster, reaching $50\\%$ in validation accuracy after $\\approx42$ iteration only while it took $\\approx 185$ iterations without it.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â MLP with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\t training loss: 2.3025\tvalidation loss: 2.3027\t validation accuracy: 0.1600\n",
      "iteration number: 2\t training loss: 2.3025\tvalidation loss: 2.3026\t validation accuracy: 0.0978\n",
      "iteration number: 3\t training loss: 2.3025\tvalidation loss: 2.3027\t validation accuracy: 0.0844\n",
      "iteration number: 4\t training loss: 2.3024\tvalidation loss: 2.3026\t validation accuracy: 0.0978\n",
      "iteration number: 5\t training loss: 2.3024\tvalidation loss: 2.3026\t validation accuracy: 0.0978\n",
      "iteration number: 6\t training loss: 2.3024\tvalidation loss: 2.3026\t validation accuracy: 0.0978\n",
      "iteration number: 7\t training loss: 2.3024\tvalidation loss: 2.3027\t validation accuracy: 0.0978\n",
      "iteration number: 8\t training loss: 2.3023\tvalidation loss: 2.3026\t validation accuracy: 0.1733\n",
      "iteration number: 9\t training loss: 2.3023\tvalidation loss: 2.3026\t validation accuracy: 0.0978\n",
      "iteration number: 10\t training loss: 2.3022\tvalidation loss: 2.3027\t validation accuracy: 0.1111\n",
      "iteration number: 11\t training loss: 2.3022\tvalidation loss: 2.3026\t validation accuracy: 0.1111\n",
      "iteration number: 12\t training loss: 2.3021\tvalidation loss: 2.3026\t validation accuracy: 0.1111\n",
      "iteration number: 13\t training loss: 2.3020\tvalidation loss: 2.3028\t validation accuracy: 0.0911\n",
      "iteration number: 14\t training loss: 2.3020\tvalidation loss: 2.3029\t validation accuracy: 0.0911\n",
      "iteration number: 15\t training loss: 2.3019\tvalidation loss: 2.3029\t validation accuracy: 0.0911\n",
      "iteration number: 16\t training loss: 2.3019\tvalidation loss: 2.3029\t validation accuracy: 0.0911\n",
      "iteration number: 17\t training loss: 2.3018\tvalidation loss: 2.3030\t validation accuracy: 0.0911\n",
      "iteration number: 18\t training loss: 2.3018\tvalidation loss: 2.3030\t validation accuracy: 0.0911\n",
      "iteration number: 19\t training loss: 2.3017\tvalidation loss: 2.3029\t validation accuracy: 0.0911\n",
      "iteration number: 20\t training loss: 2.3017\tvalidation loss: 2.3029\t validation accuracy: 0.0911\n",
      "iteration number: 21\t training loss: 2.3016\tvalidation loss: 2.3030\t validation accuracy: 0.0978\n",
      "iteration number: 22\t training loss: 2.3016\tvalidation loss: 2.3030\t validation accuracy: 0.0978\n",
      "iteration number: 23\t training loss: 2.3016\tvalidation loss: 2.3029\t validation accuracy: 0.0978\n",
      "iteration number: 24\t training loss: 2.3016\tvalidation loss: 2.3030\t validation accuracy: 0.0978\n",
      "iteration number: 25\t training loss: 2.3014\tvalidation loss: 2.3032\t validation accuracy: 0.0978\n",
      "iteration number: 26\t training loss: 2.3013\tvalidation loss: 2.3036\t validation accuracy: 0.0978\n",
      "iteration number: 27\t training loss: 2.3012\tvalidation loss: 2.3036\t validation accuracy: 0.0978\n",
      "iteration number: 28\t training loss: 2.3011\tvalidation loss: 2.3037\t validation accuracy: 0.1644\n",
      "iteration number: 29\t training loss: 2.3011\tvalidation loss: 2.3039\t validation accuracy: 0.0844\n",
      "iteration number: 30\t training loss: 2.3010\tvalidation loss: 2.3038\t validation accuracy: 0.0844\n",
      "iteration number: 31\t training loss: 2.3010\tvalidation loss: 2.3038\t validation accuracy: 0.0844\n",
      "iteration number: 32\t training loss: 2.3009\tvalidation loss: 2.3040\t validation accuracy: 0.0844\n",
      "iteration number: 33\t training loss: 2.3009\tvalidation loss: 2.3038\t validation accuracy: 0.0844\n",
      "iteration number: 34\t training loss: 2.3008\tvalidation loss: 2.3038\t validation accuracy: 0.0844\n",
      "iteration number: 35\t training loss: 2.3007\tvalidation loss: 2.3038\t validation accuracy: 0.0844\n",
      "iteration number: 36\t training loss: 2.3006\tvalidation loss: 2.3036\t validation accuracy: 0.0844\n",
      "iteration number: 37\t training loss: 2.3006\tvalidation loss: 2.3033\t validation accuracy: 0.0911\n",
      "iteration number: 38\t training loss: 2.3005\tvalidation loss: 2.3034\t validation accuracy: 0.0911\n",
      "iteration number: 39\t training loss: 2.3004\tvalidation loss: 2.3031\t validation accuracy: 0.0911\n",
      "iteration number: 40\t training loss: 2.3004\tvalidation loss: 2.3029\t validation accuracy: 0.0911\n",
      "iteration number: 41\t training loss: 2.3003\tvalidation loss: 2.3027\t validation accuracy: 0.0911\n",
      "iteration number: 42\t training loss: 2.3002\tvalidation loss: 2.3027\t validation accuracy: 0.0911\n",
      "iteration number: 43\t training loss: 2.3001\tvalidation loss: 2.3026\t validation accuracy: 0.0911\n",
      "iteration number: 44\t training loss: 2.3000\tvalidation loss: 2.3025\t validation accuracy: 0.0911\n",
      "iteration number: 45\t training loss: 2.3000\tvalidation loss: 2.3023\t validation accuracy: 0.0911\n",
      "iteration number: 46\t training loss: 2.2999\tvalidation loss: 2.3021\t validation accuracy: 0.0911\n",
      "iteration number: 47\t training loss: 2.2997\tvalidation loss: 2.3020\t validation accuracy: 0.0911\n",
      "iteration number: 48\t training loss: 2.2996\tvalidation loss: 2.3020\t validation accuracy: 0.0911\n",
      "iteration number: 49\t training loss: 2.2995\tvalidation loss: 2.3018\t validation accuracy: 0.0911\n",
      "iteration number: 50\t training loss: 2.2994\tvalidation loss: 2.3018\t validation accuracy: 0.0911\n",
      "iteration number: 51\t training loss: 2.2994\tvalidation loss: 2.3019\t validation accuracy: 0.0911\n",
      "iteration number: 52\t training loss: 2.2993\tvalidation loss: 2.3018\t validation accuracy: 0.0911\n",
      "iteration number: 53\t training loss: 2.2991\tvalidation loss: 2.3020\t validation accuracy: 0.0911\n",
      "iteration number: 54\t training loss: 2.2989\tvalidation loss: 2.3019\t validation accuracy: 0.0911\n",
      "iteration number: 55\t training loss: 2.2988\tvalidation loss: 2.3018\t validation accuracy: 0.0911\n",
      "iteration number: 56\t training loss: 2.2987\tvalidation loss: 2.3017\t validation accuracy: 0.0911\n",
      "iteration number: 57\t training loss: 2.2985\tvalidation loss: 2.3017\t validation accuracy: 0.0911\n",
      "iteration number: 58\t training loss: 2.2984\tvalidation loss: 2.3014\t validation accuracy: 0.0911\n",
      "iteration number: 59\t training loss: 2.2982\tvalidation loss: 2.3013\t validation accuracy: 0.0911\n",
      "iteration number: 60\t training loss: 2.2980\tvalidation loss: 2.3011\t validation accuracy: 0.0911\n",
      "iteration number: 61\t training loss: 2.2979\tvalidation loss: 2.3009\t validation accuracy: 0.0911\n",
      "iteration number: 62\t training loss: 2.2977\tvalidation loss: 2.3009\t validation accuracy: 0.0911\n",
      "iteration number: 63\t training loss: 2.2976\tvalidation loss: 2.3006\t validation accuracy: 0.0911\n",
      "iteration number: 64\t training loss: 2.2974\tvalidation loss: 2.3004\t validation accuracy: 0.0911\n",
      "iteration number: 65\t training loss: 2.2972\tvalidation loss: 2.3002\t validation accuracy: 0.0911\n",
      "iteration number: 66\t training loss: 2.2970\tvalidation loss: 2.3002\t validation accuracy: 0.0911\n",
      "iteration number: 67\t training loss: 2.2967\tvalidation loss: 2.2999\t validation accuracy: 0.0911\n",
      "iteration number: 68\t training loss: 2.2966\tvalidation loss: 2.2998\t validation accuracy: 0.0911\n",
      "iteration number: 69\t training loss: 2.2964\tvalidation loss: 2.2997\t validation accuracy: 0.0911\n",
      "iteration number: 70\t training loss: 2.2961\tvalidation loss: 2.2996\t validation accuracy: 0.0911\n",
      "iteration number: 71\t training loss: 2.2959\tvalidation loss: 2.2996\t validation accuracy: 0.0911\n",
      "iteration number: 72\t training loss: 2.2957\tvalidation loss: 2.2993\t validation accuracy: 0.0911\n",
      "iteration number: 73\t training loss: 2.2954\tvalidation loss: 2.2990\t validation accuracy: 0.0911\n",
      "iteration number: 74\t training loss: 2.2951\tvalidation loss: 2.2986\t validation accuracy: 0.0911\n",
      "iteration number: 75\t training loss: 2.2948\tvalidation loss: 2.2986\t validation accuracy: 0.0911\n",
      "iteration number: 76\t training loss: 2.2945\tvalidation loss: 2.2984\t validation accuracy: 0.0911\n",
      "iteration number: 77\t training loss: 2.2942\tvalidation loss: 2.2979\t validation accuracy: 0.0911\n",
      "iteration number: 78\t training loss: 2.2939\tvalidation loss: 2.2975\t validation accuracy: 0.0911\n",
      "iteration number: 79\t training loss: 2.2936\tvalidation loss: 2.2972\t validation accuracy: 0.0911\n",
      "iteration number: 80\t training loss: 2.2933\tvalidation loss: 2.2970\t validation accuracy: 0.0911\n",
      "iteration number: 81\t training loss: 2.2929\tvalidation loss: 2.2967\t validation accuracy: 0.0911\n",
      "iteration number: 82\t training loss: 2.2925\tvalidation loss: 2.2964\t validation accuracy: 0.0911\n",
      "iteration number: 83\t training loss: 2.2922\tvalidation loss: 2.2962\t validation accuracy: 0.0911\n",
      "iteration number: 84\t training loss: 2.2918\tvalidation loss: 2.2955\t validation accuracy: 0.0911\n",
      "iteration number: 85\t training loss: 2.2913\tvalidation loss: 2.2949\t validation accuracy: 0.0933\n",
      "iteration number: 86\t training loss: 2.2908\tvalidation loss: 2.2944\t validation accuracy: 0.1711\n",
      "iteration number: 87\t training loss: 2.2902\tvalidation loss: 2.2941\t validation accuracy: 0.2622\n",
      "iteration number: 88\t training loss: 2.2898\tvalidation loss: 2.2933\t validation accuracy: 0.2600\n",
      "iteration number: 89\t training loss: 2.2893\tvalidation loss: 2.2929\t validation accuracy: 0.2622\n",
      "iteration number: 90\t training loss: 2.2888\tvalidation loss: 2.2925\t validation accuracy: 0.2667\n",
      "iteration number: 91\t training loss: 2.2884\tvalidation loss: 2.2920\t validation accuracy: 0.2156\n",
      "iteration number: 92\t training loss: 2.2878\tvalidation loss: 2.2918\t validation accuracy: 0.2289\n",
      "iteration number: 93\t training loss: 2.2873\tvalidation loss: 2.2912\t validation accuracy: 0.1622\n",
      "iteration number: 94\t training loss: 2.2867\tvalidation loss: 2.2905\t validation accuracy: 0.1711\n",
      "iteration number: 95\t training loss: 2.2861\tvalidation loss: 2.2899\t validation accuracy: 0.1800\n",
      "iteration number: 96\t training loss: 2.2854\tvalidation loss: 2.2889\t validation accuracy: 0.2356\n",
      "iteration number: 97\t training loss: 2.2847\tvalidation loss: 2.2885\t validation accuracy: 0.2556\n",
      "iteration number: 98\t training loss: 2.2840\tvalidation loss: 2.2878\t validation accuracy: 0.2911\n",
      "iteration number: 99\t training loss: 2.2833\tvalidation loss: 2.2871\t validation accuracy: 0.2822\n",
      "iteration number: 100\t training loss: 2.2828\tvalidation loss: 2.2867\t validation accuracy: 0.1444\n",
      "iteration number: 101\t training loss: 2.2821\tvalidation loss: 2.2857\t validation accuracy: 0.1511\n",
      "iteration number: 102\t training loss: 2.2812\tvalidation loss: 2.2849\t validation accuracy: 0.1756\n",
      "iteration number: 103\t training loss: 2.2803\tvalidation loss: 2.2836\t validation accuracy: 0.2244\n",
      "iteration number: 104\t training loss: 2.2795\tvalidation loss: 2.2827\t validation accuracy: 0.2511\n",
      "iteration number: 105\t training loss: 2.2786\tvalidation loss: 2.2817\t validation accuracy: 0.2156\n",
      "iteration number: 106\t training loss: 2.2776\tvalidation loss: 2.2812\t validation accuracy: 0.1956\n",
      "iteration number: 107\t training loss: 2.2767\tvalidation loss: 2.2802\t validation accuracy: 0.1911\n",
      "iteration number: 108\t training loss: 2.2756\tvalidation loss: 2.2792\t validation accuracy: 0.2578\n",
      "iteration number: 109\t training loss: 2.2748\tvalidation loss: 2.2781\t validation accuracy: 0.2267\n",
      "iteration number: 110\t training loss: 2.2735\tvalidation loss: 2.2774\t validation accuracy: 0.2711\n",
      "iteration number: 111\t training loss: 2.2726\tvalidation loss: 2.2764\t validation accuracy: 0.3244\n",
      "iteration number: 112\t training loss: 2.2714\tvalidation loss: 2.2753\t validation accuracy: 0.3489\n",
      "iteration number: 113\t training loss: 2.2701\tvalidation loss: 2.2743\t validation accuracy: 0.3467\n",
      "iteration number: 114\t training loss: 2.2688\tvalidation loss: 2.2734\t validation accuracy: 0.2667\n",
      "iteration number: 115\t training loss: 2.2676\tvalidation loss: 2.2721\t validation accuracy: 0.2600\n",
      "iteration number: 116\t training loss: 2.2663\tvalidation loss: 2.2705\t validation accuracy: 0.2533\n",
      "iteration number: 117\t training loss: 2.2650\tvalidation loss: 2.2692\t validation accuracy: 0.2600\n",
      "iteration number: 118\t training loss: 2.2634\tvalidation loss: 2.2676\t validation accuracy: 0.1911\n",
      "iteration number: 119\t training loss: 2.2617\tvalidation loss: 2.2663\t validation accuracy: 0.1889\n",
      "iteration number: 120\t training loss: 2.2600\tvalidation loss: 2.2646\t validation accuracy: 0.1956\n",
      "iteration number: 121\t training loss: 2.2583\tvalidation loss: 2.2631\t validation accuracy: 0.2444\n",
      "iteration number: 122\t training loss: 2.2567\tvalidation loss: 2.2616\t validation accuracy: 0.2600\n",
      "iteration number: 123\t training loss: 2.2551\tvalidation loss: 2.2601\t validation accuracy: 0.3022\n",
      "iteration number: 124\t training loss: 2.2532\tvalidation loss: 2.2582\t validation accuracy: 0.3333\n",
      "iteration number: 125\t training loss: 2.2510\tvalidation loss: 2.2562\t validation accuracy: 0.3444\n",
      "iteration number: 126\t training loss: 2.2493\tvalidation loss: 2.2547\t validation accuracy: 0.3533\n",
      "iteration number: 127\t training loss: 2.2469\tvalidation loss: 2.2524\t validation accuracy: 0.3889\n",
      "iteration number: 128\t training loss: 2.2443\tvalidation loss: 2.2500\t validation accuracy: 0.3600\n",
      "iteration number: 129\t training loss: 2.2421\tvalidation loss: 2.2476\t validation accuracy: 0.3689\n",
      "iteration number: 130\t training loss: 2.2399\tvalidation loss: 2.2451\t validation accuracy: 0.3956\n",
      "iteration number: 131\t training loss: 2.2372\tvalidation loss: 2.2429\t validation accuracy: 0.3844\n",
      "iteration number: 132\t training loss: 2.2343\tvalidation loss: 2.2403\t validation accuracy: 0.3800\n",
      "iteration number: 133\t training loss: 2.2313\tvalidation loss: 2.2377\t validation accuracy: 0.3511\n",
      "iteration number: 134\t training loss: 2.2284\tvalidation loss: 2.2355\t validation accuracy: 0.4111\n",
      "iteration number: 135\t training loss: 2.2259\tvalidation loss: 2.2327\t validation accuracy: 0.4267\n",
      "iteration number: 136\t training loss: 2.2233\tvalidation loss: 2.2305\t validation accuracy: 0.4600\n",
      "iteration number: 137\t training loss: 2.2196\tvalidation loss: 2.2272\t validation accuracy: 0.4511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 138\t training loss: 2.2163\tvalidation loss: 2.2245\t validation accuracy: 0.4622\n",
      "iteration number: 139\t training loss: 2.2132\tvalidation loss: 2.2215\t validation accuracy: 0.4578\n",
      "iteration number: 140\t training loss: 2.2093\tvalidation loss: 2.2177\t validation accuracy: 0.4444\n",
      "iteration number: 141\t training loss: 2.2061\tvalidation loss: 2.2148\t validation accuracy: 0.4489\n",
      "iteration number: 142\t training loss: 2.2022\tvalidation loss: 2.2110\t validation accuracy: 0.4444\n",
      "iteration number: 143\t training loss: 2.1987\tvalidation loss: 2.2070\t validation accuracy: 0.4289\n",
      "iteration number: 144\t training loss: 2.1947\tvalidation loss: 2.2035\t validation accuracy: 0.4267\n",
      "iteration number: 145\t training loss: 2.1911\tvalidation loss: 2.1997\t validation accuracy: 0.4444\n",
      "iteration number: 146\t training loss: 2.1865\tvalidation loss: 2.1956\t validation accuracy: 0.4289\n",
      "iteration number: 147\t training loss: 2.1820\tvalidation loss: 2.1906\t validation accuracy: 0.4156\n",
      "iteration number: 148\t training loss: 2.1774\tvalidation loss: 2.1857\t validation accuracy: 0.3756\n",
      "iteration number: 149\t training loss: 2.1727\tvalidation loss: 2.1813\t validation accuracy: 0.3600\n",
      "iteration number: 150\t training loss: 2.1681\tvalidation loss: 2.1767\t validation accuracy: 0.3600\n",
      "iteration number: 151\t training loss: 2.1631\tvalidation loss: 2.1720\t validation accuracy: 0.3600\n",
      "iteration number: 152\t training loss: 2.1574\tvalidation loss: 2.1679\t validation accuracy: 0.4311\n",
      "iteration number: 153\t training loss: 2.1519\tvalidation loss: 2.1624\t validation accuracy: 0.4133\n",
      "iteration number: 154\t training loss: 2.1462\tvalidation loss: 2.1577\t validation accuracy: 0.3911\n",
      "iteration number: 155\t training loss: 2.1402\tvalidation loss: 2.1518\t validation accuracy: 0.3956\n",
      "iteration number: 156\t training loss: 2.1343\tvalidation loss: 2.1464\t validation accuracy: 0.4067\n",
      "iteration number: 157\t training loss: 2.1290\tvalidation loss: 2.1413\t validation accuracy: 0.4133\n",
      "iteration number: 158\t training loss: 2.1225\tvalidation loss: 2.1359\t validation accuracy: 0.4444\n",
      "iteration number: 159\t training loss: 2.1169\tvalidation loss: 2.1303\t validation accuracy: 0.4400\n",
      "iteration number: 160\t training loss: 2.1098\tvalidation loss: 2.1241\t validation accuracy: 0.4400\n",
      "iteration number: 161\t training loss: 2.1036\tvalidation loss: 2.1176\t validation accuracy: 0.4267\n",
      "iteration number: 162\t training loss: 2.0957\tvalidation loss: 2.1105\t validation accuracy: 0.4178\n",
      "iteration number: 163\t training loss: 2.0886\tvalidation loss: 2.1041\t validation accuracy: 0.4244\n",
      "iteration number: 164\t training loss: 2.0815\tvalidation loss: 2.0973\t validation accuracy: 0.4156\n",
      "iteration number: 165\t training loss: 2.0740\tvalidation loss: 2.0907\t validation accuracy: 0.4200\n",
      "iteration number: 166\t training loss: 2.0674\tvalidation loss: 2.0847\t validation accuracy: 0.4422\n",
      "iteration number: 167\t training loss: 2.0590\tvalidation loss: 2.0773\t validation accuracy: 0.4400\n",
      "iteration number: 168\t training loss: 2.0515\tvalidation loss: 2.0702\t validation accuracy: 0.4333\n",
      "iteration number: 169\t training loss: 2.0444\tvalidation loss: 2.0623\t validation accuracy: 0.4356\n",
      "iteration number: 170\t training loss: 2.0370\tvalidation loss: 2.0554\t validation accuracy: 0.4511\n",
      "iteration number: 171\t training loss: 2.0278\tvalidation loss: 2.0487\t validation accuracy: 0.4600\n",
      "iteration number: 172\t training loss: 2.0191\tvalidation loss: 2.0407\t validation accuracy: 0.4622\n",
      "iteration number: 173\t training loss: 2.0118\tvalidation loss: 2.0326\t validation accuracy: 0.4644\n",
      "iteration number: 174\t training loss: 2.0042\tvalidation loss: 2.0265\t validation accuracy: 0.4733\n",
      "iteration number: 175\t training loss: 1.9944\tvalidation loss: 2.0165\t validation accuracy: 0.4622\n",
      "iteration number: 176\t training loss: 1.9850\tvalidation loss: 2.0071\t validation accuracy: 0.4600\n",
      "iteration number: 177\t training loss: 1.9764\tvalidation loss: 1.9980\t validation accuracy: 0.4511\n",
      "iteration number: 178\t training loss: 1.9670\tvalidation loss: 1.9890\t validation accuracy: 0.4378\n",
      "iteration number: 179\t training loss: 1.9565\tvalidation loss: 1.9796\t validation accuracy: 0.4533\n",
      "iteration number: 180\t training loss: 1.9467\tvalidation loss: 1.9718\t validation accuracy: 0.4578\n",
      "iteration number: 181\t training loss: 1.9369\tvalidation loss: 1.9617\t validation accuracy: 0.4711\n",
      "iteration number: 182\t training loss: 1.9270\tvalidation loss: 1.9527\t validation accuracy: 0.4889\n",
      "iteration number: 183\t training loss: 1.9179\tvalidation loss: 1.9448\t validation accuracy: 0.4689\n",
      "iteration number: 184\t training loss: 1.9081\tvalidation loss: 1.9349\t validation accuracy: 0.4733\n",
      "iteration number: 185\t training loss: 1.8982\tvalidation loss: 1.9253\t validation accuracy: 0.4756\n",
      "iteration number: 186\t training loss: 1.8884\tvalidation loss: 1.9175\t validation accuracy: 0.4822\n",
      "iteration number: 187\t training loss: 1.8764\tvalidation loss: 1.9075\t validation accuracy: 0.4711\n",
      "iteration number: 188\t training loss: 1.8658\tvalidation loss: 1.8967\t validation accuracy: 0.4800\n",
      "iteration number: 189\t training loss: 1.8551\tvalidation loss: 1.8884\t validation accuracy: 0.4844\n",
      "iteration number: 190\t training loss: 1.8429\tvalidation loss: 1.8754\t validation accuracy: 0.4756\n",
      "iteration number: 191\t training loss: 1.8331\tvalidation loss: 1.8663\t validation accuracy: 0.5089\n",
      "iteration number: 192\t training loss: 1.8224\tvalidation loss: 1.8573\t validation accuracy: 0.5133\n",
      "iteration number: 193\t training loss: 1.8138\tvalidation loss: 1.8477\t validation accuracy: 0.5156\n",
      "iteration number: 194\t training loss: 1.8015\tvalidation loss: 1.8365\t validation accuracy: 0.4978\n",
      "iteration number: 195\t training loss: 1.7895\tvalidation loss: 1.8263\t validation accuracy: 0.5067\n",
      "iteration number: 196\t training loss: 1.7771\tvalidation loss: 1.8162\t validation accuracy: 0.5244\n",
      "iteration number: 197\t training loss: 1.7663\tvalidation loss: 1.8043\t validation accuracy: 0.5311\n",
      "iteration number: 198\t training loss: 1.7543\tvalidation loss: 1.7939\t validation accuracy: 0.5533\n",
      "iteration number: 199\t training loss: 1.7444\tvalidation loss: 1.7842\t validation accuracy: 0.5467\n",
      "iteration number: 200\t training loss: 1.7329\tvalidation loss: 1.7767\t validation accuracy: 0.5422\n",
      "iteration number: 201\t training loss: 1.7201\tvalidation loss: 1.7641\t validation accuracy: 0.5667\n",
      "iteration number: 202\t training loss: 1.7078\tvalidation loss: 1.7515\t validation accuracy: 0.5711\n",
      "iteration number: 203\t training loss: 1.6972\tvalidation loss: 1.7423\t validation accuracy: 0.5778\n",
      "iteration number: 204\t training loss: 1.6845\tvalidation loss: 1.7299\t validation accuracy: 0.5933\n",
      "iteration number: 205\t training loss: 1.6734\tvalidation loss: 1.7173\t validation accuracy: 0.6000\n",
      "iteration number: 206\t training loss: 1.6617\tvalidation loss: 1.7071\t validation accuracy: 0.5978\n",
      "iteration number: 207\t training loss: 1.6517\tvalidation loss: 1.6993\t validation accuracy: 0.6111\n",
      "iteration number: 208\t training loss: 1.6396\tvalidation loss: 1.6876\t validation accuracy: 0.6133\n",
      "iteration number: 209\t training loss: 1.6290\tvalidation loss: 1.6749\t validation accuracy: 0.6089\n",
      "iteration number: 210\t training loss: 1.6181\tvalidation loss: 1.6623\t validation accuracy: 0.5911\n",
      "iteration number: 211\t training loss: 1.6052\tvalidation loss: 1.6521\t validation accuracy: 0.6111\n",
      "iteration number: 212\t training loss: 1.5944\tvalidation loss: 1.6410\t validation accuracy: 0.6244\n",
      "iteration number: 213\t training loss: 1.5807\tvalidation loss: 1.6314\t validation accuracy: 0.5978\n",
      "iteration number: 214\t training loss: 1.5707\tvalidation loss: 1.6194\t validation accuracy: 0.6044\n",
      "iteration number: 215\t training loss: 1.5597\tvalidation loss: 1.6091\t validation accuracy: 0.6044\n",
      "iteration number: 216\t training loss: 1.5490\tvalidation loss: 1.5970\t validation accuracy: 0.6133\n",
      "iteration number: 217\t training loss: 1.5361\tvalidation loss: 1.5869\t validation accuracy: 0.6200\n",
      "iteration number: 218\t training loss: 1.5255\tvalidation loss: 1.5754\t validation accuracy: 0.6244\n",
      "iteration number: 219\t training loss: 1.5133\tvalidation loss: 1.5643\t validation accuracy: 0.5956\n",
      "iteration number: 220\t training loss: 1.5027\tvalidation loss: 1.5521\t validation accuracy: 0.6244\n",
      "iteration number: 221\t training loss: 1.4914\tvalidation loss: 1.5423\t validation accuracy: 0.5867\n",
      "iteration number: 222\t training loss: 1.4784\tvalidation loss: 1.5310\t validation accuracy: 0.6267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 223\t training loss: 1.4659\tvalidation loss: 1.5215\t validation accuracy: 0.6578\n",
      "iteration number: 224\t training loss: 1.4554\tvalidation loss: 1.5118\t validation accuracy: 0.6267\n",
      "iteration number: 225\t training loss: 1.4430\tvalidation loss: 1.5026\t validation accuracy: 0.6333\n",
      "iteration number: 226\t training loss: 1.4314\tvalidation loss: 1.4912\t validation accuracy: 0.6489\n",
      "iteration number: 227\t training loss: 1.4217\tvalidation loss: 1.4806\t validation accuracy: 0.6778\n",
      "iteration number: 228\t training loss: 1.4104\tvalidation loss: 1.4705\t validation accuracy: 0.6778\n",
      "iteration number: 229\t training loss: 1.3994\tvalidation loss: 1.4603\t validation accuracy: 0.6578\n",
      "iteration number: 230\t training loss: 1.3898\tvalidation loss: 1.4499\t validation accuracy: 0.6511\n",
      "iteration number: 231\t training loss: 1.3781\tvalidation loss: 1.4419\t validation accuracy: 0.6622\n",
      "iteration number: 232\t training loss: 1.3683\tvalidation loss: 1.4319\t validation accuracy: 0.6733\n",
      "iteration number: 233\t training loss: 1.3581\tvalidation loss: 1.4247\t validation accuracy: 0.6778\n",
      "iteration number: 234\t training loss: 1.3482\tvalidation loss: 1.4163\t validation accuracy: 0.6733\n",
      "iteration number: 235\t training loss: 1.3358\tvalidation loss: 1.4032\t validation accuracy: 0.6622\n",
      "iteration number: 236\t training loss: 1.3270\tvalidation loss: 1.3949\t validation accuracy: 0.6622\n",
      "iteration number: 237\t training loss: 1.3152\tvalidation loss: 1.3860\t validation accuracy: 0.6600\n",
      "iteration number: 238\t training loss: 1.3066\tvalidation loss: 1.3771\t validation accuracy: 0.6378\n",
      "iteration number: 239\t training loss: 1.2940\tvalidation loss: 1.3634\t validation accuracy: 0.6600\n",
      "iteration number: 240\t training loss: 1.2834\tvalidation loss: 1.3545\t validation accuracy: 0.6356\n",
      "iteration number: 241\t training loss: 1.2762\tvalidation loss: 1.3506\t validation accuracy: 0.6778\n",
      "iteration number: 242\t training loss: 1.2640\tvalidation loss: 1.3370\t validation accuracy: 0.6911\n",
      "iteration number: 243\t training loss: 1.2537\tvalidation loss: 1.3264\t validation accuracy: 0.6778\n",
      "iteration number: 244\t training loss: 1.2446\tvalidation loss: 1.3220\t validation accuracy: 0.6556\n",
      "iteration number: 245\t training loss: 1.2360\tvalidation loss: 1.3124\t validation accuracy: 0.6267\n",
      "iteration number: 246\t training loss: 1.2271\tvalidation loss: 1.3052\t validation accuracy: 0.6244\n",
      "iteration number: 247\t training loss: 1.2199\tvalidation loss: 1.2992\t validation accuracy: 0.6222\n",
      "iteration number: 248\t training loss: 1.2069\tvalidation loss: 1.2868\t validation accuracy: 0.6889\n",
      "iteration number: 249\t training loss: 1.1960\tvalidation loss: 1.2742\t validation accuracy: 0.6733\n",
      "iteration number: 250\t training loss: 1.1868\tvalidation loss: 1.2645\t validation accuracy: 0.6644\n",
      "iteration number: 251\t training loss: 1.1781\tvalidation loss: 1.2565\t validation accuracy: 0.6844\n",
      "iteration number: 252\t training loss: 1.1712\tvalidation loss: 1.2496\t validation accuracy: 0.6867\n",
      "iteration number: 253\t training loss: 1.1629\tvalidation loss: 1.2368\t validation accuracy: 0.7000\n",
      "iteration number: 254\t training loss: 1.1514\tvalidation loss: 1.2264\t validation accuracy: 0.7089\n",
      "iteration number: 255\t training loss: 1.1488\tvalidation loss: 1.2244\t validation accuracy: 0.6889\n",
      "iteration number: 256\t training loss: 1.1353\tvalidation loss: 1.2115\t validation accuracy: 0.7000\n",
      "iteration number: 257\t training loss: 1.1256\tvalidation loss: 1.2043\t validation accuracy: 0.7200\n",
      "iteration number: 258\t training loss: 1.1168\tvalidation loss: 1.1925\t validation accuracy: 0.7111\n",
      "iteration number: 259\t training loss: 1.1069\tvalidation loss: 1.1864\t validation accuracy: 0.6956\n",
      "iteration number: 260\t training loss: 1.0989\tvalidation loss: 1.1791\t validation accuracy: 0.7133\n",
      "iteration number: 261\t training loss: 1.0892\tvalidation loss: 1.1700\t validation accuracy: 0.7267\n",
      "iteration number: 262\t training loss: 1.0795\tvalidation loss: 1.1614\t validation accuracy: 0.7511\n",
      "iteration number: 263\t training loss: 1.0724\tvalidation loss: 1.1556\t validation accuracy: 0.7578\n",
      "iteration number: 264\t training loss: 1.0669\tvalidation loss: 1.1472\t validation accuracy: 0.7222\n",
      "iteration number: 265\t training loss: 1.0572\tvalidation loss: 1.1361\t validation accuracy: 0.7378\n",
      "iteration number: 266\t training loss: 1.0504\tvalidation loss: 1.1274\t validation accuracy: 0.7178\n",
      "iteration number: 267\t training loss: 1.0391\tvalidation loss: 1.1168\t validation accuracy: 0.7356\n",
      "iteration number: 268\t training loss: 1.0318\tvalidation loss: 1.1127\t validation accuracy: 0.7356\n",
      "iteration number: 269\t training loss: 1.0240\tvalidation loss: 1.1070\t validation accuracy: 0.7244\n",
      "iteration number: 270\t training loss: 1.0155\tvalidation loss: 1.1003\t validation accuracy: 0.7400\n",
      "iteration number: 271\t training loss: 1.0061\tvalidation loss: 1.0902\t validation accuracy: 0.7489\n",
      "iteration number: 272\t training loss: 0.9985\tvalidation loss: 1.0839\t validation accuracy: 0.7489\n",
      "iteration number: 273\t training loss: 0.9919\tvalidation loss: 1.0728\t validation accuracy: 0.7511\n",
      "iteration number: 274\t training loss: 0.9862\tvalidation loss: 1.0687\t validation accuracy: 0.7756\n",
      "iteration number: 275\t training loss: 0.9777\tvalidation loss: 1.0592\t validation accuracy: 0.7844\n",
      "iteration number: 276\t training loss: 0.9734\tvalidation loss: 1.0563\t validation accuracy: 0.7467\n",
      "iteration number: 277\t training loss: 0.9714\tvalidation loss: 1.0545\t validation accuracy: 0.7311\n",
      "iteration number: 278\t training loss: 0.9601\tvalidation loss: 1.0496\t validation accuracy: 0.7556\n",
      "iteration number: 279\t training loss: 0.9556\tvalidation loss: 1.0478\t validation accuracy: 0.7378\n",
      "iteration number: 280\t training loss: 0.9484\tvalidation loss: 1.0396\t validation accuracy: 0.7689\n",
      "iteration number: 281\t training loss: 0.9440\tvalidation loss: 1.0342\t validation accuracy: 0.7733\n",
      "iteration number: 282\t training loss: 0.9352\tvalidation loss: 1.0232\t validation accuracy: 0.7622\n",
      "iteration number: 283\t training loss: 0.9293\tvalidation loss: 1.0143\t validation accuracy: 0.7733\n",
      "iteration number: 284\t training loss: 0.9200\tvalidation loss: 1.0037\t validation accuracy: 0.7644\n",
      "iteration number: 285\t training loss: 0.9171\tvalidation loss: 0.9989\t validation accuracy: 0.7489\n",
      "iteration number: 286\t training loss: 0.9084\tvalidation loss: 0.9887\t validation accuracy: 0.7556\n",
      "iteration number: 287\t training loss: 0.9048\tvalidation loss: 0.9811\t validation accuracy: 0.7733\n",
      "iteration number: 288\t training loss: 0.8977\tvalidation loss: 0.9778\t validation accuracy: 0.7844\n",
      "iteration number: 289\t training loss: 0.8926\tvalidation loss: 0.9777\t validation accuracy: 0.7822\n",
      "iteration number: 290\t training loss: 0.8836\tvalidation loss: 0.9699\t validation accuracy: 0.8000\n",
      "iteration number: 291\t training loss: 0.8819\tvalidation loss: 0.9672\t validation accuracy: 0.7911\n",
      "iteration number: 292\t training loss: 0.8669\tvalidation loss: 0.9559\t validation accuracy: 0.7911\n",
      "iteration number: 293\t training loss: 0.8611\tvalidation loss: 0.9488\t validation accuracy: 0.7933\n",
      "iteration number: 294\t training loss: 0.8572\tvalidation loss: 0.9416\t validation accuracy: 0.8044\n",
      "iteration number: 295\t training loss: 0.8529\tvalidation loss: 0.9364\t validation accuracy: 0.7933\n",
      "iteration number: 296\t training loss: 0.8479\tvalidation loss: 0.9326\t validation accuracy: 0.7822\n",
      "iteration number: 297\t training loss: 0.8434\tvalidation loss: 0.9263\t validation accuracy: 0.7844\n",
      "iteration number: 298\t training loss: 0.8419\tvalidation loss: 0.9254\t validation accuracy: 0.7689\n",
      "iteration number: 299\t training loss: 0.8407\tvalidation loss: 0.9262\t validation accuracy: 0.7600\n",
      "iteration number: 300\t training loss: 0.8378\tvalidation loss: 0.9248\t validation accuracy: 0.7644\n",
      "iteration number: 301\t training loss: 0.8241\tvalidation loss: 0.9115\t validation accuracy: 0.7844\n",
      "iteration number: 302\t training loss: 0.8143\tvalidation loss: 0.9055\t validation accuracy: 0.8067\n",
      "iteration number: 303\t training loss: 0.8090\tvalidation loss: 0.8986\t validation accuracy: 0.8022\n",
      "iteration number: 304\t training loss: 0.8031\tvalidation loss: 0.8884\t validation accuracy: 0.8000\n",
      "iteration number: 305\t training loss: 0.7993\tvalidation loss: 0.8858\t validation accuracy: 0.8133\n",
      "iteration number: 306\t training loss: 0.7941\tvalidation loss: 0.8800\t validation accuracy: 0.7844\n",
      "iteration number: 307\t training loss: 0.7855\tvalidation loss: 0.8721\t validation accuracy: 0.7844\n",
      "iteration number: 308\t training loss: 0.7797\tvalidation loss: 0.8673\t validation accuracy: 0.7889\n",
      "iteration number: 309\t training loss: 0.7730\tvalidation loss: 0.8617\t validation accuracy: 0.7911\n",
      "iteration number: 310\t training loss: 0.7674\tvalidation loss: 0.8584\t validation accuracy: 0.7844\n",
      "iteration number: 311\t training loss: 0.7623\tvalidation loss: 0.8539\t validation accuracy: 0.7867\n",
      "iteration number: 312\t training loss: 0.7600\tvalidation loss: 0.8511\t validation accuracy: 0.7867\n",
      "iteration number: 313\t training loss: 0.7618\tvalidation loss: 0.8560\t validation accuracy: 0.7778\n",
      "iteration number: 314\t training loss: 0.7512\tvalidation loss: 0.8408\t validation accuracy: 0.8044\n",
      "iteration number: 315\t training loss: 0.7465\tvalidation loss: 0.8330\t validation accuracy: 0.8289\n",
      "iteration number: 316\t training loss: 0.7444\tvalidation loss: 0.8370\t validation accuracy: 0.8244\n",
      "iteration number: 317\t training loss: 0.7415\tvalidation loss: 0.8303\t validation accuracy: 0.8178\n",
      "iteration number: 318\t training loss: 0.7363\tvalidation loss: 0.8259\t validation accuracy: 0.8178\n",
      "iteration number: 319\t training loss: 0.7288\tvalidation loss: 0.8207\t validation accuracy: 0.8200\n",
      "iteration number: 320\t training loss: 0.7211\tvalidation loss: 0.8122\t validation accuracy: 0.8356\n",
      "iteration number: 321\t training loss: 0.7156\tvalidation loss: 0.8077\t validation accuracy: 0.8289\n",
      "iteration number: 322\t training loss: 0.7123\tvalidation loss: 0.8057\t validation accuracy: 0.8356\n",
      "iteration number: 323\t training loss: 0.7094\tvalidation loss: 0.8057\t validation accuracy: 0.8044\n",
      "iteration number: 324\t training loss: 0.7059\tvalidation loss: 0.8052\t validation accuracy: 0.8111\n",
      "iteration number: 325\t training loss: 0.6999\tvalidation loss: 0.7985\t validation accuracy: 0.8044\n",
      "iteration number: 326\t training loss: 0.6953\tvalidation loss: 0.7933\t validation accuracy: 0.8044\n",
      "iteration number: 327\t training loss: 0.6951\tvalidation loss: 0.7909\t validation accuracy: 0.8067\n",
      "iteration number: 328\t training loss: 0.6886\tvalidation loss: 0.7828\t validation accuracy: 0.8089\n",
      "iteration number: 329\t training loss: 0.6815\tvalidation loss: 0.7760\t validation accuracy: 0.8022\n",
      "iteration number: 330\t training loss: 0.6772\tvalidation loss: 0.7700\t validation accuracy: 0.7911\n",
      "iteration number: 331\t training loss: 0.6766\tvalidation loss: 0.7730\t validation accuracy: 0.7956\n",
      "iteration number: 332\t training loss: 0.6709\tvalidation loss: 0.7670\t validation accuracy: 0.7933\n",
      "iteration number: 333\t training loss: 0.6717\tvalidation loss: 0.7671\t validation accuracy: 0.7933\n",
      "iteration number: 334\t training loss: 0.6647\tvalidation loss: 0.7620\t validation accuracy: 0.8022\n",
      "iteration number: 335\t training loss: 0.6598\tvalidation loss: 0.7536\t validation accuracy: 0.8089\n",
      "iteration number: 336\t training loss: 0.6607\tvalidation loss: 0.7526\t validation accuracy: 0.8089\n",
      "iteration number: 337\t training loss: 0.6547\tvalidation loss: 0.7464\t validation accuracy: 0.8111\n",
      "iteration number: 338\t training loss: 0.6482\tvalidation loss: 0.7412\t validation accuracy: 0.8222\n",
      "iteration number: 339\t training loss: 0.6455\tvalidation loss: 0.7367\t validation accuracy: 0.8222\n",
      "iteration number: 340\t training loss: 0.6379\tvalidation loss: 0.7315\t validation accuracy: 0.8311\n",
      "iteration number: 341\t training loss: 0.6422\tvalidation loss: 0.7371\t validation accuracy: 0.8244\n",
      "iteration number: 342\t training loss: 0.6338\tvalidation loss: 0.7289\t validation accuracy: 0.8244\n",
      "iteration number: 343\t training loss: 0.6313\tvalidation loss: 0.7239\t validation accuracy: 0.8356\n",
      "iteration number: 344\t training loss: 0.6251\tvalidation loss: 0.7171\t validation accuracy: 0.8400\n",
      "iteration number: 345\t training loss: 0.6233\tvalidation loss: 0.7116\t validation accuracy: 0.8422\n",
      "iteration number: 346\t training loss: 0.6260\tvalidation loss: 0.7174\t validation accuracy: 0.8289\n",
      "iteration number: 347\t training loss: 0.6165\tvalidation loss: 0.7064\t validation accuracy: 0.8333\n",
      "iteration number: 348\t training loss: 0.6148\tvalidation loss: 0.7050\t validation accuracy: 0.8444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 349\t training loss: 0.6085\tvalidation loss: 0.6993\t validation accuracy: 0.8511\n",
      "iteration number: 350\t training loss: 0.6057\tvalidation loss: 0.6967\t validation accuracy: 0.8422\n",
      "iteration number: 351\t training loss: 0.6015\tvalidation loss: 0.6938\t validation accuracy: 0.8400\n",
      "iteration number: 352\t training loss: 0.6009\tvalidation loss: 0.6915\t validation accuracy: 0.8467\n",
      "iteration number: 353\t training loss: 0.5980\tvalidation loss: 0.6895\t validation accuracy: 0.8467\n",
      "iteration number: 354\t training loss: 0.5948\tvalidation loss: 0.6899\t validation accuracy: 0.8467\n",
      "iteration number: 355\t training loss: 0.5898\tvalidation loss: 0.6847\t validation accuracy: 0.8422\n",
      "iteration number: 356\t training loss: 0.5888\tvalidation loss: 0.6791\t validation accuracy: 0.8378\n",
      "iteration number: 357\t training loss: 0.5853\tvalidation loss: 0.6738\t validation accuracy: 0.8511\n",
      "iteration number: 358\t training loss: 0.5824\tvalidation loss: 0.6762\t validation accuracy: 0.8422\n",
      "iteration number: 359\t training loss: 0.5771\tvalidation loss: 0.6663\t validation accuracy: 0.8422\n",
      "iteration number: 360\t training loss: 0.5767\tvalidation loss: 0.6693\t validation accuracy: 0.8378\n",
      "iteration number: 361\t training loss: 0.5768\tvalidation loss: 0.6707\t validation accuracy: 0.8556\n",
      "iteration number: 362\t training loss: 0.5708\tvalidation loss: 0.6638\t validation accuracy: 0.8400\n",
      "iteration number: 363\t training loss: 0.5642\tvalidation loss: 0.6508\t validation accuracy: 0.8467\n",
      "iteration number: 364\t training loss: 0.5641\tvalidation loss: 0.6536\t validation accuracy: 0.8422\n",
      "iteration number: 365\t training loss: 0.5636\tvalidation loss: 0.6564\t validation accuracy: 0.8489\n",
      "iteration number: 366\t training loss: 0.5592\tvalidation loss: 0.6538\t validation accuracy: 0.8444\n",
      "iteration number: 367\t training loss: 0.5543\tvalidation loss: 0.6494\t validation accuracy: 0.8489\n",
      "iteration number: 368\t training loss: 0.5522\tvalidation loss: 0.6476\t validation accuracy: 0.8511\n",
      "iteration number: 369\t training loss: 0.5497\tvalidation loss: 0.6447\t validation accuracy: 0.8533\n",
      "iteration number: 370\t training loss: 0.5476\tvalidation loss: 0.6443\t validation accuracy: 0.8444\n",
      "iteration number: 371\t training loss: 0.5437\tvalidation loss: 0.6372\t validation accuracy: 0.8511\n",
      "iteration number: 372\t training loss: 0.5399\tvalidation loss: 0.6337\t validation accuracy: 0.8489\n",
      "iteration number: 373\t training loss: 0.5423\tvalidation loss: 0.6372\t validation accuracy: 0.8444\n",
      "iteration number: 374\t training loss: 0.5395\tvalidation loss: 0.6324\t validation accuracy: 0.8467\n",
      "iteration number: 375\t training loss: 0.5405\tvalidation loss: 0.6317\t validation accuracy: 0.8622\n",
      "iteration number: 376\t training loss: 0.5322\tvalidation loss: 0.6163\t validation accuracy: 0.8622\n",
      "iteration number: 377\t training loss: 0.5307\tvalidation loss: 0.6192\t validation accuracy: 0.8644\n",
      "iteration number: 378\t training loss: 0.5280\tvalidation loss: 0.6131\t validation accuracy: 0.8622\n",
      "iteration number: 379\t training loss: 0.5249\tvalidation loss: 0.6084\t validation accuracy: 0.8489\n",
      "iteration number: 380\t training loss: 0.5260\tvalidation loss: 0.6065\t validation accuracy: 0.8533\n",
      "iteration number: 381\t training loss: 0.5171\tvalidation loss: 0.6018\t validation accuracy: 0.8556\n",
      "iteration number: 382\t training loss: 0.5181\tvalidation loss: 0.6014\t validation accuracy: 0.8511\n",
      "iteration number: 383\t training loss: 0.5154\tvalidation loss: 0.6023\t validation accuracy: 0.8533\n",
      "iteration number: 384\t training loss: 0.5160\tvalidation loss: 0.6045\t validation accuracy: 0.8511\n",
      "iteration number: 385\t training loss: 0.5149\tvalidation loss: 0.6077\t validation accuracy: 0.8511\n",
      "iteration number: 386\t training loss: 0.5077\tvalidation loss: 0.5979\t validation accuracy: 0.8489\n",
      "iteration number: 387\t training loss: 0.5052\tvalidation loss: 0.5933\t validation accuracy: 0.8556\n",
      "iteration number: 388\t training loss: 0.5027\tvalidation loss: 0.5889\t validation accuracy: 0.8578\n",
      "iteration number: 389\t training loss: 0.5048\tvalidation loss: 0.5920\t validation accuracy: 0.8556\n",
      "iteration number: 390\t training loss: 0.4989\tvalidation loss: 0.5854\t validation accuracy: 0.8600\n",
      "iteration number: 391\t training loss: 0.4963\tvalidation loss: 0.5868\t validation accuracy: 0.8578\n",
      "iteration number: 392\t training loss: 0.4927\tvalidation loss: 0.5815\t validation accuracy: 0.8533\n",
      "iteration number: 393\t training loss: 0.4897\tvalidation loss: 0.5840\t validation accuracy: 0.8556\n",
      "iteration number: 394\t training loss: 0.4915\tvalidation loss: 0.5875\t validation accuracy: 0.8556\n",
      "iteration number: 395\t training loss: 0.4882\tvalidation loss: 0.5786\t validation accuracy: 0.8622\n",
      "iteration number: 396\t training loss: 0.4874\tvalidation loss: 0.5781\t validation accuracy: 0.8622\n",
      "iteration number: 397\t training loss: 0.4819\tvalidation loss: 0.5724\t validation accuracy: 0.8644\n",
      "iteration number: 398\t training loss: 0.4814\tvalidation loss: 0.5731\t validation accuracy: 0.8667\n",
      "iteration number: 399\t training loss: 0.4786\tvalidation loss: 0.5683\t validation accuracy: 0.8600\n",
      "iteration number: 400\t training loss: 0.4768\tvalidation loss: 0.5692\t validation accuracy: 0.8600\n",
      "iteration number: 401\t training loss: 0.4781\tvalidation loss: 0.5697\t validation accuracy: 0.8556\n",
      "iteration number: 402\t training loss: 0.4719\tvalidation loss: 0.5641\t validation accuracy: 0.8556\n",
      "iteration number: 403\t training loss: 0.4757\tvalidation loss: 0.5651\t validation accuracy: 0.8578\n",
      "iteration number: 404\t training loss: 0.4735\tvalidation loss: 0.5619\t validation accuracy: 0.8622\n",
      "iteration number: 405\t training loss: 0.4675\tvalidation loss: 0.5561\t validation accuracy: 0.8533\n",
      "iteration number: 406\t training loss: 0.4687\tvalidation loss: 0.5578\t validation accuracy: 0.8533\n",
      "iteration number: 407\t training loss: 0.4621\tvalidation loss: 0.5536\t validation accuracy: 0.8622\n",
      "iteration number: 408\t training loss: 0.4641\tvalidation loss: 0.5563\t validation accuracy: 0.8533\n",
      "iteration number: 409\t training loss: 0.4581\tvalidation loss: 0.5486\t validation accuracy: 0.8644\n",
      "iteration number: 410\t training loss: 0.4559\tvalidation loss: 0.5473\t validation accuracy: 0.8711\n",
      "iteration number: 411\t training loss: 0.4537\tvalidation loss: 0.5429\t validation accuracy: 0.8667\n",
      "iteration number: 412\t training loss: 0.4532\tvalidation loss: 0.5423\t validation accuracy: 0.8622\n",
      "iteration number: 413\t training loss: 0.4535\tvalidation loss: 0.5424\t validation accuracy: 0.8533\n",
      "iteration number: 414\t training loss: 0.4500\tvalidation loss: 0.5373\t validation accuracy: 0.8644\n",
      "iteration number: 415\t training loss: 0.4460\tvalidation loss: 0.5339\t validation accuracy: 0.8689\n",
      "iteration number: 416\t training loss: 0.4512\tvalidation loss: 0.5426\t validation accuracy: 0.8533\n",
      "iteration number: 417\t training loss: 0.4452\tvalidation loss: 0.5349\t validation accuracy: 0.8622\n",
      "iteration number: 418\t training loss: 0.4463\tvalidation loss: 0.5345\t validation accuracy: 0.8556\n",
      "iteration number: 419\t training loss: 0.4431\tvalidation loss: 0.5289\t validation accuracy: 0.8622\n",
      "iteration number: 420\t training loss: 0.4420\tvalidation loss: 0.5294\t validation accuracy: 0.8644\n",
      "iteration number: 421\t training loss: 0.4393\tvalidation loss: 0.5242\t validation accuracy: 0.8667\n",
      "iteration number: 422\t training loss: 0.4345\tvalidation loss: 0.5196\t validation accuracy: 0.8800\n",
      "iteration number: 423\t training loss: 0.4331\tvalidation loss: 0.5165\t validation accuracy: 0.8733\n",
      "iteration number: 424\t training loss: 0.4334\tvalidation loss: 0.5190\t validation accuracy: 0.8733\n",
      "iteration number: 425\t training loss: 0.4333\tvalidation loss: 0.5221\t validation accuracy: 0.8711\n",
      "iteration number: 426\t training loss: 0.4293\tvalidation loss: 0.5176\t validation accuracy: 0.8733\n",
      "iteration number: 427\t training loss: 0.4255\tvalidation loss: 0.5135\t validation accuracy: 0.8711\n",
      "iteration number: 428\t training loss: 0.4239\tvalidation loss: 0.5100\t validation accuracy: 0.8689\n",
      "iteration number: 429\t training loss: 0.4254\tvalidation loss: 0.5129\t validation accuracy: 0.8667\n",
      "iteration number: 430\t training loss: 0.4249\tvalidation loss: 0.5124\t validation accuracy: 0.8600\n",
      "iteration number: 431\t training loss: 0.4201\tvalidation loss: 0.5067\t validation accuracy: 0.8689\n",
      "iteration number: 432\t training loss: 0.4237\tvalidation loss: 0.5133\t validation accuracy: 0.8689\n",
      "iteration number: 433\t training loss: 0.4213\tvalidation loss: 0.5103\t validation accuracy: 0.8756\n",
      "iteration number: 434\t training loss: 0.4184\tvalidation loss: 0.5072\t validation accuracy: 0.8667\n",
      "iteration number: 435\t training loss: 0.4159\tvalidation loss: 0.5037\t validation accuracy: 0.8711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 436\t training loss: 0.4140\tvalidation loss: 0.5025\t validation accuracy: 0.8689\n",
      "iteration number: 437\t training loss: 0.4132\tvalidation loss: 0.5002\t validation accuracy: 0.8733\n",
      "iteration number: 438\t training loss: 0.4117\tvalidation loss: 0.4980\t validation accuracy: 0.8667\n",
      "iteration number: 439\t training loss: 0.4108\tvalidation loss: 0.5002\t validation accuracy: 0.8667\n",
      "iteration number: 440\t training loss: 0.4102\tvalidation loss: 0.5005\t validation accuracy: 0.8711\n",
      "iteration number: 441\t training loss: 0.4052\tvalidation loss: 0.4929\t validation accuracy: 0.8689\n",
      "iteration number: 442\t training loss: 0.4027\tvalidation loss: 0.4886\t validation accuracy: 0.8711\n",
      "iteration number: 443\t training loss: 0.4053\tvalidation loss: 0.4914\t validation accuracy: 0.8756\n",
      "iteration number: 444\t training loss: 0.3998\tvalidation loss: 0.4847\t validation accuracy: 0.8733\n",
      "iteration number: 445\t training loss: 0.4068\tvalidation loss: 0.4920\t validation accuracy: 0.8800\n",
      "iteration number: 446\t training loss: 0.4012\tvalidation loss: 0.4839\t validation accuracy: 0.8889\n",
      "iteration number: 447\t training loss: 0.4037\tvalidation loss: 0.4869\t validation accuracy: 0.8778\n",
      "iteration number: 448\t training loss: 0.4001\tvalidation loss: 0.4831\t validation accuracy: 0.8800\n",
      "iteration number: 449\t training loss: 0.3972\tvalidation loss: 0.4776\t validation accuracy: 0.8689\n",
      "iteration number: 450\t training loss: 0.3972\tvalidation loss: 0.4774\t validation accuracy: 0.8689\n",
      "iteration number: 451\t training loss: 0.4008\tvalidation loss: 0.4839\t validation accuracy: 0.8689\n",
      "iteration number: 452\t training loss: 0.3978\tvalidation loss: 0.4830\t validation accuracy: 0.8578\n",
      "iteration number: 453\t training loss: 0.3973\tvalidation loss: 0.4838\t validation accuracy: 0.8600\n",
      "iteration number: 454\t training loss: 0.3940\tvalidation loss: 0.4788\t validation accuracy: 0.8711\n",
      "iteration number: 455\t training loss: 0.3896\tvalidation loss: 0.4751\t validation accuracy: 0.8644\n",
      "iteration number: 456\t training loss: 0.3857\tvalidation loss: 0.4692\t validation accuracy: 0.8711\n",
      "iteration number: 457\t training loss: 0.3864\tvalidation loss: 0.4703\t validation accuracy: 0.8578\n",
      "iteration number: 458\t training loss: 0.3827\tvalidation loss: 0.4663\t validation accuracy: 0.8689\n",
      "iteration number: 459\t training loss: 0.3813\tvalidation loss: 0.4647\t validation accuracy: 0.8667\n",
      "iteration number: 460\t training loss: 0.3834\tvalidation loss: 0.4647\t validation accuracy: 0.8667\n",
      "iteration number: 461\t training loss: 0.3795\tvalidation loss: 0.4603\t validation accuracy: 0.8778\n",
      "iteration number: 462\t training loss: 0.3791\tvalidation loss: 0.4609\t validation accuracy: 0.8689\n",
      "iteration number: 463\t training loss: 0.3761\tvalidation loss: 0.4557\t validation accuracy: 0.8800\n",
      "iteration number: 464\t training loss: 0.3757\tvalidation loss: 0.4577\t validation accuracy: 0.8778\n",
      "iteration number: 465\t training loss: 0.3749\tvalidation loss: 0.4563\t validation accuracy: 0.8756\n",
      "iteration number: 466\t training loss: 0.3731\tvalidation loss: 0.4558\t validation accuracy: 0.8800\n",
      "iteration number: 467\t training loss: 0.3710\tvalidation loss: 0.4538\t validation accuracy: 0.8822\n",
      "iteration number: 468\t training loss: 0.3725\tvalidation loss: 0.4520\t validation accuracy: 0.8800\n",
      "iteration number: 469\t training loss: 0.3715\tvalidation loss: 0.4529\t validation accuracy: 0.8756\n",
      "iteration number: 470\t training loss: 0.3672\tvalidation loss: 0.4484\t validation accuracy: 0.8867\n",
      "iteration number: 471\t training loss: 0.3686\tvalidation loss: 0.4504\t validation accuracy: 0.8800\n",
      "iteration number: 472\t training loss: 0.3663\tvalidation loss: 0.4489\t validation accuracy: 0.8822\n",
      "iteration number: 473\t training loss: 0.3649\tvalidation loss: 0.4461\t validation accuracy: 0.8822\n",
      "iteration number: 474\t training loss: 0.3626\tvalidation loss: 0.4446\t validation accuracy: 0.8867\n",
      "iteration number: 475\t training loss: 0.3608\tvalidation loss: 0.4455\t validation accuracy: 0.8844\n",
      "iteration number: 476\t training loss: 0.3587\tvalidation loss: 0.4432\t validation accuracy: 0.8800\n",
      "iteration number: 477\t training loss: 0.3601\tvalidation loss: 0.4476\t validation accuracy: 0.8844\n",
      "iteration number: 478\t training loss: 0.3597\tvalidation loss: 0.4473\t validation accuracy: 0.8867\n",
      "iteration number: 479\t training loss: 0.3602\tvalidation loss: 0.4487\t validation accuracy: 0.8800\n",
      "iteration number: 480\t training loss: 0.3598\tvalidation loss: 0.4470\t validation accuracy: 0.8756\n",
      "iteration number: 481\t training loss: 0.3550\tvalidation loss: 0.4420\t validation accuracy: 0.8778\n",
      "iteration number: 482\t training loss: 0.3527\tvalidation loss: 0.4383\t validation accuracy: 0.8800\n",
      "iteration number: 483\t training loss: 0.3521\tvalidation loss: 0.4345\t validation accuracy: 0.8800\n",
      "iteration number: 484\t training loss: 0.3509\tvalidation loss: 0.4341\t validation accuracy: 0.8867\n",
      "iteration number: 485\t training loss: 0.3497\tvalidation loss: 0.4334\t validation accuracy: 0.8844\n",
      "iteration number: 486\t training loss: 0.3482\tvalidation loss: 0.4320\t validation accuracy: 0.8867\n",
      "iteration number: 487\t training loss: 0.3480\tvalidation loss: 0.4306\t validation accuracy: 0.8822\n",
      "iteration number: 488\t training loss: 0.3458\tvalidation loss: 0.4307\t validation accuracy: 0.8778\n",
      "iteration number: 489\t training loss: 0.3479\tvalidation loss: 0.4349\t validation accuracy: 0.8844\n",
      "iteration number: 490\t training loss: 0.3436\tvalidation loss: 0.4266\t validation accuracy: 0.8822\n",
      "iteration number: 491\t training loss: 0.3446\tvalidation loss: 0.4258\t validation accuracy: 0.8867\n",
      "iteration number: 492\t training loss: 0.3455\tvalidation loss: 0.4258\t validation accuracy: 0.8889\n",
      "iteration number: 493\t training loss: 0.3429\tvalidation loss: 0.4254\t validation accuracy: 0.8867\n",
      "iteration number: 494\t training loss: 0.3421\tvalidation loss: 0.4245\t validation accuracy: 0.8956\n",
      "iteration number: 495\t training loss: 0.3450\tvalidation loss: 0.4262\t validation accuracy: 0.8844\n",
      "iteration number: 496\t training loss: 0.3411\tvalidation loss: 0.4268\t validation accuracy: 0.8867\n",
      "iteration number: 497\t training loss: 0.3421\tvalidation loss: 0.4276\t validation accuracy: 0.8822\n",
      "iteration number: 498\t training loss: 0.3415\tvalidation loss: 0.4284\t validation accuracy: 0.8844\n",
      "iteration number: 499\t training loss: 0.3379\tvalidation loss: 0.4249\t validation accuracy: 0.8867\n",
      "iteration number: 500\t training loss: 0.3369\tvalidation loss: 0.4219\t validation accuracy: 0.8867\n",
      "iteration number: 501\t training loss: 0.3371\tvalidation loss: 0.4245\t validation accuracy: 0.8889\n",
      "iteration number: 502\t training loss: 0.3329\tvalidation loss: 0.4160\t validation accuracy: 0.8956\n",
      "iteration number: 503\t training loss: 0.3322\tvalidation loss: 0.4149\t validation accuracy: 0.8956\n",
      "iteration number: 504\t training loss: 0.3320\tvalidation loss: 0.4158\t validation accuracy: 0.8911\n",
      "iteration number: 505\t training loss: 0.3308\tvalidation loss: 0.4140\t validation accuracy: 0.8867\n",
      "iteration number: 506\t training loss: 0.3316\tvalidation loss: 0.4156\t validation accuracy: 0.8867\n",
      "iteration number: 507\t training loss: 0.3296\tvalidation loss: 0.4107\t validation accuracy: 0.8933\n",
      "iteration number: 508\t training loss: 0.3299\tvalidation loss: 0.4125\t validation accuracy: 0.8844\n",
      "iteration number: 509\t training loss: 0.3327\tvalidation loss: 0.4164\t validation accuracy: 0.8889\n",
      "iteration number: 510\t training loss: 0.3308\tvalidation loss: 0.4150\t validation accuracy: 0.8867\n",
      "iteration number: 511\t training loss: 0.3281\tvalidation loss: 0.4110\t validation accuracy: 0.8867\n",
      "iteration number: 512\t training loss: 0.3284\tvalidation loss: 0.4110\t validation accuracy: 0.8867\n",
      "iteration number: 513\t training loss: 0.3299\tvalidation loss: 0.4143\t validation accuracy: 0.8867\n",
      "iteration number: 514\t training loss: 0.3290\tvalidation loss: 0.4122\t validation accuracy: 0.8844\n",
      "iteration number: 515\t training loss: 0.3219\tvalidation loss: 0.4048\t validation accuracy: 0.8889\n",
      "iteration number: 516\t training loss: 0.3203\tvalidation loss: 0.4021\t validation accuracy: 0.8933\n",
      "iteration number: 517\t training loss: 0.3206\tvalidation loss: 0.4030\t validation accuracy: 0.8933\n",
      "iteration number: 518\t training loss: 0.3190\tvalidation loss: 0.4024\t validation accuracy: 0.8933\n",
      "iteration number: 519\t training loss: 0.3203\tvalidation loss: 0.4043\t validation accuracy: 0.8956\n",
      "iteration number: 520\t training loss: 0.3223\tvalidation loss: 0.4063\t validation accuracy: 0.8911\n",
      "iteration number: 521\t training loss: 0.3192\tvalidation loss: 0.4034\t validation accuracy: 0.8889\n",
      "iteration number: 522\t training loss: 0.3185\tvalidation loss: 0.4031\t validation accuracy: 0.8889\n",
      "iteration number: 523\t training loss: 0.3200\tvalidation loss: 0.4054\t validation accuracy: 0.8844\n",
      "iteration number: 524\t training loss: 0.3190\tvalidation loss: 0.4002\t validation accuracy: 0.8933\n",
      "iteration number: 525\t training loss: 0.3219\tvalidation loss: 0.4005\t validation accuracy: 0.8911\n",
      "iteration number: 526\t training loss: 0.3148\tvalidation loss: 0.3943\t validation accuracy: 0.8956\n",
      "iteration number: 527\t training loss: 0.3147\tvalidation loss: 0.3947\t validation accuracy: 0.8933\n",
      "iteration number: 528\t training loss: 0.3131\tvalidation loss: 0.3969\t validation accuracy: 0.8956\n",
      "iteration number: 529\t training loss: 0.3104\tvalidation loss: 0.3939\t validation accuracy: 0.8933\n",
      "iteration number: 530\t training loss: 0.3111\tvalidation loss: 0.3935\t validation accuracy: 0.8911\n",
      "iteration number: 531\t training loss: 0.3100\tvalidation loss: 0.3935\t validation accuracy: 0.8933\n",
      "iteration number: 532\t training loss: 0.3107\tvalidation loss: 0.3952\t validation accuracy: 0.8911\n",
      "iteration number: 533\t training loss: 0.3071\tvalidation loss: 0.3894\t validation accuracy: 0.8911\n",
      "iteration number: 534\t training loss: 0.3053\tvalidation loss: 0.3869\t validation accuracy: 0.8956\n",
      "iteration number: 535\t training loss: 0.3058\tvalidation loss: 0.3892\t validation accuracy: 0.8978\n",
      "iteration number: 536\t training loss: 0.3100\tvalidation loss: 0.3938\t validation accuracy: 0.8978\n",
      "iteration number: 537\t training loss: 0.3102\tvalidation loss: 0.3971\t validation accuracy: 0.8956\n",
      "iteration number: 538\t training loss: 0.3063\tvalidation loss: 0.3924\t validation accuracy: 0.9000\n",
      "iteration number: 539\t training loss: 0.3038\tvalidation loss: 0.3859\t validation accuracy: 0.8933\n",
      "iteration number: 540\t training loss: 0.3029\tvalidation loss: 0.3835\t validation accuracy: 0.8911\n",
      "iteration number: 541\t training loss: 0.3017\tvalidation loss: 0.3809\t validation accuracy: 0.8911\n",
      "iteration number: 542\t training loss: 0.3036\tvalidation loss: 0.3854\t validation accuracy: 0.8956\n",
      "iteration number: 543\t training loss: 0.3021\tvalidation loss: 0.3844\t validation accuracy: 0.8933\n",
      "iteration number: 544\t training loss: 0.2990\tvalidation loss: 0.3801\t validation accuracy: 0.8956\n",
      "iteration number: 545\t training loss: 0.2974\tvalidation loss: 0.3765\t validation accuracy: 0.8889\n",
      "iteration number: 546\t training loss: 0.2973\tvalidation loss: 0.3773\t validation accuracy: 0.8956\n",
      "iteration number: 547\t training loss: 0.2959\tvalidation loss: 0.3779\t validation accuracy: 0.8956\n",
      "iteration number: 548\t training loss: 0.2952\tvalidation loss: 0.3761\t validation accuracy: 0.8956\n",
      "iteration number: 549\t training loss: 0.2954\tvalidation loss: 0.3786\t validation accuracy: 0.8956\n",
      "iteration number: 550\t training loss: 0.2975\tvalidation loss: 0.3794\t validation accuracy: 0.9044\n",
      "iteration number: 551\t training loss: 0.2947\tvalidation loss: 0.3765\t validation accuracy: 0.9000\n",
      "iteration number: 552\t training loss: 0.2925\tvalidation loss: 0.3752\t validation accuracy: 0.9022\n",
      "iteration number: 553\t training loss: 0.2929\tvalidation loss: 0.3752\t validation accuracy: 0.9022\n",
      "iteration number: 554\t training loss: 0.2906\tvalidation loss: 0.3735\t validation accuracy: 0.9022\n",
      "iteration number: 555\t training loss: 0.2909\tvalidation loss: 0.3734\t validation accuracy: 0.9067\n",
      "iteration number: 556\t training loss: 0.2930\tvalidation loss: 0.3762\t validation accuracy: 0.9044\n",
      "iteration number: 557\t training loss: 0.2936\tvalidation loss: 0.3761\t validation accuracy: 0.8978\n",
      "iteration number: 558\t training loss: 0.2912\tvalidation loss: 0.3759\t validation accuracy: 0.8933\n",
      "iteration number: 559\t training loss: 0.2905\tvalidation loss: 0.3754\t validation accuracy: 0.8933\n",
      "iteration number: 560\t training loss: 0.2871\tvalidation loss: 0.3715\t validation accuracy: 0.8978\n",
      "iteration number: 561\t training loss: 0.2872\tvalidation loss: 0.3707\t validation accuracy: 0.8956\n",
      "iteration number: 562\t training loss: 0.2851\tvalidation loss: 0.3682\t validation accuracy: 0.9000\n",
      "iteration number: 563\t training loss: 0.2861\tvalidation loss: 0.3678\t validation accuracy: 0.8978\n",
      "iteration number: 564\t training loss: 0.2850\tvalidation loss: 0.3683\t validation accuracy: 0.8978\n",
      "iteration number: 565\t training loss: 0.2834\tvalidation loss: 0.3672\t validation accuracy: 0.8978\n",
      "iteration number: 566\t training loss: 0.2871\tvalidation loss: 0.3716\t validation accuracy: 0.8933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 567\t training loss: 0.2841\tvalidation loss: 0.3635\t validation accuracy: 0.8933\n",
      "iteration number: 568\t training loss: 0.2824\tvalidation loss: 0.3627\t validation accuracy: 0.8956\n",
      "iteration number: 569\t training loss: 0.2824\tvalidation loss: 0.3648\t validation accuracy: 0.8978\n",
      "iteration number: 570\t training loss: 0.2819\tvalidation loss: 0.3644\t validation accuracy: 0.8978\n",
      "iteration number: 571\t training loss: 0.2814\tvalidation loss: 0.3667\t validation accuracy: 0.8956\n",
      "iteration number: 572\t training loss: 0.2867\tvalidation loss: 0.3732\t validation accuracy: 0.8889\n",
      "iteration number: 573\t training loss: 0.2850\tvalidation loss: 0.3708\t validation accuracy: 0.9022\n",
      "iteration number: 574\t training loss: 0.2816\tvalidation loss: 0.3664\t validation accuracy: 0.9000\n",
      "iteration number: 575\t training loss: 0.2804\tvalidation loss: 0.3645\t validation accuracy: 0.9022\n",
      "iteration number: 576\t training loss: 0.2812\tvalidation loss: 0.3662\t validation accuracy: 0.9022\n",
      "iteration number: 577\t training loss: 0.2770\tvalidation loss: 0.3617\t validation accuracy: 0.8978\n",
      "iteration number: 578\t training loss: 0.2775\tvalidation loss: 0.3638\t validation accuracy: 0.8978\n",
      "iteration number: 579\t training loss: 0.2748\tvalidation loss: 0.3596\t validation accuracy: 0.9000\n",
      "iteration number: 580\t training loss: 0.2739\tvalidation loss: 0.3588\t validation accuracy: 0.8978\n",
      "iteration number: 581\t training loss: 0.2723\tvalidation loss: 0.3571\t validation accuracy: 0.9044\n",
      "iteration number: 582\t training loss: 0.2714\tvalidation loss: 0.3552\t validation accuracy: 0.9022\n",
      "iteration number: 583\t training loss: 0.2705\tvalidation loss: 0.3551\t validation accuracy: 0.9044\n",
      "iteration number: 584\t training loss: 0.2709\tvalidation loss: 0.3547\t validation accuracy: 0.9000\n",
      "iteration number: 585\t training loss: 0.2729\tvalidation loss: 0.3548\t validation accuracy: 0.9089\n",
      "iteration number: 586\t training loss: 0.2708\tvalidation loss: 0.3543\t validation accuracy: 0.9000\n",
      "iteration number: 587\t training loss: 0.2704\tvalidation loss: 0.3532\t validation accuracy: 0.9067\n",
      "iteration number: 588\t training loss: 0.2684\tvalidation loss: 0.3489\t validation accuracy: 0.9044\n",
      "iteration number: 589\t training loss: 0.2691\tvalidation loss: 0.3503\t validation accuracy: 0.9022\n",
      "iteration number: 590\t training loss: 0.2670\tvalidation loss: 0.3476\t validation accuracy: 0.9044\n",
      "iteration number: 591\t training loss: 0.2668\tvalidation loss: 0.3450\t validation accuracy: 0.9089\n",
      "iteration number: 592\t training loss: 0.2650\tvalidation loss: 0.3421\t validation accuracy: 0.9089\n",
      "iteration number: 593\t training loss: 0.2648\tvalidation loss: 0.3412\t validation accuracy: 0.9067\n",
      "iteration number: 594\t training loss: 0.2667\tvalidation loss: 0.3448\t validation accuracy: 0.9044\n",
      "iteration number: 595\t training loss: 0.2650\tvalidation loss: 0.3426\t validation accuracy: 0.9089\n",
      "iteration number: 596\t training loss: 0.2669\tvalidation loss: 0.3404\t validation accuracy: 0.9089\n",
      "iteration number: 597\t training loss: 0.2634\tvalidation loss: 0.3401\t validation accuracy: 0.9067\n",
      "iteration number: 598\t training loss: 0.2638\tvalidation loss: 0.3405\t validation accuracy: 0.9111\n",
      "iteration number: 599\t training loss: 0.2626\tvalidation loss: 0.3387\t validation accuracy: 0.9111\n",
      "iteration number: 600\t training loss: 0.2601\tvalidation loss: 0.3386\t validation accuracy: 0.9067\n",
      "iteration number: 601\t training loss: 0.2595\tvalidation loss: 0.3371\t validation accuracy: 0.9111\n",
      "iteration number: 602\t training loss: 0.2597\tvalidation loss: 0.3364\t validation accuracy: 0.9089\n",
      "iteration number: 603\t training loss: 0.2589\tvalidation loss: 0.3354\t validation accuracy: 0.9133\n",
      "iteration number: 604\t training loss: 0.2579\tvalidation loss: 0.3359\t validation accuracy: 0.9089\n",
      "iteration number: 605\t training loss: 0.2575\tvalidation loss: 0.3348\t validation accuracy: 0.9067\n",
      "iteration number: 606\t training loss: 0.2599\tvalidation loss: 0.3372\t validation accuracy: 0.9022\n",
      "iteration number: 607\t training loss: 0.2569\tvalidation loss: 0.3337\t validation accuracy: 0.9089\n",
      "iteration number: 608\t training loss: 0.2577\tvalidation loss: 0.3361\t validation accuracy: 0.9089\n",
      "iteration number: 609\t training loss: 0.2601\tvalidation loss: 0.3373\t validation accuracy: 0.9111\n",
      "iteration number: 610\t training loss: 0.2596\tvalidation loss: 0.3378\t validation accuracy: 0.9067\n",
      "iteration number: 611\t training loss: 0.2611\tvalidation loss: 0.3376\t validation accuracy: 0.9067\n",
      "iteration number: 612\t training loss: 0.2591\tvalidation loss: 0.3353\t validation accuracy: 0.9067\n",
      "iteration number: 613\t training loss: 0.2542\tvalidation loss: 0.3341\t validation accuracy: 0.9089\n",
      "iteration number: 614\t training loss: 0.2559\tvalidation loss: 0.3361\t validation accuracy: 0.9111\n",
      "iteration number: 615\t training loss: 0.2569\tvalidation loss: 0.3385\t validation accuracy: 0.9022\n",
      "iteration number: 616\t training loss: 0.2512\tvalidation loss: 0.3321\t validation accuracy: 0.9022\n",
      "iteration number: 617\t training loss: 0.2502\tvalidation loss: 0.3307\t validation accuracy: 0.9067\n",
      "iteration number: 618\t training loss: 0.2498\tvalidation loss: 0.3311\t validation accuracy: 0.9022\n",
      "iteration number: 619\t training loss: 0.2497\tvalidation loss: 0.3311\t validation accuracy: 0.9022\n",
      "iteration number: 620\t training loss: 0.2484\tvalidation loss: 0.3280\t validation accuracy: 0.9044\n",
      "iteration number: 621\t training loss: 0.2485\tvalidation loss: 0.3289\t validation accuracy: 0.9067\n",
      "iteration number: 622\t training loss: 0.2490\tvalidation loss: 0.3291\t validation accuracy: 0.9022\n",
      "iteration number: 623\t training loss: 0.2508\tvalidation loss: 0.3306\t validation accuracy: 0.9000\n",
      "iteration number: 624\t training loss: 0.2497\tvalidation loss: 0.3296\t validation accuracy: 0.9000\n",
      "iteration number: 625\t training loss: 0.2512\tvalidation loss: 0.3288\t validation accuracy: 0.9044\n",
      "iteration number: 626\t training loss: 0.2452\tvalidation loss: 0.3217\t validation accuracy: 0.9067\n",
      "iteration number: 627\t training loss: 0.2466\tvalidation loss: 0.3196\t validation accuracy: 0.9111\n",
      "iteration number: 628\t training loss: 0.2478\tvalidation loss: 0.3229\t validation accuracy: 0.9111\n",
      "iteration number: 629\t training loss: 0.2459\tvalidation loss: 0.3212\t validation accuracy: 0.9067\n",
      "iteration number: 630\t training loss: 0.2484\tvalidation loss: 0.3232\t validation accuracy: 0.9067\n",
      "iteration number: 631\t training loss: 0.2488\tvalidation loss: 0.3245\t validation accuracy: 0.9044\n",
      "iteration number: 632\t training loss: 0.2443\tvalidation loss: 0.3210\t validation accuracy: 0.9111\n",
      "iteration number: 633\t training loss: 0.2415\tvalidation loss: 0.3210\t validation accuracy: 0.9067\n",
      "iteration number: 634\t training loss: 0.2408\tvalidation loss: 0.3213\t validation accuracy: 0.9044\n",
      "iteration number: 635\t training loss: 0.2412\tvalidation loss: 0.3204\t validation accuracy: 0.9133\n",
      "iteration number: 636\t training loss: 0.2408\tvalidation loss: 0.3199\t validation accuracy: 0.9089\n",
      "iteration number: 637\t training loss: 0.2398\tvalidation loss: 0.3195\t validation accuracy: 0.9111\n",
      "iteration number: 638\t training loss: 0.2414\tvalidation loss: 0.3229\t validation accuracy: 0.9067\n",
      "iteration number: 639\t training loss: 0.2420\tvalidation loss: 0.3246\t validation accuracy: 0.9022\n",
      "iteration number: 640\t training loss: 0.2411\tvalidation loss: 0.3233\t validation accuracy: 0.9089\n",
      "iteration number: 641\t training loss: 0.2414\tvalidation loss: 0.3239\t validation accuracy: 0.9022\n",
      "iteration number: 642\t training loss: 0.2376\tvalidation loss: 0.3180\t validation accuracy: 0.9111\n",
      "iteration number: 643\t training loss: 0.2399\tvalidation loss: 0.3220\t validation accuracy: 0.9067\n",
      "iteration number: 644\t training loss: 0.2363\tvalidation loss: 0.3162\t validation accuracy: 0.9089\n",
      "iteration number: 645\t training loss: 0.2362\tvalidation loss: 0.3147\t validation accuracy: 0.9133\n",
      "iteration number: 646\t training loss: 0.2353\tvalidation loss: 0.3156\t validation accuracy: 0.9133\n",
      "iteration number: 647\t training loss: 0.2353\tvalidation loss: 0.3135\t validation accuracy: 0.9133\n",
      "iteration number: 648\t training loss: 0.2360\tvalidation loss: 0.3154\t validation accuracy: 0.9111\n",
      "iteration number: 649\t training loss: 0.2360\tvalidation loss: 0.3139\t validation accuracy: 0.9111\n",
      "iteration number: 650\t training loss: 0.2349\tvalidation loss: 0.3135\t validation accuracy: 0.9089\n",
      "iteration number: 651\t training loss: 0.2364\tvalidation loss: 0.3131\t validation accuracy: 0.9200\n",
      "iteration number: 652\t training loss: 0.2339\tvalidation loss: 0.3117\t validation accuracy: 0.9111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 653\t training loss: 0.2335\tvalidation loss: 0.3138\t validation accuracy: 0.9089\n",
      "iteration number: 654\t training loss: 0.2337\tvalidation loss: 0.3131\t validation accuracy: 0.9044\n",
      "iteration number: 655\t training loss: 0.2355\tvalidation loss: 0.3151\t validation accuracy: 0.9044\n",
      "iteration number: 656\t training loss: 0.2339\tvalidation loss: 0.3141\t validation accuracy: 0.9044\n",
      "iteration number: 657\t training loss: 0.2316\tvalidation loss: 0.3106\t validation accuracy: 0.9133\n",
      "iteration number: 658\t training loss: 0.2319\tvalidation loss: 0.3078\t validation accuracy: 0.9156\n",
      "iteration number: 659\t training loss: 0.2314\tvalidation loss: 0.3056\t validation accuracy: 0.9244\n",
      "iteration number: 660\t training loss: 0.2317\tvalidation loss: 0.3078\t validation accuracy: 0.9178\n",
      "iteration number: 661\t training loss: 0.2314\tvalidation loss: 0.3065\t validation accuracy: 0.9200\n",
      "iteration number: 662\t training loss: 0.2312\tvalidation loss: 0.3074\t validation accuracy: 0.9156\n",
      "iteration number: 663\t training loss: 0.2319\tvalidation loss: 0.3072\t validation accuracy: 0.9156\n",
      "iteration number: 664\t training loss: 0.2292\tvalidation loss: 0.3048\t validation accuracy: 0.9111\n",
      "iteration number: 665\t training loss: 0.2284\tvalidation loss: 0.3044\t validation accuracy: 0.9044\n",
      "iteration number: 666\t training loss: 0.2270\tvalidation loss: 0.3014\t validation accuracy: 0.9178\n",
      "iteration number: 667\t training loss: 0.2273\tvalidation loss: 0.3022\t validation accuracy: 0.9178\n",
      "iteration number: 668\t training loss: 0.2270\tvalidation loss: 0.3039\t validation accuracy: 0.9200\n",
      "iteration number: 669\t training loss: 0.2241\tvalidation loss: 0.3002\t validation accuracy: 0.9200\n",
      "iteration number: 670\t training loss: 0.2233\tvalidation loss: 0.2997\t validation accuracy: 0.9156\n",
      "iteration number: 671\t training loss: 0.2245\tvalidation loss: 0.3015\t validation accuracy: 0.9089\n",
      "iteration number: 672\t training loss: 0.2239\tvalidation loss: 0.3016\t validation accuracy: 0.9044\n",
      "iteration number: 673\t training loss: 0.2232\tvalidation loss: 0.3006\t validation accuracy: 0.9067\n",
      "iteration number: 674\t training loss: 0.2250\tvalidation loss: 0.3051\t validation accuracy: 0.9067\n",
      "iteration number: 675\t training loss: 0.2242\tvalidation loss: 0.3040\t validation accuracy: 0.9022\n",
      "iteration number: 676\t training loss: 0.2215\tvalidation loss: 0.2976\t validation accuracy: 0.9089\n",
      "iteration number: 677\t training loss: 0.2218\tvalidation loss: 0.3007\t validation accuracy: 0.9067\n",
      "iteration number: 678\t training loss: 0.2196\tvalidation loss: 0.2973\t validation accuracy: 0.9111\n",
      "iteration number: 679\t training loss: 0.2191\tvalidation loss: 0.2974\t validation accuracy: 0.9089\n",
      "iteration number: 680\t training loss: 0.2193\tvalidation loss: 0.2960\t validation accuracy: 0.9089\n",
      "iteration number: 681\t training loss: 0.2185\tvalidation loss: 0.2940\t validation accuracy: 0.9200\n",
      "iteration number: 682\t training loss: 0.2181\tvalidation loss: 0.2939\t validation accuracy: 0.9222\n",
      "iteration number: 683\t training loss: 0.2205\tvalidation loss: 0.2949\t validation accuracy: 0.9244\n",
      "iteration number: 684\t training loss: 0.2229\tvalidation loss: 0.2995\t validation accuracy: 0.9200\n",
      "iteration number: 685\t training loss: 0.2222\tvalidation loss: 0.2959\t validation accuracy: 0.9178\n",
      "iteration number: 686\t training loss: 0.2195\tvalidation loss: 0.2965\t validation accuracy: 0.9089\n",
      "iteration number: 687\t training loss: 0.2166\tvalidation loss: 0.2913\t validation accuracy: 0.9222\n",
      "iteration number: 688\t training loss: 0.2154\tvalidation loss: 0.2920\t validation accuracy: 0.9089\n",
      "iteration number: 689\t training loss: 0.2148\tvalidation loss: 0.2920\t validation accuracy: 0.9111\n",
      "iteration number: 690\t training loss: 0.2175\tvalidation loss: 0.2955\t validation accuracy: 0.9111\n",
      "iteration number: 691\t training loss: 0.2205\tvalidation loss: 0.2973\t validation accuracy: 0.9111\n",
      "iteration number: 692\t training loss: 0.2163\tvalidation loss: 0.2928\t validation accuracy: 0.9111\n",
      "iteration number: 693\t training loss: 0.2150\tvalidation loss: 0.2928\t validation accuracy: 0.9067\n",
      "iteration number: 694\t training loss: 0.2149\tvalidation loss: 0.2886\t validation accuracy: 0.9133\n",
      "iteration number: 695\t training loss: 0.2168\tvalidation loss: 0.2866\t validation accuracy: 0.9267\n",
      "iteration number: 696\t training loss: 0.2123\tvalidation loss: 0.2844\t validation accuracy: 0.9222\n",
      "iteration number: 697\t training loss: 0.2131\tvalidation loss: 0.2841\t validation accuracy: 0.9222\n",
      "iteration number: 698\t training loss: 0.2166\tvalidation loss: 0.2883\t validation accuracy: 0.9267\n",
      "iteration number: 699\t training loss: 0.2130\tvalidation loss: 0.2893\t validation accuracy: 0.9111\n",
      "iteration number: 700\t training loss: 0.2145\tvalidation loss: 0.2906\t validation accuracy: 0.9044\n",
      "iteration number: 701\t training loss: 0.2133\tvalidation loss: 0.2891\t validation accuracy: 0.9089\n",
      "iteration number: 702\t training loss: 0.2130\tvalidation loss: 0.2855\t validation accuracy: 0.9222\n",
      "iteration number: 703\t training loss: 0.2107\tvalidation loss: 0.2859\t validation accuracy: 0.9178\n",
      "iteration number: 704\t training loss: 0.2096\tvalidation loss: 0.2864\t validation accuracy: 0.9067\n",
      "iteration number: 705\t training loss: 0.2106\tvalidation loss: 0.2868\t validation accuracy: 0.9089\n",
      "iteration number: 706\t training loss: 0.2164\tvalidation loss: 0.2923\t validation accuracy: 0.9156\n",
      "iteration number: 707\t training loss: 0.2144\tvalidation loss: 0.2884\t validation accuracy: 0.9156\n",
      "iteration number: 708\t training loss: 0.2100\tvalidation loss: 0.2839\t validation accuracy: 0.9244\n",
      "iteration number: 709\t training loss: 0.2082\tvalidation loss: 0.2854\t validation accuracy: 0.9133\n",
      "iteration number: 710\t training loss: 0.2080\tvalidation loss: 0.2825\t validation accuracy: 0.9244\n",
      "iteration number: 711\t training loss: 0.2097\tvalidation loss: 0.2841\t validation accuracy: 0.9200\n",
      "iteration number: 712\t training loss: 0.2069\tvalidation loss: 0.2804\t validation accuracy: 0.9267\n",
      "iteration number: 713\t training loss: 0.2068\tvalidation loss: 0.2795\t validation accuracy: 0.9178\n",
      "iteration number: 714\t training loss: 0.2105\tvalidation loss: 0.2809\t validation accuracy: 0.9178\n",
      "iteration number: 715\t training loss: 0.2069\tvalidation loss: 0.2793\t validation accuracy: 0.9200\n",
      "iteration number: 716\t training loss: 0.2059\tvalidation loss: 0.2775\t validation accuracy: 0.9200\n",
      "iteration number: 717\t training loss: 0.2081\tvalidation loss: 0.2784\t validation accuracy: 0.9200\n",
      "iteration number: 718\t training loss: 0.2054\tvalidation loss: 0.2770\t validation accuracy: 0.9244\n",
      "iteration number: 719\t training loss: 0.2055\tvalidation loss: 0.2774\t validation accuracy: 0.9200\n",
      "iteration number: 720\t training loss: 0.2038\tvalidation loss: 0.2773\t validation accuracy: 0.9200\n",
      "iteration number: 721\t training loss: 0.2037\tvalidation loss: 0.2776\t validation accuracy: 0.9133\n",
      "iteration number: 722\t training loss: 0.2044\tvalidation loss: 0.2798\t validation accuracy: 0.9067\n",
      "iteration number: 723\t training loss: 0.2054\tvalidation loss: 0.2784\t validation accuracy: 0.9222\n",
      "iteration number: 724\t training loss: 0.2057\tvalidation loss: 0.2790\t validation accuracy: 0.9200\n",
      "iteration number: 725\t training loss: 0.2020\tvalidation loss: 0.2784\t validation accuracy: 0.9156\n",
      "iteration number: 726\t training loss: 0.2055\tvalidation loss: 0.2827\t validation accuracy: 0.9156\n",
      "iteration number: 727\t training loss: 0.2050\tvalidation loss: 0.2834\t validation accuracy: 0.9067\n",
      "iteration number: 728\t training loss: 0.2039\tvalidation loss: 0.2796\t validation accuracy: 0.9178\n",
      "iteration number: 729\t training loss: 0.2057\tvalidation loss: 0.2782\t validation accuracy: 0.9178\n",
      "iteration number: 730\t training loss: 0.2049\tvalidation loss: 0.2758\t validation accuracy: 0.9244\n",
      "iteration number: 731\t training loss: 0.2050\tvalidation loss: 0.2766\t validation accuracy: 0.9156\n",
      "iteration number: 732\t training loss: 0.2018\tvalidation loss: 0.2743\t validation accuracy: 0.9222\n",
      "iteration number: 733\t training loss: 0.2017\tvalidation loss: 0.2743\t validation accuracy: 0.9156\n",
      "iteration number: 734\t training loss: 0.2009\tvalidation loss: 0.2737\t validation accuracy: 0.9200\n",
      "iteration number: 735\t training loss: 0.2017\tvalidation loss: 0.2728\t validation accuracy: 0.9178\n",
      "iteration number: 736\t training loss: 0.1988\tvalidation loss: 0.2723\t validation accuracy: 0.9111\n",
      "iteration number: 737\t training loss: 0.1985\tvalidation loss: 0.2711\t validation accuracy: 0.9133\n",
      "iteration number: 738\t training loss: 0.1979\tvalidation loss: 0.2686\t validation accuracy: 0.9244\n",
      "iteration number: 739\t training loss: 0.1971\tvalidation loss: 0.2699\t validation accuracy: 0.9200\n",
      "iteration number: 740\t training loss: 0.1969\tvalidation loss: 0.2725\t validation accuracy: 0.9089\n",
      "iteration number: 741\t training loss: 0.1984\tvalidation loss: 0.2724\t validation accuracy: 0.9089\n",
      "iteration number: 742\t training loss: 0.1982\tvalidation loss: 0.2754\t validation accuracy: 0.9111\n",
      "iteration number: 743\t training loss: 0.1978\tvalidation loss: 0.2753\t validation accuracy: 0.9111\n",
      "iteration number: 744\t training loss: 0.1959\tvalidation loss: 0.2716\t validation accuracy: 0.9133\n",
      "iteration number: 745\t training loss: 0.1953\tvalidation loss: 0.2692\t validation accuracy: 0.9200\n",
      "iteration number: 746\t training loss: 0.1938\tvalidation loss: 0.2670\t validation accuracy: 0.9178\n",
      "iteration number: 747\t training loss: 0.1947\tvalidation loss: 0.2667\t validation accuracy: 0.9244\n",
      "iteration number: 748\t training loss: 0.1953\tvalidation loss: 0.2663\t validation accuracy: 0.9222\n",
      "iteration number: 749\t training loss: 0.1975\tvalidation loss: 0.2671\t validation accuracy: 0.9267\n",
      "iteration number: 750\t training loss: 0.1965\tvalidation loss: 0.2691\t validation accuracy: 0.9244\n",
      "iteration number: 751\t training loss: 0.1939\tvalidation loss: 0.2646\t validation accuracy: 0.9244\n",
      "iteration number: 752\t training loss: 0.1911\tvalidation loss: 0.2649\t validation accuracy: 0.9200\n",
      "iteration number: 753\t training loss: 0.1933\tvalidation loss: 0.2636\t validation accuracy: 0.9311\n",
      "iteration number: 754\t training loss: 0.1941\tvalidation loss: 0.2643\t validation accuracy: 0.9333\n",
      "iteration number: 755\t training loss: 0.1956\tvalidation loss: 0.2646\t validation accuracy: 0.9200\n",
      "iteration number: 756\t training loss: 0.1958\tvalidation loss: 0.2644\t validation accuracy: 0.9222\n",
      "iteration number: 757\t training loss: 0.1953\tvalidation loss: 0.2634\t validation accuracy: 0.9267\n",
      "iteration number: 758\t training loss: 0.1985\tvalidation loss: 0.2648\t validation accuracy: 0.9244\n",
      "iteration number: 759\t training loss: 0.1957\tvalidation loss: 0.2629\t validation accuracy: 0.9267\n",
      "iteration number: 760\t training loss: 0.1934\tvalidation loss: 0.2617\t validation accuracy: 0.9267\n",
      "iteration number: 761\t training loss: 0.1903\tvalidation loss: 0.2620\t validation accuracy: 0.9289\n",
      "iteration number: 762\t training loss: 0.1905\tvalidation loss: 0.2618\t validation accuracy: 0.9267\n",
      "iteration number: 763\t training loss: 0.1910\tvalidation loss: 0.2631\t validation accuracy: 0.9200\n",
      "iteration number: 764\t training loss: 0.1900\tvalidation loss: 0.2628\t validation accuracy: 0.9222\n",
      "iteration number: 765\t training loss: 0.1890\tvalidation loss: 0.2615\t validation accuracy: 0.9267\n",
      "iteration number: 766\t training loss: 0.1900\tvalidation loss: 0.2619\t validation accuracy: 0.9267\n",
      "iteration number: 767\t training loss: 0.1881\tvalidation loss: 0.2616\t validation accuracy: 0.9200\n",
      "iteration number: 768\t training loss: 0.1868\tvalidation loss: 0.2612\t validation accuracy: 0.9178\n",
      "iteration number: 769\t training loss: 0.1866\tvalidation loss: 0.2605\t validation accuracy: 0.9267\n",
      "iteration number: 770\t training loss: 0.1871\tvalidation loss: 0.2624\t validation accuracy: 0.9133\n",
      "iteration number: 771\t training loss: 0.1878\tvalidation loss: 0.2630\t validation accuracy: 0.9133\n",
      "iteration number: 772\t training loss: 0.1858\tvalidation loss: 0.2613\t validation accuracy: 0.9133\n",
      "iteration number: 773\t training loss: 0.1862\tvalidation loss: 0.2618\t validation accuracy: 0.9133\n",
      "iteration number: 774\t training loss: 0.1882\tvalidation loss: 0.2630\t validation accuracy: 0.9133\n",
      "iteration number: 775\t training loss: 0.1867\tvalidation loss: 0.2601\t validation accuracy: 0.9133\n",
      "iteration number: 776\t training loss: 0.1844\tvalidation loss: 0.2570\t validation accuracy: 0.9133\n",
      "iteration number: 777\t training loss: 0.1858\tvalidation loss: 0.2585\t validation accuracy: 0.9111\n",
      "iteration number: 778\t training loss: 0.1888\tvalidation loss: 0.2639\t validation accuracy: 0.9111\n",
      "iteration number: 779\t training loss: 0.1868\tvalidation loss: 0.2618\t validation accuracy: 0.9133\n",
      "iteration number: 780\t training loss: 0.1824\tvalidation loss: 0.2555\t validation accuracy: 0.9178\n",
      "iteration number: 781\t training loss: 0.1830\tvalidation loss: 0.2548\t validation accuracy: 0.9222\n",
      "iteration number: 782\t training loss: 0.1824\tvalidation loss: 0.2571\t validation accuracy: 0.9156\n",
      "iteration number: 783\t training loss: 0.1830\tvalidation loss: 0.2576\t validation accuracy: 0.9178\n",
      "iteration number: 784\t training loss: 0.1828\tvalidation loss: 0.2585\t validation accuracy: 0.9156\n",
      "iteration number: 785\t training loss: 0.1816\tvalidation loss: 0.2561\t validation accuracy: 0.9178\n",
      "iteration number: 786\t training loss: 0.1824\tvalidation loss: 0.2594\t validation accuracy: 0.9067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 787\t training loss: 0.1821\tvalidation loss: 0.2587\t validation accuracy: 0.9067\n",
      "iteration number: 788\t training loss: 0.1821\tvalidation loss: 0.2585\t validation accuracy: 0.9111\n",
      "iteration number: 789\t training loss: 0.1817\tvalidation loss: 0.2571\t validation accuracy: 0.9111\n",
      "iteration number: 790\t training loss: 0.1796\tvalidation loss: 0.2532\t validation accuracy: 0.9178\n",
      "iteration number: 791\t training loss: 0.1788\tvalidation loss: 0.2530\t validation accuracy: 0.9200\n",
      "iteration number: 792\t training loss: 0.1782\tvalidation loss: 0.2513\t validation accuracy: 0.9244\n",
      "iteration number: 793\t training loss: 0.1783\tvalidation loss: 0.2501\t validation accuracy: 0.9267\n",
      "iteration number: 794\t training loss: 0.1793\tvalidation loss: 0.2499\t validation accuracy: 0.9267\n",
      "iteration number: 795\t training loss: 0.1787\tvalidation loss: 0.2514\t validation accuracy: 0.9133\n",
      "iteration number: 796\t training loss: 0.1780\tvalidation loss: 0.2497\t validation accuracy: 0.9222\n",
      "iteration number: 797\t training loss: 0.1784\tvalidation loss: 0.2490\t validation accuracy: 0.9244\n",
      "iteration number: 798\t training loss: 0.1787\tvalidation loss: 0.2493\t validation accuracy: 0.9311\n",
      "iteration number: 799\t training loss: 0.1781\tvalidation loss: 0.2499\t validation accuracy: 0.9222\n",
      "iteration number: 800\t training loss: 0.1774\tvalidation loss: 0.2495\t validation accuracy: 0.9289\n",
      "iteration number: 801\t training loss: 0.1776\tvalidation loss: 0.2474\t validation accuracy: 0.9289\n",
      "iteration number: 802\t training loss: 0.1762\tvalidation loss: 0.2476\t validation accuracy: 0.9244\n",
      "iteration number: 803\t training loss: 0.1764\tvalidation loss: 0.2484\t validation accuracy: 0.9200\n",
      "iteration number: 804\t training loss: 0.1767\tvalidation loss: 0.2488\t validation accuracy: 0.9200\n",
      "iteration number: 805\t training loss: 0.1766\tvalidation loss: 0.2463\t validation accuracy: 0.9311\n",
      "iteration number: 806\t training loss: 0.1767\tvalidation loss: 0.2481\t validation accuracy: 0.9311\n",
      "iteration number: 807\t training loss: 0.1813\tvalidation loss: 0.2506\t validation accuracy: 0.9444\n",
      "iteration number: 808\t training loss: 0.1815\tvalidation loss: 0.2484\t validation accuracy: 0.9400\n",
      "iteration number: 809\t training loss: 0.1783\tvalidation loss: 0.2482\t validation accuracy: 0.9422\n",
      "iteration number: 810\t training loss: 0.1768\tvalidation loss: 0.2453\t validation accuracy: 0.9311\n",
      "iteration number: 811\t training loss: 0.1753\tvalidation loss: 0.2430\t validation accuracy: 0.9311\n",
      "iteration number: 812\t training loss: 0.1734\tvalidation loss: 0.2431\t validation accuracy: 0.9267\n",
      "iteration number: 813\t training loss: 0.1727\tvalidation loss: 0.2449\t validation accuracy: 0.9200\n",
      "iteration number: 814\t training loss: 0.1726\tvalidation loss: 0.2445\t validation accuracy: 0.9267\n",
      "iteration number: 815\t training loss: 0.1730\tvalidation loss: 0.2482\t validation accuracy: 0.9222\n",
      "iteration number: 816\t training loss: 0.1727\tvalidation loss: 0.2473\t validation accuracy: 0.9200\n",
      "iteration number: 817\t training loss: 0.1747\tvalidation loss: 0.2488\t validation accuracy: 0.9311\n",
      "iteration number: 818\t training loss: 0.1729\tvalidation loss: 0.2483\t validation accuracy: 0.9244\n",
      "iteration number: 819\t training loss: 0.1750\tvalidation loss: 0.2491\t validation accuracy: 0.9311\n",
      "iteration number: 820\t training loss: 0.1759\tvalidation loss: 0.2482\t validation accuracy: 0.9333\n",
      "iteration number: 821\t training loss: 0.1780\tvalidation loss: 0.2471\t validation accuracy: 0.9356\n",
      "iteration number: 822\t training loss: 0.1726\tvalidation loss: 0.2461\t validation accuracy: 0.9333\n",
      "iteration number: 823\t training loss: 0.1734\tvalidation loss: 0.2479\t validation accuracy: 0.9333\n",
      "iteration number: 824\t training loss: 0.1700\tvalidation loss: 0.2450\t validation accuracy: 0.9222\n",
      "iteration number: 825\t training loss: 0.1705\tvalidation loss: 0.2468\t validation accuracy: 0.9178\n",
      "iteration number: 826\t training loss: 0.1699\tvalidation loss: 0.2442\t validation accuracy: 0.9222\n",
      "iteration number: 827\t training loss: 0.1702\tvalidation loss: 0.2421\t validation accuracy: 0.9267\n",
      "iteration number: 828\t training loss: 0.1702\tvalidation loss: 0.2439\t validation accuracy: 0.9200\n",
      "iteration number: 829\t training loss: 0.1711\tvalidation loss: 0.2445\t validation accuracy: 0.9244\n",
      "iteration number: 830\t training loss: 0.1703\tvalidation loss: 0.2418\t validation accuracy: 0.9267\n",
      "iteration number: 831\t training loss: 0.1708\tvalidation loss: 0.2417\t validation accuracy: 0.9311\n",
      "iteration number: 832\t training loss: 0.1752\tvalidation loss: 0.2419\t validation accuracy: 0.9289\n",
      "iteration number: 833\t training loss: 0.1698\tvalidation loss: 0.2411\t validation accuracy: 0.9244\n",
      "iteration number: 834\t training loss: 0.1689\tvalidation loss: 0.2407\t validation accuracy: 0.9244\n",
      "iteration number: 835\t training loss: 0.1718\tvalidation loss: 0.2411\t validation accuracy: 0.9267\n",
      "iteration number: 836\t training loss: 0.1733\tvalidation loss: 0.2423\t validation accuracy: 0.9289\n",
      "iteration number: 837\t training loss: 0.1693\tvalidation loss: 0.2434\t validation accuracy: 0.9200\n",
      "iteration number: 838\t training loss: 0.1705\tvalidation loss: 0.2447\t validation accuracy: 0.9244\n",
      "iteration number: 839\t training loss: 0.1708\tvalidation loss: 0.2481\t validation accuracy: 0.9178\n",
      "iteration number: 840\t training loss: 0.1679\tvalidation loss: 0.2443\t validation accuracy: 0.9178\n",
      "iteration number: 841\t training loss: 0.1664\tvalidation loss: 0.2420\t validation accuracy: 0.9178\n",
      "iteration number: 842\t training loss: 0.1669\tvalidation loss: 0.2442\t validation accuracy: 0.9178\n",
      "iteration number: 843\t training loss: 0.1677\tvalidation loss: 0.2451\t validation accuracy: 0.9178\n",
      "iteration number: 844\t training loss: 0.1688\tvalidation loss: 0.2484\t validation accuracy: 0.9133\n",
      "iteration number: 845\t training loss: 0.1682\tvalidation loss: 0.2485\t validation accuracy: 0.9156\n",
      "iteration number: 846\t training loss: 0.1665\tvalidation loss: 0.2390\t validation accuracy: 0.9356\n",
      "iteration number: 847\t training loss: 0.1637\tvalidation loss: 0.2378\t validation accuracy: 0.9244\n",
      "iteration number: 848\t training loss: 0.1641\tvalidation loss: 0.2396\t validation accuracy: 0.9156\n",
      "iteration number: 849\t training loss: 0.1664\tvalidation loss: 0.2436\t validation accuracy: 0.9178\n",
      "iteration number: 850\t training loss: 0.1652\tvalidation loss: 0.2411\t validation accuracy: 0.9222\n",
      "iteration number: 851\t training loss: 0.1635\tvalidation loss: 0.2394\t validation accuracy: 0.9156\n",
      "iteration number: 852\t training loss: 0.1633\tvalidation loss: 0.2359\t validation accuracy: 0.9311\n",
      "iteration number: 853\t training loss: 0.1632\tvalidation loss: 0.2358\t validation accuracy: 0.9311\n",
      "iteration number: 854\t training loss: 0.1625\tvalidation loss: 0.2368\t validation accuracy: 0.9156\n",
      "iteration number: 855\t training loss: 0.1624\tvalidation loss: 0.2374\t validation accuracy: 0.9178\n",
      "iteration number: 856\t training loss: 0.1621\tvalidation loss: 0.2326\t validation accuracy: 0.9289\n",
      "iteration number: 857\t training loss: 0.1644\tvalidation loss: 0.2334\t validation accuracy: 0.9444\n",
      "iteration number: 858\t training loss: 0.1620\tvalidation loss: 0.2328\t validation accuracy: 0.9356\n",
      "iteration number: 859\t training loss: 0.1616\tvalidation loss: 0.2320\t validation accuracy: 0.9267\n",
      "iteration number: 860\t training loss: 0.1611\tvalidation loss: 0.2325\t validation accuracy: 0.9222\n",
      "iteration number: 861\t training loss: 0.1603\tvalidation loss: 0.2350\t validation accuracy: 0.9178\n",
      "iteration number: 862\t training loss: 0.1613\tvalidation loss: 0.2385\t validation accuracy: 0.9200\n",
      "iteration number: 863\t training loss: 0.1605\tvalidation loss: 0.2362\t validation accuracy: 0.9178\n",
      "iteration number: 864\t training loss: 0.1614\tvalidation loss: 0.2378\t validation accuracy: 0.9200\n",
      "iteration number: 865\t training loss: 0.1611\tvalidation loss: 0.2361\t validation accuracy: 0.9178\n",
      "iteration number: 866\t training loss: 0.1610\tvalidation loss: 0.2363\t validation accuracy: 0.9156\n",
      "iteration number: 867\t training loss: 0.1605\tvalidation loss: 0.2346\t validation accuracy: 0.9200\n",
      "iteration number: 868\t training loss: 0.1589\tvalidation loss: 0.2342\t validation accuracy: 0.9200\n",
      "iteration number: 869\t training loss: 0.1586\tvalidation loss: 0.2348\t validation accuracy: 0.9133\n",
      "iteration number: 870\t training loss: 0.1592\tvalidation loss: 0.2342\t validation accuracy: 0.9222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 871\t training loss: 0.1579\tvalidation loss: 0.2321\t validation accuracy: 0.9200\n",
      "iteration number: 872\t training loss: 0.1584\tvalidation loss: 0.2329\t validation accuracy: 0.9200\n",
      "iteration number: 873\t training loss: 0.1596\tvalidation loss: 0.2371\t validation accuracy: 0.9222\n",
      "iteration number: 874\t training loss: 0.1606\tvalidation loss: 0.2392\t validation accuracy: 0.9200\n",
      "iteration number: 875\t training loss: 0.1587\tvalidation loss: 0.2352\t validation accuracy: 0.9200\n",
      "iteration number: 876\t training loss: 0.1576\tvalidation loss: 0.2311\t validation accuracy: 0.9200\n",
      "iteration number: 877\t training loss: 0.1574\tvalidation loss: 0.2292\t validation accuracy: 0.9200\n",
      "iteration number: 878\t training loss: 0.1590\tvalidation loss: 0.2303\t validation accuracy: 0.9244\n",
      "iteration number: 879\t training loss: 0.1583\tvalidation loss: 0.2337\t validation accuracy: 0.9222\n",
      "iteration number: 880\t training loss: 0.1587\tvalidation loss: 0.2355\t validation accuracy: 0.9222\n",
      "iteration number: 881\t training loss: 0.1577\tvalidation loss: 0.2349\t validation accuracy: 0.9156\n",
      "iteration number: 882\t training loss: 0.1570\tvalidation loss: 0.2324\t validation accuracy: 0.9267\n",
      "iteration number: 883\t training loss: 0.1573\tvalidation loss: 0.2321\t validation accuracy: 0.9289\n",
      "iteration number: 884\t training loss: 0.1568\tvalidation loss: 0.2325\t validation accuracy: 0.9267\n",
      "iteration number: 885\t training loss: 0.1557\tvalidation loss: 0.2315\t validation accuracy: 0.9200\n",
      "iteration number: 886\t training loss: 0.1557\tvalidation loss: 0.2319\t validation accuracy: 0.9222\n",
      "iteration number: 887\t training loss: 0.1553\tvalidation loss: 0.2309\t validation accuracy: 0.9222\n",
      "iteration number: 888\t training loss: 0.1545\tvalidation loss: 0.2283\t validation accuracy: 0.9267\n",
      "iteration number: 889\t training loss: 0.1547\tvalidation loss: 0.2282\t validation accuracy: 0.9222\n",
      "iteration number: 890\t training loss: 0.1550\tvalidation loss: 0.2295\t validation accuracy: 0.9222\n",
      "iteration number: 891\t training loss: 0.1547\tvalidation loss: 0.2278\t validation accuracy: 0.9267\n",
      "iteration number: 892\t training loss: 0.1551\tvalidation loss: 0.2284\t validation accuracy: 0.9289\n",
      "iteration number: 893\t training loss: 0.1541\tvalidation loss: 0.2290\t validation accuracy: 0.9222\n",
      "iteration number: 894\t training loss: 0.1538\tvalidation loss: 0.2308\t validation accuracy: 0.9200\n",
      "iteration number: 895\t training loss: 0.1532\tvalidation loss: 0.2282\t validation accuracy: 0.9222\n",
      "iteration number: 896\t training loss: 0.1542\tvalidation loss: 0.2277\t validation accuracy: 0.9289\n",
      "iteration number: 897\t training loss: 0.1553\tvalidation loss: 0.2311\t validation accuracy: 0.9267\n",
      "iteration number: 898\t training loss: 0.1534\tvalidation loss: 0.2279\t validation accuracy: 0.9378\n",
      "iteration number: 899\t training loss: 0.1528\tvalidation loss: 0.2259\t validation accuracy: 0.9400\n",
      "iteration number: 900\t training loss: 0.1524\tvalidation loss: 0.2251\t validation accuracy: 0.9378\n",
      "iteration number: 901\t training loss: 0.1530\tvalidation loss: 0.2268\t validation accuracy: 0.9333\n",
      "iteration number: 902\t training loss: 0.1516\tvalidation loss: 0.2270\t validation accuracy: 0.9289\n",
      "iteration number: 903\t training loss: 0.1507\tvalidation loss: 0.2262\t validation accuracy: 0.9222\n",
      "iteration number: 904\t training loss: 0.1515\tvalidation loss: 0.2278\t validation accuracy: 0.9311\n",
      "iteration number: 905\t training loss: 0.1515\tvalidation loss: 0.2277\t validation accuracy: 0.9311\n",
      "iteration number: 906\t training loss: 0.1516\tvalidation loss: 0.2274\t validation accuracy: 0.9267\n",
      "iteration number: 907\t training loss: 0.1513\tvalidation loss: 0.2245\t validation accuracy: 0.9400\n",
      "iteration number: 908\t training loss: 0.1527\tvalidation loss: 0.2266\t validation accuracy: 0.9378\n",
      "iteration number: 909\t training loss: 0.1530\tvalidation loss: 0.2280\t validation accuracy: 0.9378\n",
      "iteration number: 910\t training loss: 0.1519\tvalidation loss: 0.2286\t validation accuracy: 0.9178\n",
      "iteration number: 911\t training loss: 0.1515\tvalidation loss: 0.2283\t validation accuracy: 0.9244\n",
      "iteration number: 912\t training loss: 0.1504\tvalidation loss: 0.2270\t validation accuracy: 0.9289\n",
      "iteration number: 913\t training loss: 0.1505\tvalidation loss: 0.2279\t validation accuracy: 0.9200\n",
      "iteration number: 914\t training loss: 0.1496\tvalidation loss: 0.2254\t validation accuracy: 0.9311\n",
      "iteration number: 915\t training loss: 0.1503\tvalidation loss: 0.2238\t validation accuracy: 0.9378\n",
      "iteration number: 916\t training loss: 0.1495\tvalidation loss: 0.2230\t validation accuracy: 0.9378\n",
      "iteration number: 917\t training loss: 0.1496\tvalidation loss: 0.2251\t validation accuracy: 0.9378\n",
      "iteration number: 918\t training loss: 0.1494\tvalidation loss: 0.2262\t validation accuracy: 0.9289\n",
      "iteration number: 919\t training loss: 0.1491\tvalidation loss: 0.2238\t validation accuracy: 0.9267\n",
      "iteration number: 920\t training loss: 0.1501\tvalidation loss: 0.2252\t validation accuracy: 0.9267\n",
      "iteration number: 921\t training loss: 0.1479\tvalidation loss: 0.2214\t validation accuracy: 0.9311\n",
      "iteration number: 922\t training loss: 0.1486\tvalidation loss: 0.2227\t validation accuracy: 0.9333\n",
      "iteration number: 923\t training loss: 0.1477\tvalidation loss: 0.2209\t validation accuracy: 0.9333\n",
      "iteration number: 924\t training loss: 0.1478\tvalidation loss: 0.2204\t validation accuracy: 0.9378\n",
      "iteration number: 925\t training loss: 0.1470\tvalidation loss: 0.2218\t validation accuracy: 0.9267\n",
      "iteration number: 926\t training loss: 0.1465\tvalidation loss: 0.2208\t validation accuracy: 0.9311\n",
      "iteration number: 927\t training loss: 0.1466\tvalidation loss: 0.2196\t validation accuracy: 0.9311\n",
      "iteration number: 928\t training loss: 0.1472\tvalidation loss: 0.2193\t validation accuracy: 0.9289\n",
      "iteration number: 929\t training loss: 0.1464\tvalidation loss: 0.2197\t validation accuracy: 0.9356\n",
      "iteration number: 930\t training loss: 0.1483\tvalidation loss: 0.2227\t validation accuracy: 0.9378\n",
      "iteration number: 931\t training loss: 0.1476\tvalidation loss: 0.2201\t validation accuracy: 0.9378\n",
      "iteration number: 932\t training loss: 0.1466\tvalidation loss: 0.2196\t validation accuracy: 0.9333\n",
      "iteration number: 933\t training loss: 0.1498\tvalidation loss: 0.2264\t validation accuracy: 0.9356\n",
      "iteration number: 934\t training loss: 0.1478\tvalidation loss: 0.2246\t validation accuracy: 0.9311\n",
      "iteration number: 935\t training loss: 0.1480\tvalidation loss: 0.2253\t validation accuracy: 0.9356\n",
      "iteration number: 936\t training loss: 0.1466\tvalidation loss: 0.2240\t validation accuracy: 0.9244\n",
      "iteration number: 937\t training loss: 0.1476\tvalidation loss: 0.2252\t validation accuracy: 0.9267\n",
      "iteration number: 938\t training loss: 0.1467\tvalidation loss: 0.2218\t validation accuracy: 0.9333\n",
      "iteration number: 939\t training loss: 0.1474\tvalidation loss: 0.2230\t validation accuracy: 0.9356\n",
      "iteration number: 940\t training loss: 0.1462\tvalidation loss: 0.2221\t validation accuracy: 0.9356\n",
      "iteration number: 941\t training loss: 0.1481\tvalidation loss: 0.2240\t validation accuracy: 0.9422\n",
      "iteration number: 942\t training loss: 0.1449\tvalidation loss: 0.2180\t validation accuracy: 0.9378\n",
      "iteration number: 943\t training loss: 0.1440\tvalidation loss: 0.2197\t validation accuracy: 0.9289\n",
      "iteration number: 944\t training loss: 0.1445\tvalidation loss: 0.2205\t validation accuracy: 0.9267\n",
      "iteration number: 945\t training loss: 0.1444\tvalidation loss: 0.2152\t validation accuracy: 0.9378\n",
      "iteration number: 946\t training loss: 0.1427\tvalidation loss: 0.2169\t validation accuracy: 0.9378\n",
      "iteration number: 947\t training loss: 0.1437\tvalidation loss: 0.2172\t validation accuracy: 0.9400\n",
      "iteration number: 948\t training loss: 0.1435\tvalidation loss: 0.2171\t validation accuracy: 0.9444\n",
      "iteration number: 949\t training loss: 0.1425\tvalidation loss: 0.2169\t validation accuracy: 0.9311\n",
      "iteration number: 950\t training loss: 0.1418\tvalidation loss: 0.2161\t validation accuracy: 0.9356\n",
      "iteration number: 951\t training loss: 0.1416\tvalidation loss: 0.2149\t validation accuracy: 0.9356\n",
      "iteration number: 952\t training loss: 0.1414\tvalidation loss: 0.2144\t validation accuracy: 0.9356\n",
      "iteration number: 953\t training loss: 0.1412\tvalidation loss: 0.2169\t validation accuracy: 0.9289\n",
      "iteration number: 954\t training loss: 0.1414\tvalidation loss: 0.2164\t validation accuracy: 0.9378\n",
      "iteration number: 955\t training loss: 0.1406\tvalidation loss: 0.2157\t validation accuracy: 0.9356\n",
      "iteration number: 956\t training loss: 0.1410\tvalidation loss: 0.2183\t validation accuracy: 0.9267\n",
      "iteration number: 957\t training loss: 0.1422\tvalidation loss: 0.2162\t validation accuracy: 0.9311\n",
      "iteration number: 958\t training loss: 0.1424\tvalidation loss: 0.2188\t validation accuracy: 0.9289\n",
      "iteration number: 959\t training loss: 0.1414\tvalidation loss: 0.2194\t validation accuracy: 0.9267\n",
      "iteration number: 960\t training loss: 0.1405\tvalidation loss: 0.2187\t validation accuracy: 0.9333\n",
      "iteration number: 961\t training loss: 0.1414\tvalidation loss: 0.2183\t validation accuracy: 0.9333\n",
      "iteration number: 962\t training loss: 0.1418\tvalidation loss: 0.2172\t validation accuracy: 0.9311\n",
      "iteration number: 963\t training loss: 0.1417\tvalidation loss: 0.2151\t validation accuracy: 0.9311\n",
      "iteration number: 964\t training loss: 0.1399\tvalidation loss: 0.2146\t validation accuracy: 0.9378\n",
      "iteration number: 965\t training loss: 0.1393\tvalidation loss: 0.2163\t validation accuracy: 0.9311\n",
      "iteration number: 966\t training loss: 0.1399\tvalidation loss: 0.2171\t validation accuracy: 0.9311\n",
      "iteration number: 967\t training loss: 0.1406\tvalidation loss: 0.2196\t validation accuracy: 0.9311\n",
      "iteration number: 968\t training loss: 0.1425\tvalidation loss: 0.2230\t validation accuracy: 0.9311\n",
      "iteration number: 969\t training loss: 0.1435\tvalidation loss: 0.2245\t validation accuracy: 0.9289\n",
      "iteration number: 970\t training loss: 0.1415\tvalidation loss: 0.2218\t validation accuracy: 0.9267\n",
      "iteration number: 971\t training loss: 0.1428\tvalidation loss: 0.2234\t validation accuracy: 0.9244\n",
      "iteration number: 972\t training loss: 0.1442\tvalidation loss: 0.2253\t validation accuracy: 0.9289\n",
      "iteration number: 973\t training loss: 0.1383\tvalidation loss: 0.2153\t validation accuracy: 0.9333\n",
      "iteration number: 974\t training loss: 0.1380\tvalidation loss: 0.2149\t validation accuracy: 0.9289\n",
      "iteration number: 975\t training loss: 0.1383\tvalidation loss: 0.2144\t validation accuracy: 0.9378\n",
      "iteration number: 976\t training loss: 0.1381\tvalidation loss: 0.2137\t validation accuracy: 0.9333\n",
      "iteration number: 977\t training loss: 0.1385\tvalidation loss: 0.2162\t validation accuracy: 0.9289\n",
      "iteration number: 978\t training loss: 0.1377\tvalidation loss: 0.2138\t validation accuracy: 0.9378\n",
      "iteration number: 979\t training loss: 0.1369\tvalidation loss: 0.2144\t validation accuracy: 0.9289\n",
      "iteration number: 980\t training loss: 0.1366\tvalidation loss: 0.2138\t validation accuracy: 0.9267\n",
      "iteration number: 981\t training loss: 0.1381\tvalidation loss: 0.2160\t validation accuracy: 0.9267\n",
      "iteration number: 982\t training loss: 0.1364\tvalidation loss: 0.2117\t validation accuracy: 0.9400\n",
      "iteration number: 983\t training loss: 0.1361\tvalidation loss: 0.2121\t validation accuracy: 0.9311\n",
      "iteration number: 984\t training loss: 0.1359\tvalidation loss: 0.2121\t validation accuracy: 0.9311\n",
      "iteration number: 985\t training loss: 0.1367\tvalidation loss: 0.2109\t validation accuracy: 0.9378\n",
      "iteration number: 986\t training loss: 0.1364\tvalidation loss: 0.2129\t validation accuracy: 0.9289\n",
      "iteration number: 987\t training loss: 0.1374\tvalidation loss: 0.2169\t validation accuracy: 0.9333\n",
      "iteration number: 988\t training loss: 0.1374\tvalidation loss: 0.2164\t validation accuracy: 0.9289\n",
      "iteration number: 989\t training loss: 0.1405\tvalidation loss: 0.2214\t validation accuracy: 0.9289\n",
      "iteration number: 990\t training loss: 0.1426\tvalidation loss: 0.2238\t validation accuracy: 0.9289\n",
      "iteration number: 991\t training loss: 0.1385\tvalidation loss: 0.2174\t validation accuracy: 0.9311\n",
      "iteration number: 992\t training loss: 0.1357\tvalidation loss: 0.2133\t validation accuracy: 0.9333\n",
      "iteration number: 993\t training loss: 0.1376\tvalidation loss: 0.2150\t validation accuracy: 0.9356\n",
      "iteration number: 994\t training loss: 0.1384\tvalidation loss: 0.2151\t validation accuracy: 0.9400\n",
      "iteration number: 995\t training loss: 0.1361\tvalidation loss: 0.2127\t validation accuracy: 0.9400\n",
      "iteration number: 996\t training loss: 0.1348\tvalidation loss: 0.2102\t validation accuracy: 0.9422\n",
      "iteration number: 997\t training loss: 0.1351\tvalidation loss: 0.2087\t validation accuracy: 0.9444\n",
      "iteration number: 998\t training loss: 0.1377\tvalidation loss: 0.2121\t validation accuracy: 0.9378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 999\t training loss: 0.1374\tvalidation loss: 0.2130\t validation accuracy: 0.9378\n",
      "iteration number: 1000\t training loss: 0.1389\tvalidation loss: 0.2180\t validation accuracy: 0.9378\n",
      "iteration number: 1001\t training loss: 0.1366\tvalidation loss: 0.2115\t validation accuracy: 0.9467\n",
      "iteration number: 1002\t training loss: 0.1346\tvalidation loss: 0.2100\t validation accuracy: 0.9422\n",
      "iteration number: 1003\t training loss: 0.1348\tvalidation loss: 0.2121\t validation accuracy: 0.9378\n",
      "iteration number: 1004\t training loss: 0.1334\tvalidation loss: 0.2086\t validation accuracy: 0.9378\n",
      "iteration number: 1005\t training loss: 0.1327\tvalidation loss: 0.2075\t validation accuracy: 0.9400\n",
      "iteration number: 1006\t training loss: 0.1330\tvalidation loss: 0.2101\t validation accuracy: 0.9356\n",
      "iteration number: 1007\t training loss: 0.1327\tvalidation loss: 0.2094\t validation accuracy: 0.9356\n",
      "iteration number: 1008\t training loss: 0.1324\tvalidation loss: 0.2070\t validation accuracy: 0.9422\n",
      "iteration number: 1009\t training loss: 0.1325\tvalidation loss: 0.2057\t validation accuracy: 0.9400\n",
      "iteration number: 1010\t training loss: 0.1327\tvalidation loss: 0.2050\t validation accuracy: 0.9378\n",
      "iteration number: 1011\t training loss: 0.1323\tvalidation loss: 0.2049\t validation accuracy: 0.9378\n",
      "iteration number: 1012\t training loss: 0.1324\tvalidation loss: 0.2053\t validation accuracy: 0.9422\n",
      "iteration number: 1013\t training loss: 0.1328\tvalidation loss: 0.2066\t validation accuracy: 0.9400\n",
      "iteration number: 1014\t training loss: 0.1319\tvalidation loss: 0.2056\t validation accuracy: 0.9400\n",
      "iteration number: 1015\t training loss: 0.1318\tvalidation loss: 0.2069\t validation accuracy: 0.9422\n",
      "iteration number: 1016\t training loss: 0.1319\tvalidation loss: 0.2083\t validation accuracy: 0.9356\n",
      "iteration number: 1017\t training loss: 0.1309\tvalidation loss: 0.2072\t validation accuracy: 0.9244\n",
      "iteration number: 1018\t training loss: 0.1321\tvalidation loss: 0.2084\t validation accuracy: 0.9311\n",
      "iteration number: 1019\t training loss: 0.1309\tvalidation loss: 0.2086\t validation accuracy: 0.9267\n",
      "iteration number: 1020\t training loss: 0.1309\tvalidation loss: 0.2087\t validation accuracy: 0.9267\n",
      "iteration number: 1021\t training loss: 0.1308\tvalidation loss: 0.2063\t validation accuracy: 0.9356\n",
      "iteration number: 1022\t training loss: 0.1322\tvalidation loss: 0.2100\t validation accuracy: 0.9311\n",
      "iteration number: 1023\t training loss: 0.1334\tvalidation loss: 0.2088\t validation accuracy: 0.9333\n",
      "iteration number: 1024\t training loss: 0.1341\tvalidation loss: 0.2084\t validation accuracy: 0.9311\n",
      "iteration number: 1025\t training loss: 0.1328\tvalidation loss: 0.2085\t validation accuracy: 0.9333\n",
      "iteration number: 1026\t training loss: 0.1321\tvalidation loss: 0.2061\t validation accuracy: 0.9356\n",
      "iteration number: 1027\t training loss: 0.1319\tvalidation loss: 0.2060\t validation accuracy: 0.9356\n",
      "iteration number: 1028\t training loss: 0.1310\tvalidation loss: 0.2096\t validation accuracy: 0.9289\n",
      "iteration number: 1029\t training loss: 0.1301\tvalidation loss: 0.2091\t validation accuracy: 0.9311\n",
      "iteration number: 1030\t training loss: 0.1298\tvalidation loss: 0.2087\t validation accuracy: 0.9289\n",
      "iteration number: 1031\t training loss: 0.1306\tvalidation loss: 0.2096\t validation accuracy: 0.9289\n",
      "iteration number: 1032\t training loss: 0.1311\tvalidation loss: 0.2104\t validation accuracy: 0.9289\n",
      "iteration number: 1033\t training loss: 0.1291\tvalidation loss: 0.2055\t validation accuracy: 0.9356\n",
      "iteration number: 1034\t training loss: 0.1288\tvalidation loss: 0.2048\t validation accuracy: 0.9400\n",
      "iteration number: 1035\t training loss: 0.1289\tvalidation loss: 0.2069\t validation accuracy: 0.9311\n",
      "iteration number: 1036\t training loss: 0.1286\tvalidation loss: 0.2063\t validation accuracy: 0.9356\n",
      "iteration number: 1037\t training loss: 0.1290\tvalidation loss: 0.2070\t validation accuracy: 0.9356\n",
      "iteration number: 1038\t training loss: 0.1285\tvalidation loss: 0.2040\t validation accuracy: 0.9444\n",
      "iteration number: 1039\t training loss: 0.1287\tvalidation loss: 0.2055\t validation accuracy: 0.9422\n",
      "iteration number: 1040\t training loss: 0.1276\tvalidation loss: 0.2025\t validation accuracy: 0.9422\n",
      "iteration number: 1041\t training loss: 0.1277\tvalidation loss: 0.2039\t validation accuracy: 0.9378\n",
      "iteration number: 1042\t training loss: 0.1282\tvalidation loss: 0.2038\t validation accuracy: 0.9378\n",
      "iteration number: 1043\t training loss: 0.1279\tvalidation loss: 0.2028\t validation accuracy: 0.9378\n",
      "iteration number: 1044\t training loss: 0.1284\tvalidation loss: 0.2017\t validation accuracy: 0.9356\n",
      "iteration number: 1045\t training loss: 0.1272\tvalidation loss: 0.2016\t validation accuracy: 0.9400\n",
      "iteration number: 1046\t training loss: 0.1267\tvalidation loss: 0.2023\t validation accuracy: 0.9400\n",
      "iteration number: 1047\t training loss: 0.1281\tvalidation loss: 0.2030\t validation accuracy: 0.9378\n",
      "iteration number: 1048\t training loss: 0.1280\tvalidation loss: 0.1999\t validation accuracy: 0.9356\n",
      "iteration number: 1049\t training loss: 0.1292\tvalidation loss: 0.2013\t validation accuracy: 0.9333\n",
      "iteration number: 1050\t training loss: 0.1275\tvalidation loss: 0.2005\t validation accuracy: 0.9333\n",
      "iteration number: 1051\t training loss: 0.1278\tvalidation loss: 0.2009\t validation accuracy: 0.9378\n",
      "iteration number: 1052\t training loss: 0.1280\tvalidation loss: 0.2021\t validation accuracy: 0.9378\n",
      "iteration number: 1053\t training loss: 0.1281\tvalidation loss: 0.2042\t validation accuracy: 0.9400\n",
      "iteration number: 1054\t training loss: 0.1275\tvalidation loss: 0.2041\t validation accuracy: 0.9400\n",
      "iteration number: 1055\t training loss: 0.1279\tvalidation loss: 0.2032\t validation accuracy: 0.9422\n",
      "iteration number: 1056\t training loss: 0.1274\tvalidation loss: 0.2054\t validation accuracy: 0.9378\n",
      "iteration number: 1057\t training loss: 0.1264\tvalidation loss: 0.2046\t validation accuracy: 0.9356\n",
      "iteration number: 1058\t training loss: 0.1267\tvalidation loss: 0.2051\t validation accuracy: 0.9333\n",
      "iteration number: 1059\t training loss: 0.1262\tvalidation loss: 0.2043\t validation accuracy: 0.9356\n",
      "iteration number: 1060\t training loss: 0.1264\tvalidation loss: 0.2053\t validation accuracy: 0.9289\n",
      "iteration number: 1061\t training loss: 0.1253\tvalidation loss: 0.2016\t validation accuracy: 0.9333\n",
      "iteration number: 1062\t training loss: 0.1274\tvalidation loss: 0.2042\t validation accuracy: 0.9378\n",
      "iteration number: 1063\t training loss: 0.1259\tvalidation loss: 0.2033\t validation accuracy: 0.9400\n",
      "iteration number: 1064\t training loss: 0.1268\tvalidation loss: 0.1998\t validation accuracy: 0.9467\n",
      "iteration number: 1065\t training loss: 0.1265\tvalidation loss: 0.2013\t validation accuracy: 0.9444\n",
      "iteration number: 1066\t training loss: 0.1249\tvalidation loss: 0.2010\t validation accuracy: 0.9422\n",
      "iteration number: 1067\t training loss: 0.1251\tvalidation loss: 0.2026\t validation accuracy: 0.9400\n",
      "iteration number: 1068\t training loss: 0.1242\tvalidation loss: 0.2012\t validation accuracy: 0.9422\n",
      "iteration number: 1069\t training loss: 0.1246\tvalidation loss: 0.1986\t validation accuracy: 0.9467\n",
      "iteration number: 1070\t training loss: 0.1239\tvalidation loss: 0.1986\t validation accuracy: 0.9467\n",
      "iteration number: 1071\t training loss: 0.1233\tvalidation loss: 0.1975\t validation accuracy: 0.9467\n",
      "iteration number: 1072\t training loss: 0.1237\tvalidation loss: 0.1969\t validation accuracy: 0.9467\n",
      "iteration number: 1073\t training loss: 0.1250\tvalidation loss: 0.1989\t validation accuracy: 0.9422\n",
      "iteration number: 1074\t training loss: 0.1240\tvalidation loss: 0.1977\t validation accuracy: 0.9444\n",
      "iteration number: 1075\t training loss: 0.1258\tvalidation loss: 0.2018\t validation accuracy: 0.9444\n",
      "iteration number: 1076\t training loss: 0.1236\tvalidation loss: 0.1953\t validation accuracy: 0.9467\n",
      "iteration number: 1077\t training loss: 0.1243\tvalidation loss: 0.1961\t validation accuracy: 0.9422\n",
      "iteration number: 1078\t training loss: 0.1242\tvalidation loss: 0.1955\t validation accuracy: 0.9378\n",
      "iteration number: 1079\t training loss: 0.1228\tvalidation loss: 0.1948\t validation accuracy: 0.9489\n",
      "iteration number: 1080\t training loss: 0.1243\tvalidation loss: 0.1947\t validation accuracy: 0.9489\n",
      "iteration number: 1081\t training loss: 0.1253\tvalidation loss: 0.1954\t validation accuracy: 0.9489\n",
      "iteration number: 1082\t training loss: 0.1239\tvalidation loss: 0.1951\t validation accuracy: 0.9489\n",
      "iteration number: 1083\t training loss: 0.1231\tvalidation loss: 0.1934\t validation accuracy: 0.9467\n",
      "iteration number: 1084\t training loss: 0.1225\tvalidation loss: 0.1925\t validation accuracy: 0.9489\n",
      "iteration number: 1085\t training loss: 0.1228\tvalidation loss: 0.1967\t validation accuracy: 0.9333\n",
      "iteration number: 1086\t training loss: 0.1230\tvalidation loss: 0.2007\t validation accuracy: 0.9378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1087\t training loss: 0.1204\tvalidation loss: 0.1947\t validation accuracy: 0.9467\n",
      "iteration number: 1088\t training loss: 0.1216\tvalidation loss: 0.1943\t validation accuracy: 0.9511\n",
      "iteration number: 1089\t training loss: 0.1257\tvalidation loss: 0.1926\t validation accuracy: 0.9600\n",
      "iteration number: 1090\t training loss: 0.1227\tvalidation loss: 0.1901\t validation accuracy: 0.9489\n",
      "iteration number: 1091\t training loss: 0.1209\tvalidation loss: 0.1911\t validation accuracy: 0.9511\n",
      "iteration number: 1092\t training loss: 0.1210\tvalidation loss: 0.1913\t validation accuracy: 0.9444\n",
      "iteration number: 1093\t training loss: 0.1222\tvalidation loss: 0.1910\t validation accuracy: 0.9444\n",
      "iteration number: 1094\t training loss: 0.1206\tvalidation loss: 0.1918\t validation accuracy: 0.9444\n",
      "iteration number: 1095\t training loss: 0.1217\tvalidation loss: 0.1975\t validation accuracy: 0.9378\n",
      "iteration number: 1096\t training loss: 0.1204\tvalidation loss: 0.1955\t validation accuracy: 0.9400\n",
      "iteration number: 1097\t training loss: 0.1192\tvalidation loss: 0.1920\t validation accuracy: 0.9422\n",
      "iteration number: 1098\t training loss: 0.1191\tvalidation loss: 0.1910\t validation accuracy: 0.9467\n",
      "iteration number: 1099\t training loss: 0.1191\tvalidation loss: 0.1899\t validation accuracy: 0.9489\n",
      "iteration number: 1100\t training loss: 0.1190\tvalidation loss: 0.1903\t validation accuracy: 0.9489\n",
      "iteration number: 1101\t training loss: 0.1190\tvalidation loss: 0.1899\t validation accuracy: 0.9467\n",
      "iteration number: 1102\t training loss: 0.1194\tvalidation loss: 0.1908\t validation accuracy: 0.9489\n",
      "iteration number: 1103\t training loss: 0.1196\tvalidation loss: 0.1944\t validation accuracy: 0.9489\n",
      "iteration number: 1104\t training loss: 0.1217\tvalidation loss: 0.1971\t validation accuracy: 0.9444\n",
      "iteration number: 1105\t training loss: 0.1217\tvalidation loss: 0.1978\t validation accuracy: 0.9422\n",
      "iteration number: 1106\t training loss: 0.1211\tvalidation loss: 0.1963\t validation accuracy: 0.9422\n",
      "iteration number: 1107\t training loss: 0.1205\tvalidation loss: 0.1958\t validation accuracy: 0.9467\n",
      "iteration number: 1108\t training loss: 0.1183\tvalidation loss: 0.1903\t validation accuracy: 0.9444\n",
      "iteration number: 1109\t training loss: 0.1179\tvalidation loss: 0.1898\t validation accuracy: 0.9467\n",
      "iteration number: 1110\t training loss: 0.1176\tvalidation loss: 0.1898\t validation accuracy: 0.9467\n",
      "iteration number: 1111\t training loss: 0.1174\tvalidation loss: 0.1898\t validation accuracy: 0.9511\n",
      "iteration number: 1112\t training loss: 0.1180\tvalidation loss: 0.1890\t validation accuracy: 0.9489\n",
      "iteration number: 1113\t training loss: 0.1174\tvalidation loss: 0.1893\t validation accuracy: 0.9489\n",
      "iteration number: 1114\t training loss: 0.1193\tvalidation loss: 0.1875\t validation accuracy: 0.9489\n",
      "iteration number: 1115\t training loss: 0.1180\tvalidation loss: 0.1872\t validation accuracy: 0.9444\n",
      "iteration number: 1116\t training loss: 0.1185\tvalidation loss: 0.1873\t validation accuracy: 0.9467\n",
      "iteration number: 1117\t training loss: 0.1174\tvalidation loss: 0.1883\t validation accuracy: 0.9489\n",
      "iteration number: 1118\t training loss: 0.1171\tvalidation loss: 0.1885\t validation accuracy: 0.9467\n",
      "iteration number: 1119\t training loss: 0.1172\tvalidation loss: 0.1900\t validation accuracy: 0.9489\n",
      "iteration number: 1120\t training loss: 0.1165\tvalidation loss: 0.1895\t validation accuracy: 0.9489\n",
      "iteration number: 1121\t training loss: 0.1166\tvalidation loss: 0.1891\t validation accuracy: 0.9489\n",
      "iteration number: 1122\t training loss: 0.1177\tvalidation loss: 0.1919\t validation accuracy: 0.9511\n",
      "iteration number: 1123\t training loss: 0.1172\tvalidation loss: 0.1916\t validation accuracy: 0.9489\n",
      "iteration number: 1124\t training loss: 0.1182\tvalidation loss: 0.1925\t validation accuracy: 0.9444\n",
      "iteration number: 1125\t training loss: 0.1194\tvalidation loss: 0.1953\t validation accuracy: 0.9378\n",
      "iteration number: 1126\t training loss: 0.1182\tvalidation loss: 0.1943\t validation accuracy: 0.9422\n",
      "iteration number: 1127\t training loss: 0.1171\tvalidation loss: 0.1935\t validation accuracy: 0.9444\n",
      "iteration number: 1128\t training loss: 0.1167\tvalidation loss: 0.1924\t validation accuracy: 0.9422\n",
      "iteration number: 1129\t training loss: 0.1158\tvalidation loss: 0.1913\t validation accuracy: 0.9444\n",
      "iteration number: 1130\t training loss: 0.1177\tvalidation loss: 0.1938\t validation accuracy: 0.9511\n",
      "iteration number: 1131\t training loss: 0.1173\tvalidation loss: 0.1941\t validation accuracy: 0.9489\n",
      "iteration number: 1132\t training loss: 0.1161\tvalidation loss: 0.1927\t validation accuracy: 0.9467\n",
      "iteration number: 1133\t training loss: 0.1173\tvalidation loss: 0.1933\t validation accuracy: 0.9511\n",
      "iteration number: 1134\t training loss: 0.1157\tvalidation loss: 0.1913\t validation accuracy: 0.9422\n",
      "iteration number: 1135\t training loss: 0.1156\tvalidation loss: 0.1911\t validation accuracy: 0.9467\n",
      "iteration number: 1136\t training loss: 0.1160\tvalidation loss: 0.1878\t validation accuracy: 0.9489\n",
      "iteration number: 1137\t training loss: 0.1148\tvalidation loss: 0.1883\t validation accuracy: 0.9444\n",
      "iteration number: 1138\t training loss: 0.1141\tvalidation loss: 0.1877\t validation accuracy: 0.9467\n",
      "iteration number: 1139\t training loss: 0.1141\tvalidation loss: 0.1880\t validation accuracy: 0.9467\n",
      "iteration number: 1140\t training loss: 0.1142\tvalidation loss: 0.1878\t validation accuracy: 0.9444\n",
      "iteration number: 1141\t training loss: 0.1150\tvalidation loss: 0.1871\t validation accuracy: 0.9467\n",
      "iteration number: 1142\t training loss: 0.1142\tvalidation loss: 0.1882\t validation accuracy: 0.9511\n",
      "iteration number: 1143\t training loss: 0.1143\tvalidation loss: 0.1898\t validation accuracy: 0.9511\n",
      "iteration number: 1144\t training loss: 0.1140\tvalidation loss: 0.1884\t validation accuracy: 0.9444\n",
      "iteration number: 1145\t training loss: 0.1150\tvalidation loss: 0.1867\t validation accuracy: 0.9489\n",
      "iteration number: 1146\t training loss: 0.1150\tvalidation loss: 0.1863\t validation accuracy: 0.9467\n",
      "iteration number: 1147\t training loss: 0.1137\tvalidation loss: 0.1883\t validation accuracy: 0.9444\n",
      "iteration number: 1148\t training loss: 0.1136\tvalidation loss: 0.1891\t validation accuracy: 0.9467\n",
      "iteration number: 1149\t training loss: 0.1141\tvalidation loss: 0.1889\t validation accuracy: 0.9467\n",
      "iteration number: 1150\t training loss: 0.1140\tvalidation loss: 0.1889\t validation accuracy: 0.9444\n",
      "iteration number: 1151\t training loss: 0.1137\tvalidation loss: 0.1887\t validation accuracy: 0.9511\n",
      "iteration number: 1152\t training loss: 0.1132\tvalidation loss: 0.1875\t validation accuracy: 0.9444\n",
      "iteration number: 1153\t training loss: 0.1147\tvalidation loss: 0.1875\t validation accuracy: 0.9467\n",
      "iteration number: 1154\t training loss: 0.1152\tvalidation loss: 0.1901\t validation accuracy: 0.9467\n",
      "iteration number: 1155\t training loss: 0.1150\tvalidation loss: 0.1917\t validation accuracy: 0.9467\n",
      "iteration number: 1156\t training loss: 0.1135\tvalidation loss: 0.1894\t validation accuracy: 0.9444\n",
      "iteration number: 1157\t training loss: 0.1134\tvalidation loss: 0.1891\t validation accuracy: 0.9467\n",
      "iteration number: 1158\t training loss: 0.1140\tvalidation loss: 0.1904\t validation accuracy: 0.9467\n",
      "iteration number: 1159\t training loss: 0.1138\tvalidation loss: 0.1899\t validation accuracy: 0.9422\n",
      "iteration number: 1160\t training loss: 0.1140\tvalidation loss: 0.1892\t validation accuracy: 0.9400\n",
      "iteration number: 1161\t training loss: 0.1147\tvalidation loss: 0.1881\t validation accuracy: 0.9400\n",
      "iteration number: 1162\t training loss: 0.1138\tvalidation loss: 0.1883\t validation accuracy: 0.9400\n",
      "iteration number: 1163\t training loss: 0.1139\tvalidation loss: 0.1863\t validation accuracy: 0.9467\n",
      "iteration number: 1164\t training loss: 0.1150\tvalidation loss: 0.1897\t validation accuracy: 0.9422\n",
      "iteration number: 1165\t training loss: 0.1157\tvalidation loss: 0.1904\t validation accuracy: 0.9400\n",
      "iteration number: 1166\t training loss: 0.1137\tvalidation loss: 0.1907\t validation accuracy: 0.9444\n",
      "iteration number: 1167\t training loss: 0.1133\tvalidation loss: 0.1899\t validation accuracy: 0.9444\n",
      "iteration number: 1168\t training loss: 0.1123\tvalidation loss: 0.1875\t validation accuracy: 0.9444\n",
      "iteration number: 1169\t training loss: 0.1118\tvalidation loss: 0.1887\t validation accuracy: 0.9444\n",
      "iteration number: 1170\t training loss: 0.1128\tvalidation loss: 0.1910\t validation accuracy: 0.9467\n",
      "iteration number: 1171\t training loss: 0.1118\tvalidation loss: 0.1875\t validation accuracy: 0.9489\n",
      "iteration number: 1172\t training loss: 0.1109\tvalidation loss: 0.1869\t validation accuracy: 0.9422\n",
      "iteration number: 1173\t training loss: 0.1110\tvalidation loss: 0.1875\t validation accuracy: 0.9467\n",
      "iteration number: 1174\t training loss: 0.1113\tvalidation loss: 0.1873\t validation accuracy: 0.9533\n",
      "iteration number: 1175\t training loss: 0.1114\tvalidation loss: 0.1869\t validation accuracy: 0.9511\n",
      "iteration number: 1176\t training loss: 0.1136\tvalidation loss: 0.1877\t validation accuracy: 0.9511\n",
      "iteration number: 1177\t training loss: 0.1141\tvalidation loss: 0.1909\t validation accuracy: 0.9444\n",
      "iteration number: 1178\t training loss: 0.1136\tvalidation loss: 0.1920\t validation accuracy: 0.9400\n",
      "iteration number: 1179\t training loss: 0.1142\tvalidation loss: 0.1919\t validation accuracy: 0.9444\n",
      "iteration number: 1180\t training loss: 0.1113\tvalidation loss: 0.1882\t validation accuracy: 0.9467\n",
      "iteration number: 1181\t training loss: 0.1104\tvalidation loss: 0.1862\t validation accuracy: 0.9467\n",
      "iteration number: 1182\t training loss: 0.1110\tvalidation loss: 0.1862\t validation accuracy: 0.9400\n",
      "iteration number: 1183\t training loss: 0.1104\tvalidation loss: 0.1850\t validation accuracy: 0.9467\n",
      "iteration number: 1184\t training loss: 0.1099\tvalidation loss: 0.1853\t validation accuracy: 0.9467\n",
      "iteration number: 1185\t training loss: 0.1101\tvalidation loss: 0.1864\t validation accuracy: 0.9444\n",
      "iteration number: 1186\t training loss: 0.1105\tvalidation loss: 0.1872\t validation accuracy: 0.9444\n",
      "iteration number: 1187\t training loss: 0.1106\tvalidation loss: 0.1843\t validation accuracy: 0.9467\n",
      "iteration number: 1188\t training loss: 0.1112\tvalidation loss: 0.1876\t validation accuracy: 0.9422\n",
      "iteration number: 1189\t training loss: 0.1105\tvalidation loss: 0.1876\t validation accuracy: 0.9467\n",
      "iteration number: 1190\t training loss: 0.1097\tvalidation loss: 0.1872\t validation accuracy: 0.9422\n",
      "iteration number: 1191\t training loss: 0.1093\tvalidation loss: 0.1859\t validation accuracy: 0.9489\n",
      "iteration number: 1192\t training loss: 0.1090\tvalidation loss: 0.1869\t validation accuracy: 0.9511\n",
      "iteration number: 1193\t training loss: 0.1095\tvalidation loss: 0.1864\t validation accuracy: 0.9467\n",
      "iteration number: 1194\t training loss: 0.1094\tvalidation loss: 0.1844\t validation accuracy: 0.9489\n",
      "iteration number: 1195\t training loss: 0.1088\tvalidation loss: 0.1846\t validation accuracy: 0.9489\n",
      "iteration number: 1196\t training loss: 0.1101\tvalidation loss: 0.1884\t validation accuracy: 0.9467\n",
      "iteration number: 1197\t training loss: 0.1092\tvalidation loss: 0.1869\t validation accuracy: 0.9511\n",
      "iteration number: 1198\t training loss: 0.1099\tvalidation loss: 0.1897\t validation accuracy: 0.9511\n",
      "iteration number: 1199\t training loss: 0.1105\tvalidation loss: 0.1894\t validation accuracy: 0.9511\n",
      "iteration number: 1200\t training loss: 0.1099\tvalidation loss: 0.1889\t validation accuracy: 0.9511\n",
      "iteration number: 1201\t training loss: 0.1090\tvalidation loss: 0.1866\t validation accuracy: 0.9489\n",
      "iteration number: 1202\t training loss: 0.1088\tvalidation loss: 0.1867\t validation accuracy: 0.9467\n",
      "iteration number: 1203\t training loss: 0.1085\tvalidation loss: 0.1867\t validation accuracy: 0.9489\n",
      "iteration number: 1204\t training loss: 0.1092\tvalidation loss: 0.1893\t validation accuracy: 0.9489\n",
      "iteration number: 1205\t training loss: 0.1085\tvalidation loss: 0.1866\t validation accuracy: 0.9489\n",
      "iteration number: 1206\t training loss: 0.1099\tvalidation loss: 0.1893\t validation accuracy: 0.9444\n",
      "iteration number: 1207\t training loss: 0.1081\tvalidation loss: 0.1855\t validation accuracy: 0.9489\n",
      "iteration number: 1208\t training loss: 0.1080\tvalidation loss: 0.1854\t validation accuracy: 0.9467\n",
      "iteration number: 1209\t training loss: 0.1100\tvalidation loss: 0.1872\t validation accuracy: 0.9444\n",
      "iteration number: 1210\t training loss: 0.1089\tvalidation loss: 0.1849\t validation accuracy: 0.9511\n",
      "iteration number: 1211\t training loss: 0.1089\tvalidation loss: 0.1850\t validation accuracy: 0.9511\n",
      "iteration number: 1212\t training loss: 0.1092\tvalidation loss: 0.1847\t validation accuracy: 0.9467\n",
      "iteration number: 1213\t training loss: 0.1090\tvalidation loss: 0.1851\t validation accuracy: 0.9467\n",
      "iteration number: 1214\t training loss: 0.1084\tvalidation loss: 0.1828\t validation accuracy: 0.9533\n",
      "iteration number: 1215\t training loss: 0.1097\tvalidation loss: 0.1830\t validation accuracy: 0.9467\n",
      "iteration number: 1216\t training loss: 0.1087\tvalidation loss: 0.1827\t validation accuracy: 0.9444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1217\t training loss: 0.1107\tvalidation loss: 0.1826\t validation accuracy: 0.9489\n",
      "iteration number: 1218\t training loss: 0.1082\tvalidation loss: 0.1823\t validation accuracy: 0.9489\n",
      "iteration number: 1219\t training loss: 0.1067\tvalidation loss: 0.1849\t validation accuracy: 0.9467\n",
      "iteration number: 1220\t training loss: 0.1071\tvalidation loss: 0.1859\t validation accuracy: 0.9422\n",
      "iteration number: 1221\t training loss: 0.1079\tvalidation loss: 0.1855\t validation accuracy: 0.9422\n",
      "iteration number: 1222\t training loss: 0.1079\tvalidation loss: 0.1870\t validation accuracy: 0.9400\n",
      "iteration number: 1223\t training loss: 0.1076\tvalidation loss: 0.1861\t validation accuracy: 0.9444\n",
      "iteration number: 1224\t training loss: 0.1077\tvalidation loss: 0.1840\t validation accuracy: 0.9400\n",
      "iteration number: 1225\t training loss: 0.1074\tvalidation loss: 0.1845\t validation accuracy: 0.9422\n",
      "iteration number: 1226\t training loss: 0.1061\tvalidation loss: 0.1846\t validation accuracy: 0.9444\n",
      "iteration number: 1227\t training loss: 0.1057\tvalidation loss: 0.1842\t validation accuracy: 0.9489\n",
      "iteration number: 1228\t training loss: 0.1067\tvalidation loss: 0.1837\t validation accuracy: 0.9444\n",
      "iteration number: 1229\t training loss: 0.1077\tvalidation loss: 0.1850\t validation accuracy: 0.9400\n",
      "iteration number: 1230\t training loss: 0.1067\tvalidation loss: 0.1825\t validation accuracy: 0.9467\n",
      "iteration number: 1231\t training loss: 0.1055\tvalidation loss: 0.1814\t validation accuracy: 0.9444\n",
      "iteration number: 1232\t training loss: 0.1053\tvalidation loss: 0.1816\t validation accuracy: 0.9444\n",
      "iteration number: 1233\t training loss: 0.1052\tvalidation loss: 0.1814\t validation accuracy: 0.9467\n",
      "iteration number: 1234\t training loss: 0.1048\tvalidation loss: 0.1821\t validation accuracy: 0.9489\n",
      "iteration number: 1235\t training loss: 0.1060\tvalidation loss: 0.1817\t validation accuracy: 0.9489\n",
      "iteration number: 1236\t training loss: 0.1053\tvalidation loss: 0.1810\t validation accuracy: 0.9489\n",
      "iteration number: 1237\t training loss: 0.1046\tvalidation loss: 0.1806\t validation accuracy: 0.9489\n",
      "iteration number: 1238\t training loss: 0.1039\tvalidation loss: 0.1821\t validation accuracy: 0.9489\n",
      "iteration number: 1239\t training loss: 0.1059\tvalidation loss: 0.1820\t validation accuracy: 0.9467\n",
      "iteration number: 1240\t training loss: 0.1057\tvalidation loss: 0.1805\t validation accuracy: 0.9467\n",
      "iteration number: 1241\t training loss: 0.1049\tvalidation loss: 0.1811\t validation accuracy: 0.9444\n",
      "iteration number: 1242\t training loss: 0.1044\tvalidation loss: 0.1806\t validation accuracy: 0.9489\n",
      "iteration number: 1243\t training loss: 0.1035\tvalidation loss: 0.1819\t validation accuracy: 0.9489\n",
      "iteration number: 1244\t training loss: 0.1042\tvalidation loss: 0.1807\t validation accuracy: 0.9489\n",
      "iteration number: 1245\t training loss: 0.1043\tvalidation loss: 0.1837\t validation accuracy: 0.9467\n",
      "iteration number: 1246\t training loss: 0.1039\tvalidation loss: 0.1844\t validation accuracy: 0.9444\n",
      "iteration number: 1247\t training loss: 0.1040\tvalidation loss: 0.1842\t validation accuracy: 0.9444\n",
      "iteration number: 1248\t training loss: 0.1042\tvalidation loss: 0.1843\t validation accuracy: 0.9467\n",
      "iteration number: 1249\t training loss: 0.1041\tvalidation loss: 0.1840\t validation accuracy: 0.9467\n",
      "iteration number: 1250\t training loss: 0.1042\tvalidation loss: 0.1844\t validation accuracy: 0.9444\n",
      "iteration number: 1251\t training loss: 0.1042\tvalidation loss: 0.1853\t validation accuracy: 0.9489\n",
      "iteration number: 1252\t training loss: 0.1036\tvalidation loss: 0.1836\t validation accuracy: 0.9467\n",
      "iteration number: 1253\t training loss: 0.1039\tvalidation loss: 0.1844\t validation accuracy: 0.9489\n",
      "iteration number: 1254\t training loss: 0.1035\tvalidation loss: 0.1825\t validation accuracy: 0.9444\n",
      "iteration number: 1255\t training loss: 0.1035\tvalidation loss: 0.1836\t validation accuracy: 0.9467\n",
      "iteration number: 1256\t training loss: 0.1042\tvalidation loss: 0.1841\t validation accuracy: 0.9467\n",
      "iteration number: 1257\t training loss: 0.1060\tvalidation loss: 0.1870\t validation accuracy: 0.9444\n",
      "iteration number: 1258\t training loss: 0.1030\tvalidation loss: 0.1814\t validation accuracy: 0.9489\n",
      "iteration number: 1259\t training loss: 0.1027\tvalidation loss: 0.1812\t validation accuracy: 0.9489\n",
      "iteration number: 1260\t training loss: 0.1031\tvalidation loss: 0.1820\t validation accuracy: 0.9533\n",
      "iteration number: 1261\t training loss: 0.1040\tvalidation loss: 0.1837\t validation accuracy: 0.9556\n",
      "iteration number: 1262\t training loss: 0.1028\tvalidation loss: 0.1806\t validation accuracy: 0.9556\n",
      "iteration number: 1263\t training loss: 0.1035\tvalidation loss: 0.1793\t validation accuracy: 0.9578\n",
      "iteration number: 1264\t training loss: 0.1025\tvalidation loss: 0.1781\t validation accuracy: 0.9511\n",
      "iteration number: 1265\t training loss: 0.1020\tvalidation loss: 0.1786\t validation accuracy: 0.9489\n",
      "iteration number: 1266\t training loss: 0.1031\tvalidation loss: 0.1785\t validation accuracy: 0.9556\n",
      "iteration number: 1267\t training loss: 0.1034\tvalidation loss: 0.1817\t validation accuracy: 0.9556\n",
      "iteration number: 1268\t training loss: 0.1027\tvalidation loss: 0.1809\t validation accuracy: 0.9511\n",
      "iteration number: 1269\t training loss: 0.1028\tvalidation loss: 0.1817\t validation accuracy: 0.9511\n",
      "iteration number: 1270\t training loss: 0.1013\tvalidation loss: 0.1784\t validation accuracy: 0.9533\n",
      "iteration number: 1271\t training loss: 0.1025\tvalidation loss: 0.1815\t validation accuracy: 0.9489\n",
      "iteration number: 1272\t training loss: 0.1024\tvalidation loss: 0.1829\t validation accuracy: 0.9489\n",
      "iteration number: 1273\t training loss: 0.1009\tvalidation loss: 0.1803\t validation accuracy: 0.9533\n",
      "iteration number: 1274\t training loss: 0.1008\tvalidation loss: 0.1800\t validation accuracy: 0.9489\n",
      "iteration number: 1275\t training loss: 0.1004\tvalidation loss: 0.1792\t validation accuracy: 0.9511\n",
      "iteration number: 1276\t training loss: 0.1013\tvalidation loss: 0.1777\t validation accuracy: 0.9511\n",
      "iteration number: 1277\t training loss: 0.1019\tvalidation loss: 0.1799\t validation accuracy: 0.9489\n",
      "iteration number: 1278\t training loss: 0.1020\tvalidation loss: 0.1807\t validation accuracy: 0.9444\n",
      "iteration number: 1279\t training loss: 0.1018\tvalidation loss: 0.1822\t validation accuracy: 0.9467\n",
      "iteration number: 1280\t training loss: 0.1030\tvalidation loss: 0.1847\t validation accuracy: 0.9400\n",
      "iteration number: 1281\t training loss: 0.1037\tvalidation loss: 0.1880\t validation accuracy: 0.9422\n",
      "iteration number: 1282\t training loss: 0.1026\tvalidation loss: 0.1849\t validation accuracy: 0.9422\n",
      "iteration number: 1283\t training loss: 0.1018\tvalidation loss: 0.1839\t validation accuracy: 0.9444\n",
      "iteration number: 1284\t training loss: 0.1020\tvalidation loss: 0.1837\t validation accuracy: 0.9467\n",
      "iteration number: 1285\t training loss: 0.1022\tvalidation loss: 0.1836\t validation accuracy: 0.9444\n",
      "iteration number: 1286\t training loss: 0.1015\tvalidation loss: 0.1816\t validation accuracy: 0.9422\n",
      "iteration number: 1287\t training loss: 0.1009\tvalidation loss: 0.1799\t validation accuracy: 0.9422\n",
      "iteration number: 1288\t training loss: 0.1005\tvalidation loss: 0.1792\t validation accuracy: 0.9400\n",
      "iteration number: 1289\t training loss: 0.1011\tvalidation loss: 0.1796\t validation accuracy: 0.9400\n",
      "iteration number: 1290\t training loss: 0.1002\tvalidation loss: 0.1796\t validation accuracy: 0.9444\n",
      "iteration number: 1291\t training loss: 0.1004\tvalidation loss: 0.1819\t validation accuracy: 0.9444\n",
      "iteration number: 1292\t training loss: 0.1002\tvalidation loss: 0.1778\t validation accuracy: 0.9422\n",
      "iteration number: 1293\t training loss: 0.1003\tvalidation loss: 0.1775\t validation accuracy: 0.9511\n",
      "iteration number: 1294\t training loss: 0.1025\tvalidation loss: 0.1776\t validation accuracy: 0.9489\n",
      "iteration number: 1295\t training loss: 0.1005\tvalidation loss: 0.1765\t validation accuracy: 0.9533\n",
      "iteration number: 1296\t training loss: 0.0999\tvalidation loss: 0.1766\t validation accuracy: 0.9511\n",
      "iteration number: 1297\t training loss: 0.0990\tvalidation loss: 0.1763\t validation accuracy: 0.9511\n",
      "iteration number: 1298\t training loss: 0.0993\tvalidation loss: 0.1761\t validation accuracy: 0.9533\n",
      "iteration number: 1299\t training loss: 0.0996\tvalidation loss: 0.1755\t validation accuracy: 0.9533\n",
      "iteration number: 1300\t training loss: 0.1005\tvalidation loss: 0.1804\t validation accuracy: 0.9533\n",
      "iteration number: 1301\t training loss: 0.0994\tvalidation loss: 0.1766\t validation accuracy: 0.9533\n",
      "iteration number: 1302\t training loss: 0.0990\tvalidation loss: 0.1766\t validation accuracy: 0.9533\n",
      "iteration number: 1303\t training loss: 0.0994\tvalidation loss: 0.1750\t validation accuracy: 0.9533\n",
      "iteration number: 1304\t training loss: 0.0987\tvalidation loss: 0.1742\t validation accuracy: 0.9533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1305\t training loss: 0.1004\tvalidation loss: 0.1733\t validation accuracy: 0.9556\n",
      "iteration number: 1306\t training loss: 0.1001\tvalidation loss: 0.1727\t validation accuracy: 0.9556\n",
      "iteration number: 1307\t training loss: 0.0991\tvalidation loss: 0.1729\t validation accuracy: 0.9533\n",
      "iteration number: 1308\t training loss: 0.0981\tvalidation loss: 0.1730\t validation accuracy: 0.9511\n",
      "iteration number: 1309\t training loss: 0.0983\tvalidation loss: 0.1730\t validation accuracy: 0.9511\n",
      "iteration number: 1310\t training loss: 0.0988\tvalidation loss: 0.1743\t validation accuracy: 0.9489\n",
      "iteration number: 1311\t training loss: 0.0982\tvalidation loss: 0.1759\t validation accuracy: 0.9489\n",
      "iteration number: 1312\t training loss: 0.0980\tvalidation loss: 0.1758\t validation accuracy: 0.9511\n",
      "iteration number: 1313\t training loss: 0.0979\tvalidation loss: 0.1744\t validation accuracy: 0.9511\n",
      "iteration number: 1314\t training loss: 0.0978\tvalidation loss: 0.1749\t validation accuracy: 0.9489\n",
      "iteration number: 1315\t training loss: 0.0973\tvalidation loss: 0.1759\t validation accuracy: 0.9556\n",
      "iteration number: 1316\t training loss: 0.0976\tvalidation loss: 0.1744\t validation accuracy: 0.9533\n",
      "iteration number: 1317\t training loss: 0.0982\tvalidation loss: 0.1753\t validation accuracy: 0.9578\n",
      "iteration number: 1318\t training loss: 0.1000\tvalidation loss: 0.1729\t validation accuracy: 0.9556\n",
      "iteration number: 1319\t training loss: 0.1029\tvalidation loss: 0.1728\t validation accuracy: 0.9511\n",
      "iteration number: 1320\t training loss: 0.1030\tvalidation loss: 0.1733\t validation accuracy: 0.9489\n",
      "iteration number: 1321\t training loss: 0.1006\tvalidation loss: 0.1732\t validation accuracy: 0.9489\n",
      "iteration number: 1322\t training loss: 0.0993\tvalidation loss: 0.1729\t validation accuracy: 0.9511\n",
      "iteration number: 1323\t training loss: 0.0972\tvalidation loss: 0.1747\t validation accuracy: 0.9511\n",
      "iteration number: 1324\t training loss: 0.0969\tvalidation loss: 0.1766\t validation accuracy: 0.9489\n",
      "iteration number: 1325\t training loss: 0.0965\tvalidation loss: 0.1755\t validation accuracy: 0.9511\n",
      "iteration number: 1326\t training loss: 0.0964\tvalidation loss: 0.1763\t validation accuracy: 0.9511\n",
      "iteration number: 1327\t training loss: 0.0965\tvalidation loss: 0.1762\t validation accuracy: 0.9511\n",
      "iteration number: 1328\t training loss: 0.0964\tvalidation loss: 0.1740\t validation accuracy: 0.9511\n",
      "iteration number: 1329\t training loss: 0.0961\tvalidation loss: 0.1747\t validation accuracy: 0.9533\n",
      "iteration number: 1330\t training loss: 0.0961\tvalidation loss: 0.1742\t validation accuracy: 0.9511\n",
      "iteration number: 1331\t training loss: 0.0964\tvalidation loss: 0.1761\t validation accuracy: 0.9533\n",
      "iteration number: 1332\t training loss: 0.0974\tvalidation loss: 0.1780\t validation accuracy: 0.9511\n",
      "iteration number: 1333\t training loss: 0.0986\tvalidation loss: 0.1806\t validation accuracy: 0.9467\n",
      "iteration number: 1334\t training loss: 0.0999\tvalidation loss: 0.1816\t validation accuracy: 0.9400\n",
      "iteration number: 1335\t training loss: 0.0983\tvalidation loss: 0.1788\t validation accuracy: 0.9467\n",
      "iteration number: 1336\t training loss: 0.0985\tvalidation loss: 0.1794\t validation accuracy: 0.9444\n",
      "iteration number: 1337\t training loss: 0.0975\tvalidation loss: 0.1759\t validation accuracy: 0.9467\n",
      "iteration number: 1338\t training loss: 0.0981\tvalidation loss: 0.1783\t validation accuracy: 0.9467\n",
      "iteration number: 1339\t training loss: 0.0980\tvalidation loss: 0.1771\t validation accuracy: 0.9444\n",
      "iteration number: 1340\t training loss: 0.0979\tvalidation loss: 0.1779\t validation accuracy: 0.9444\n",
      "iteration number: 1341\t training loss: 0.0971\tvalidation loss: 0.1771\t validation accuracy: 0.9467\n",
      "iteration number: 1342\t training loss: 0.0978\tvalidation loss: 0.1779\t validation accuracy: 0.9444\n",
      "iteration number: 1343\t training loss: 0.0975\tvalidation loss: 0.1792\t validation accuracy: 0.9422\n",
      "iteration number: 1344\t training loss: 0.0986\tvalidation loss: 0.1818\t validation accuracy: 0.9467\n",
      "iteration number: 1345\t training loss: 0.0981\tvalidation loss: 0.1818\t validation accuracy: 0.9467\n",
      "iteration number: 1346\t training loss: 0.0977\tvalidation loss: 0.1809\t validation accuracy: 0.9467\n",
      "iteration number: 1347\t training loss: 0.0958\tvalidation loss: 0.1767\t validation accuracy: 0.9467\n",
      "iteration number: 1348\t training loss: 0.0959\tvalidation loss: 0.1770\t validation accuracy: 0.9467\n",
      "iteration number: 1349\t training loss: 0.0950\tvalidation loss: 0.1745\t validation accuracy: 0.9511\n",
      "iteration number: 1350\t training loss: 0.0942\tvalidation loss: 0.1727\t validation accuracy: 0.9533\n",
      "iteration number: 1351\t training loss: 0.0944\tvalidation loss: 0.1739\t validation accuracy: 0.9467\n",
      "iteration number: 1352\t training loss: 0.0943\tvalidation loss: 0.1736\t validation accuracy: 0.9511\n",
      "iteration number: 1353\t training loss: 0.0946\tvalidation loss: 0.1711\t validation accuracy: 0.9489\n",
      "iteration number: 1354\t training loss: 0.0950\tvalidation loss: 0.1739\t validation accuracy: 0.9467\n",
      "iteration number: 1355\t training loss: 0.0945\tvalidation loss: 0.1734\t validation accuracy: 0.9489\n",
      "iteration number: 1356\t training loss: 0.0944\tvalidation loss: 0.1730\t validation accuracy: 0.9489\n",
      "iteration number: 1357\t training loss: 0.0942\tvalidation loss: 0.1723\t validation accuracy: 0.9489\n",
      "iteration number: 1358\t training loss: 0.0937\tvalidation loss: 0.1717\t validation accuracy: 0.9511\n",
      "iteration number: 1359\t training loss: 0.0938\tvalidation loss: 0.1727\t validation accuracy: 0.9511\n",
      "iteration number: 1360\t training loss: 0.0942\tvalidation loss: 0.1750\t validation accuracy: 0.9489\n",
      "iteration number: 1361\t training loss: 0.0936\tvalidation loss: 0.1732\t validation accuracy: 0.9489\n",
      "iteration number: 1362\t training loss: 0.0933\tvalidation loss: 0.1720\t validation accuracy: 0.9533\n",
      "iteration number: 1363\t training loss: 0.0934\tvalidation loss: 0.1727\t validation accuracy: 0.9511\n",
      "iteration number: 1364\t training loss: 0.0941\tvalidation loss: 0.1751\t validation accuracy: 0.9511\n",
      "iteration number: 1365\t training loss: 0.0930\tvalidation loss: 0.1726\t validation accuracy: 0.9533\n",
      "iteration number: 1366\t training loss: 0.0935\tvalidation loss: 0.1756\t validation accuracy: 0.9511\n",
      "iteration number: 1367\t training loss: 0.0937\tvalidation loss: 0.1743\t validation accuracy: 0.9533\n",
      "iteration number: 1368\t training loss: 0.0951\tvalidation loss: 0.1718\t validation accuracy: 0.9489\n",
      "iteration number: 1369\t training loss: 0.0947\tvalidation loss: 0.1735\t validation accuracy: 0.9444\n",
      "iteration number: 1370\t training loss: 0.0947\tvalidation loss: 0.1731\t validation accuracy: 0.9444\n",
      "iteration number: 1371\t training loss: 0.0946\tvalidation loss: 0.1718\t validation accuracy: 0.9489\n",
      "iteration number: 1372\t training loss: 0.0937\tvalidation loss: 0.1717\t validation accuracy: 0.9489\n",
      "iteration number: 1373\t training loss: 0.0930\tvalidation loss: 0.1722\t validation accuracy: 0.9556\n",
      "iteration number: 1374\t training loss: 0.0926\tvalidation loss: 0.1717\t validation accuracy: 0.9533\n",
      "iteration number: 1375\t training loss: 0.0930\tvalidation loss: 0.1715\t validation accuracy: 0.9533\n",
      "iteration number: 1376\t training loss: 0.0929\tvalidation loss: 0.1731\t validation accuracy: 0.9533\n",
      "iteration number: 1377\t training loss: 0.0928\tvalidation loss: 0.1693\t validation accuracy: 0.9489\n",
      "iteration number: 1378\t training loss: 0.0926\tvalidation loss: 0.1700\t validation accuracy: 0.9511\n",
      "iteration number: 1379\t training loss: 0.0922\tvalidation loss: 0.1711\t validation accuracy: 0.9511\n",
      "iteration number: 1380\t training loss: 0.0929\tvalidation loss: 0.1733\t validation accuracy: 0.9489\n",
      "iteration number: 1381\t training loss: 0.0932\tvalidation loss: 0.1751\t validation accuracy: 0.9511\n",
      "iteration number: 1382\t training loss: 0.0916\tvalidation loss: 0.1722\t validation accuracy: 0.9533\n",
      "iteration number: 1383\t training loss: 0.0915\tvalidation loss: 0.1711\t validation accuracy: 0.9511\n",
      "iteration number: 1384\t training loss: 0.0919\tvalidation loss: 0.1716\t validation accuracy: 0.9511\n",
      "iteration number: 1385\t training loss: 0.0920\tvalidation loss: 0.1729\t validation accuracy: 0.9467\n",
      "iteration number: 1386\t training loss: 0.0919\tvalidation loss: 0.1729\t validation accuracy: 0.9511\n",
      "iteration number: 1387\t training loss: 0.0915\tvalidation loss: 0.1712\t validation accuracy: 0.9533\n",
      "iteration number: 1388\t training loss: 0.0917\tvalidation loss: 0.1723\t validation accuracy: 0.9489\n",
      "iteration number: 1389\t training loss: 0.0917\tvalidation loss: 0.1724\t validation accuracy: 0.9533\n",
      "iteration number: 1390\t training loss: 0.0918\tvalidation loss: 0.1720\t validation accuracy: 0.9511\n",
      "iteration number: 1391\t training loss: 0.0917\tvalidation loss: 0.1727\t validation accuracy: 0.9489\n",
      "iteration number: 1392\t training loss: 0.0913\tvalidation loss: 0.1717\t validation accuracy: 0.9533\n",
      "iteration number: 1393\t training loss: 0.0908\tvalidation loss: 0.1687\t validation accuracy: 0.9511\n",
      "iteration number: 1394\t training loss: 0.0910\tvalidation loss: 0.1694\t validation accuracy: 0.9533\n",
      "iteration number: 1395\t training loss: 0.0914\tvalidation loss: 0.1707\t validation accuracy: 0.9511\n",
      "iteration number: 1396\t training loss: 0.0909\tvalidation loss: 0.1688\t validation accuracy: 0.9533\n",
      "iteration number: 1397\t training loss: 0.0914\tvalidation loss: 0.1703\t validation accuracy: 0.9489\n",
      "iteration number: 1398\t training loss: 0.0908\tvalidation loss: 0.1714\t validation accuracy: 0.9556\n",
      "iteration number: 1399\t training loss: 0.0907\tvalidation loss: 0.1720\t validation accuracy: 0.9533\n",
      "iteration number: 1400\t training loss: 0.0909\tvalidation loss: 0.1711\t validation accuracy: 0.9556\n",
      "iteration number: 1401\t training loss: 0.0911\tvalidation loss: 0.1724\t validation accuracy: 0.9556\n",
      "iteration number: 1402\t training loss: 0.0918\tvalidation loss: 0.1721\t validation accuracy: 0.9578\n",
      "iteration number: 1403\t training loss: 0.0918\tvalidation loss: 0.1710\t validation accuracy: 0.9556\n",
      "iteration number: 1404\t training loss: 0.0923\tvalidation loss: 0.1712\t validation accuracy: 0.9578\n",
      "iteration number: 1405\t training loss: 0.0917\tvalidation loss: 0.1716\t validation accuracy: 0.9533\n",
      "iteration number: 1406\t training loss: 0.0922\tvalidation loss: 0.1712\t validation accuracy: 0.9489\n",
      "iteration number: 1407\t training loss: 0.0905\tvalidation loss: 0.1713\t validation accuracy: 0.9511\n",
      "iteration number: 1408\t training loss: 0.0905\tvalidation loss: 0.1707\t validation accuracy: 0.9533\n",
      "iteration number: 1409\t training loss: 0.0918\tvalidation loss: 0.1714\t validation accuracy: 0.9533\n",
      "iteration number: 1410\t training loss: 0.0920\tvalidation loss: 0.1728\t validation accuracy: 0.9556\n",
      "iteration number: 1411\t training loss: 0.0909\tvalidation loss: 0.1749\t validation accuracy: 0.9533\n",
      "iteration number: 1412\t training loss: 0.0910\tvalidation loss: 0.1733\t validation accuracy: 0.9556\n",
      "iteration number: 1413\t training loss: 0.0931\tvalidation loss: 0.1771\t validation accuracy: 0.9533\n",
      "iteration number: 1414\t training loss: 0.0911\tvalidation loss: 0.1752\t validation accuracy: 0.9533\n",
      "iteration number: 1415\t training loss: 0.0912\tvalidation loss: 0.1755\t validation accuracy: 0.9533\n",
      "iteration number: 1416\t training loss: 0.0906\tvalidation loss: 0.1746\t validation accuracy: 0.9533\n",
      "iteration number: 1417\t training loss: 0.0914\tvalidation loss: 0.1727\t validation accuracy: 0.9533\n",
      "iteration number: 1418\t training loss: 0.0915\tvalidation loss: 0.1726\t validation accuracy: 0.9511\n",
      "iteration number: 1419\t training loss: 0.0919\tvalidation loss: 0.1721\t validation accuracy: 0.9467\n",
      "iteration number: 1420\t training loss: 0.0903\tvalidation loss: 0.1722\t validation accuracy: 0.9511\n",
      "iteration number: 1421\t training loss: 0.0901\tvalidation loss: 0.1735\t validation accuracy: 0.9489\n",
      "iteration number: 1422\t training loss: 0.0901\tvalidation loss: 0.1742\t validation accuracy: 0.9511\n",
      "iteration number: 1423\t training loss: 0.0897\tvalidation loss: 0.1736\t validation accuracy: 0.9511\n",
      "iteration number: 1424\t training loss: 0.0890\tvalidation loss: 0.1707\t validation accuracy: 0.9533\n",
      "iteration number: 1425\t training loss: 0.0889\tvalidation loss: 0.1707\t validation accuracy: 0.9533\n",
      "iteration number: 1426\t training loss: 0.0895\tvalidation loss: 0.1715\t validation accuracy: 0.9533\n",
      "iteration number: 1427\t training loss: 0.0891\tvalidation loss: 0.1708\t validation accuracy: 0.9533\n",
      "iteration number: 1428\t training loss: 0.0890\tvalidation loss: 0.1712\t validation accuracy: 0.9533\n",
      "iteration number: 1429\t training loss: 0.0893\tvalidation loss: 0.1732\t validation accuracy: 0.9511\n",
      "iteration number: 1430\t training loss: 0.0899\tvalidation loss: 0.1741\t validation accuracy: 0.9533\n",
      "iteration number: 1431\t training loss: 0.0904\tvalidation loss: 0.1719\t validation accuracy: 0.9556\n",
      "iteration number: 1432\t training loss: 0.0907\tvalidation loss: 0.1737\t validation accuracy: 0.9556\n",
      "iteration number: 1433\t training loss: 0.0896\tvalidation loss: 0.1712\t validation accuracy: 0.9578\n",
      "iteration number: 1434\t training loss: 0.0902\tvalidation loss: 0.1714\t validation accuracy: 0.9533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1435\t training loss: 0.0901\tvalidation loss: 0.1696\t validation accuracy: 0.9556\n",
      "iteration number: 1436\t training loss: 0.0902\tvalidation loss: 0.1707\t validation accuracy: 0.9511\n",
      "iteration number: 1437\t training loss: 0.0893\tvalidation loss: 0.1693\t validation accuracy: 0.9556\n",
      "iteration number: 1438\t training loss: 0.0886\tvalidation loss: 0.1680\t validation accuracy: 0.9533\n",
      "iteration number: 1439\t training loss: 0.0892\tvalidation loss: 0.1671\t validation accuracy: 0.9533\n",
      "iteration number: 1440\t training loss: 0.0882\tvalidation loss: 0.1677\t validation accuracy: 0.9556\n",
      "iteration number: 1441\t training loss: 0.0878\tvalidation loss: 0.1689\t validation accuracy: 0.9556\n",
      "iteration number: 1442\t training loss: 0.0879\tvalidation loss: 0.1690\t validation accuracy: 0.9533\n",
      "iteration number: 1443\t training loss: 0.0876\tvalidation loss: 0.1691\t validation accuracy: 0.9533\n",
      "iteration number: 1444\t training loss: 0.0877\tvalidation loss: 0.1700\t validation accuracy: 0.9533\n",
      "iteration number: 1445\t training loss: 0.0879\tvalidation loss: 0.1683\t validation accuracy: 0.9467\n",
      "iteration number: 1446\t training loss: 0.0888\tvalidation loss: 0.1679\t validation accuracy: 0.9489\n",
      "iteration number: 1447\t training loss: 0.0880\tvalidation loss: 0.1675\t validation accuracy: 0.9533\n",
      "iteration number: 1448\t training loss: 0.0890\tvalidation loss: 0.1674\t validation accuracy: 0.9489\n",
      "iteration number: 1449\t training loss: 0.0876\tvalidation loss: 0.1674\t validation accuracy: 0.9533\n",
      "iteration number: 1450\t training loss: 0.0884\tvalidation loss: 0.1683\t validation accuracy: 0.9467\n",
      "iteration number: 1451\t training loss: 0.0896\tvalidation loss: 0.1684\t validation accuracy: 0.9467\n",
      "iteration number: 1452\t training loss: 0.0886\tvalidation loss: 0.1677\t validation accuracy: 0.9467\n",
      "iteration number: 1453\t training loss: 0.0888\tvalidation loss: 0.1690\t validation accuracy: 0.9444\n",
      "iteration number: 1454\t training loss: 0.0888\tvalidation loss: 0.1692\t validation accuracy: 0.9467\n",
      "iteration number: 1455\t training loss: 0.0879\tvalidation loss: 0.1683\t validation accuracy: 0.9533\n",
      "iteration number: 1456\t training loss: 0.0878\tvalidation loss: 0.1701\t validation accuracy: 0.9556\n",
      "iteration number: 1457\t training loss: 0.0884\tvalidation loss: 0.1693\t validation accuracy: 0.9511\n",
      "iteration number: 1458\t training loss: 0.0888\tvalidation loss: 0.1704\t validation accuracy: 0.9556\n",
      "iteration number: 1459\t training loss: 0.0882\tvalidation loss: 0.1699\t validation accuracy: 0.9533\n",
      "iteration number: 1460\t training loss: 0.0871\tvalidation loss: 0.1683\t validation accuracy: 0.9556\n",
      "iteration number: 1461\t training loss: 0.0878\tvalidation loss: 0.1685\t validation accuracy: 0.9556\n",
      "iteration number: 1462\t training loss: 0.0887\tvalidation loss: 0.1677\t validation accuracy: 0.9533\n",
      "iteration number: 1463\t training loss: 0.0879\tvalidation loss: 0.1699\t validation accuracy: 0.9533\n",
      "iteration number: 1464\t training loss: 0.0876\tvalidation loss: 0.1698\t validation accuracy: 0.9533\n",
      "iteration number: 1465\t training loss: 0.0864\tvalidation loss: 0.1697\t validation accuracy: 0.9533\n",
      "iteration number: 1466\t training loss: 0.0863\tvalidation loss: 0.1687\t validation accuracy: 0.9511\n",
      "iteration number: 1467\t training loss: 0.0859\tvalidation loss: 0.1690\t validation accuracy: 0.9556\n",
      "iteration number: 1468\t training loss: 0.0860\tvalidation loss: 0.1697\t validation accuracy: 0.9511\n",
      "iteration number: 1469\t training loss: 0.0870\tvalidation loss: 0.1704\t validation accuracy: 0.9489\n",
      "iteration number: 1470\t training loss: 0.0865\tvalidation loss: 0.1698\t validation accuracy: 0.9511\n",
      "iteration number: 1471\t training loss: 0.0865\tvalidation loss: 0.1686\t validation accuracy: 0.9511\n",
      "iteration number: 1472\t training loss: 0.0861\tvalidation loss: 0.1691\t validation accuracy: 0.9489\n",
      "iteration number: 1473\t training loss: 0.0863\tvalidation loss: 0.1697\t validation accuracy: 0.9511\n",
      "iteration number: 1474\t training loss: 0.0860\tvalidation loss: 0.1705\t validation accuracy: 0.9533\n",
      "iteration number: 1475\t training loss: 0.0855\tvalidation loss: 0.1682\t validation accuracy: 0.9511\n",
      "iteration number: 1476\t training loss: 0.0859\tvalidation loss: 0.1679\t validation accuracy: 0.9489\n",
      "iteration number: 1477\t training loss: 0.0894\tvalidation loss: 0.1681\t validation accuracy: 0.9467\n",
      "iteration number: 1478\t training loss: 0.0863\tvalidation loss: 0.1669\t validation accuracy: 0.9533\n",
      "iteration number: 1479\t training loss: 0.0856\tvalidation loss: 0.1671\t validation accuracy: 0.9511\n",
      "iteration number: 1480\t training loss: 0.0860\tvalidation loss: 0.1677\t validation accuracy: 0.9511\n",
      "iteration number: 1481\t training loss: 0.0869\tvalidation loss: 0.1677\t validation accuracy: 0.9489\n",
      "iteration number: 1482\t training loss: 0.0867\tvalidation loss: 0.1695\t validation accuracy: 0.9578\n",
      "iteration number: 1483\t training loss: 0.0875\tvalidation loss: 0.1690\t validation accuracy: 0.9556\n",
      "iteration number: 1484\t training loss: 0.0882\tvalidation loss: 0.1694\t validation accuracy: 0.9578\n",
      "iteration number: 1485\t training loss: 0.0878\tvalidation loss: 0.1700\t validation accuracy: 0.9578\n",
      "iteration number: 1486\t training loss: 0.0883\tvalidation loss: 0.1677\t validation accuracy: 0.9556\n",
      "iteration number: 1487\t training loss: 0.0889\tvalidation loss: 0.1682\t validation accuracy: 0.9533\n",
      "iteration number: 1488\t training loss: 0.0881\tvalidation loss: 0.1668\t validation accuracy: 0.9533\n",
      "iteration number: 1489\t training loss: 0.0853\tvalidation loss: 0.1651\t validation accuracy: 0.9511\n",
      "iteration number: 1490\t training loss: 0.0851\tvalidation loss: 0.1658\t validation accuracy: 0.9511\n",
      "iteration number: 1491\t training loss: 0.0854\tvalidation loss: 0.1658\t validation accuracy: 0.9511\n",
      "iteration number: 1492\t training loss: 0.0851\tvalidation loss: 0.1644\t validation accuracy: 0.9511\n",
      "iteration number: 1493\t training loss: 0.0853\tvalidation loss: 0.1636\t validation accuracy: 0.9533\n",
      "iteration number: 1494\t training loss: 0.0852\tvalidation loss: 0.1636\t validation accuracy: 0.9489\n",
      "iteration number: 1495\t training loss: 0.0847\tvalidation loss: 0.1645\t validation accuracy: 0.9467\n",
      "iteration number: 1496\t training loss: 0.0843\tvalidation loss: 0.1653\t validation accuracy: 0.9533\n",
      "iteration number: 1497\t training loss: 0.0849\tvalidation loss: 0.1683\t validation accuracy: 0.9511\n",
      "iteration number: 1498\t training loss: 0.0844\tvalidation loss: 0.1656\t validation accuracy: 0.9511\n",
      "iteration number: 1499\t training loss: 0.0838\tvalidation loss: 0.1657\t validation accuracy: 0.9511\n",
      "iteration number: 1500\t training loss: 0.0841\tvalidation loss: 0.1666\t validation accuracy: 0.9511\n",
      "iteration number: 1501\t training loss: 0.0845\tvalidation loss: 0.1684\t validation accuracy: 0.9511\n",
      "iteration number: 1502\t training loss: 0.0841\tvalidation loss: 0.1662\t validation accuracy: 0.9511\n",
      "iteration number: 1503\t training loss: 0.0843\tvalidation loss: 0.1656\t validation accuracy: 0.9511\n",
      "iteration number: 1504\t training loss: 0.0842\tvalidation loss: 0.1678\t validation accuracy: 0.9533\n",
      "iteration number: 1505\t training loss: 0.0844\tvalidation loss: 0.1647\t validation accuracy: 0.9511\n",
      "iteration number: 1506\t training loss: 0.0842\tvalidation loss: 0.1651\t validation accuracy: 0.9556\n",
      "iteration number: 1507\t training loss: 0.0848\tvalidation loss: 0.1672\t validation accuracy: 0.9578\n",
      "iteration number: 1508\t training loss: 0.0867\tvalidation loss: 0.1716\t validation accuracy: 0.9556\n",
      "iteration number: 1509\t training loss: 0.0848\tvalidation loss: 0.1674\t validation accuracy: 0.9578\n",
      "iteration number: 1510\t training loss: 0.0859\tvalidation loss: 0.1694\t validation accuracy: 0.9533\n",
      "iteration number: 1511\t training loss: 0.0852\tvalidation loss: 0.1686\t validation accuracy: 0.9533\n",
      "iteration number: 1512\t training loss: 0.0843\tvalidation loss: 0.1662\t validation accuracy: 0.9511\n",
      "iteration number: 1513\t training loss: 0.0834\tvalidation loss: 0.1659\t validation accuracy: 0.9533\n",
      "iteration number: 1514\t training loss: 0.0839\tvalidation loss: 0.1646\t validation accuracy: 0.9467\n",
      "iteration number: 1515\t training loss: 0.0840\tvalidation loss: 0.1635\t validation accuracy: 0.9556\n",
      "iteration number: 1516\t training loss: 0.0831\tvalidation loss: 0.1642\t validation accuracy: 0.9533\n",
      "iteration number: 1517\t training loss: 0.0830\tvalidation loss: 0.1641\t validation accuracy: 0.9511\n",
      "iteration number: 1518\t training loss: 0.0831\tvalidation loss: 0.1655\t validation accuracy: 0.9511\n",
      "iteration number: 1519\t training loss: 0.0838\tvalidation loss: 0.1677\t validation accuracy: 0.9511\n",
      "iteration number: 1520\t training loss: 0.0837\tvalidation loss: 0.1673\t validation accuracy: 0.9467\n",
      "iteration number: 1521\t training loss: 0.0832\tvalidation loss: 0.1668\t validation accuracy: 0.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1522\t training loss: 0.0831\tvalidation loss: 0.1656\t validation accuracy: 0.9556\n",
      "iteration number: 1523\t training loss: 0.0828\tvalidation loss: 0.1640\t validation accuracy: 0.9511\n",
      "iteration number: 1524\t training loss: 0.0827\tvalidation loss: 0.1642\t validation accuracy: 0.9556\n",
      "iteration number: 1525\t training loss: 0.0830\tvalidation loss: 0.1667\t validation accuracy: 0.9511\n",
      "iteration number: 1526\t training loss: 0.0828\tvalidation loss: 0.1665\t validation accuracy: 0.9533\n",
      "iteration number: 1527\t training loss: 0.0830\tvalidation loss: 0.1658\t validation accuracy: 0.9489\n",
      "iteration number: 1528\t training loss: 0.0824\tvalidation loss: 0.1642\t validation accuracy: 0.9489\n",
      "iteration number: 1529\t training loss: 0.0822\tvalidation loss: 0.1644\t validation accuracy: 0.9489\n",
      "iteration number: 1530\t training loss: 0.0822\tvalidation loss: 0.1645\t validation accuracy: 0.9511\n",
      "iteration number: 1531\t training loss: 0.0819\tvalidation loss: 0.1646\t validation accuracy: 0.9533\n",
      "iteration number: 1532\t training loss: 0.0823\tvalidation loss: 0.1650\t validation accuracy: 0.9556\n",
      "iteration number: 1533\t training loss: 0.0820\tvalidation loss: 0.1642\t validation accuracy: 0.9533\n",
      "iteration number: 1534\t training loss: 0.0818\tvalidation loss: 0.1637\t validation accuracy: 0.9533\n",
      "iteration number: 1535\t training loss: 0.0816\tvalidation loss: 0.1640\t validation accuracy: 0.9533\n",
      "iteration number: 1536\t training loss: 0.0820\tvalidation loss: 0.1640\t validation accuracy: 0.9489\n",
      "iteration number: 1537\t training loss: 0.0816\tvalidation loss: 0.1644\t validation accuracy: 0.9533\n",
      "iteration number: 1538\t training loss: 0.0816\tvalidation loss: 0.1638\t validation accuracy: 0.9511\n",
      "iteration number: 1539\t training loss: 0.0823\tvalidation loss: 0.1628\t validation accuracy: 0.9444\n",
      "iteration number: 1540\t training loss: 0.0824\tvalidation loss: 0.1636\t validation accuracy: 0.9511\n",
      "iteration number: 1541\t training loss: 0.0821\tvalidation loss: 0.1633\t validation accuracy: 0.9511\n",
      "iteration number: 1542\t training loss: 0.0816\tvalidation loss: 0.1645\t validation accuracy: 0.9489\n",
      "iteration number: 1543\t training loss: 0.0820\tvalidation loss: 0.1637\t validation accuracy: 0.9489\n",
      "iteration number: 1544\t training loss: 0.0818\tvalidation loss: 0.1657\t validation accuracy: 0.9489\n",
      "iteration number: 1545\t training loss: 0.0826\tvalidation loss: 0.1678\t validation accuracy: 0.9467\n",
      "iteration number: 1546\t training loss: 0.0820\tvalidation loss: 0.1643\t validation accuracy: 0.9467\n",
      "iteration number: 1547\t training loss: 0.0828\tvalidation loss: 0.1646\t validation accuracy: 0.9489\n",
      "iteration number: 1548\t training loss: 0.0825\tvalidation loss: 0.1659\t validation accuracy: 0.9467\n",
      "iteration number: 1549\t training loss: 0.0815\tvalidation loss: 0.1641\t validation accuracy: 0.9533\n",
      "iteration number: 1550\t training loss: 0.0815\tvalidation loss: 0.1632\t validation accuracy: 0.9533\n",
      "iteration number: 1551\t training loss: 0.0824\tvalidation loss: 0.1623\t validation accuracy: 0.9511\n",
      "iteration number: 1552\t training loss: 0.0822\tvalidation loss: 0.1618\t validation accuracy: 0.9511\n",
      "iteration number: 1553\t training loss: 0.0827\tvalidation loss: 0.1629\t validation accuracy: 0.9489\n",
      "iteration number: 1554\t training loss: 0.0831\tvalidation loss: 0.1644\t validation accuracy: 0.9489\n",
      "iteration number: 1555\t training loss: 0.0838\tvalidation loss: 0.1621\t validation accuracy: 0.9467\n",
      "iteration number: 1556\t training loss: 0.0826\tvalidation loss: 0.1622\t validation accuracy: 0.9489\n",
      "iteration number: 1557\t training loss: 0.0826\tvalidation loss: 0.1627\t validation accuracy: 0.9489\n",
      "iteration number: 1558\t training loss: 0.0821\tvalidation loss: 0.1636\t validation accuracy: 0.9511\n",
      "iteration number: 1559\t training loss: 0.0813\tvalidation loss: 0.1630\t validation accuracy: 0.9511\n",
      "iteration number: 1560\t training loss: 0.0807\tvalidation loss: 0.1630\t validation accuracy: 0.9511\n",
      "iteration number: 1561\t training loss: 0.0812\tvalidation loss: 0.1629\t validation accuracy: 0.9489\n",
      "iteration number: 1562\t training loss: 0.0808\tvalidation loss: 0.1631\t validation accuracy: 0.9511\n",
      "iteration number: 1563\t training loss: 0.0807\tvalidation loss: 0.1644\t validation accuracy: 0.9511\n",
      "iteration number: 1564\t training loss: 0.0804\tvalidation loss: 0.1629\t validation accuracy: 0.9511\n",
      "iteration number: 1565\t training loss: 0.0808\tvalidation loss: 0.1636\t validation accuracy: 0.9489\n",
      "iteration number: 1566\t training loss: 0.0806\tvalidation loss: 0.1622\t validation accuracy: 0.9511\n",
      "iteration number: 1567\t training loss: 0.0802\tvalidation loss: 0.1636\t validation accuracy: 0.9578\n",
      "iteration number: 1568\t training loss: 0.0804\tvalidation loss: 0.1644\t validation accuracy: 0.9556\n",
      "iteration number: 1569\t training loss: 0.0811\tvalidation loss: 0.1659\t validation accuracy: 0.9533\n",
      "iteration number: 1570\t training loss: 0.0809\tvalidation loss: 0.1660\t validation accuracy: 0.9511\n",
      "iteration number: 1571\t training loss: 0.0798\tvalidation loss: 0.1640\t validation accuracy: 0.9533\n",
      "iteration number: 1572\t training loss: 0.0807\tvalidation loss: 0.1657\t validation accuracy: 0.9578\n",
      "iteration number: 1573\t training loss: 0.0806\tvalidation loss: 0.1647\t validation accuracy: 0.9533\n",
      "iteration number: 1574\t training loss: 0.0806\tvalidation loss: 0.1657\t validation accuracy: 0.9578\n",
      "iteration number: 1575\t training loss: 0.0805\tvalidation loss: 0.1632\t validation accuracy: 0.9556\n",
      "iteration number: 1576\t training loss: 0.0792\tvalidation loss: 0.1627\t validation accuracy: 0.9533\n",
      "iteration number: 1577\t training loss: 0.0790\tvalidation loss: 0.1630\t validation accuracy: 0.9533\n",
      "iteration number: 1578\t training loss: 0.0790\tvalidation loss: 0.1637\t validation accuracy: 0.9556\n",
      "iteration number: 1579\t training loss: 0.0789\tvalidation loss: 0.1636\t validation accuracy: 0.9533\n",
      "iteration number: 1580\t training loss: 0.0791\tvalidation loss: 0.1639\t validation accuracy: 0.9533\n",
      "iteration number: 1581\t training loss: 0.0789\tvalidation loss: 0.1648\t validation accuracy: 0.9533\n",
      "iteration number: 1582\t training loss: 0.0793\tvalidation loss: 0.1664\t validation accuracy: 0.9533\n",
      "iteration number: 1583\t training loss: 0.0789\tvalidation loss: 0.1663\t validation accuracy: 0.9533\n",
      "iteration number: 1584\t training loss: 0.0794\tvalidation loss: 0.1678\t validation accuracy: 0.9533\n",
      "iteration number: 1585\t training loss: 0.0791\tvalidation loss: 0.1667\t validation accuracy: 0.9511\n",
      "iteration number: 1586\t training loss: 0.0787\tvalidation loss: 0.1647\t validation accuracy: 0.9511\n",
      "iteration number: 1587\t training loss: 0.0796\tvalidation loss: 0.1669\t validation accuracy: 0.9533\n",
      "iteration number: 1588\t training loss: 0.0795\tvalidation loss: 0.1646\t validation accuracy: 0.9467\n",
      "iteration number: 1589\t training loss: 0.0797\tvalidation loss: 0.1656\t validation accuracy: 0.9533\n",
      "iteration number: 1590\t training loss: 0.0799\tvalidation loss: 0.1650\t validation accuracy: 0.9511\n",
      "iteration number: 1591\t training loss: 0.0800\tvalidation loss: 0.1647\t validation accuracy: 0.9533\n",
      "iteration number: 1592\t training loss: 0.0813\tvalidation loss: 0.1627\t validation accuracy: 0.9489\n",
      "iteration number: 1593\t training loss: 0.0795\tvalidation loss: 0.1637\t validation accuracy: 0.9533\n",
      "iteration number: 1594\t training loss: 0.0801\tvalidation loss: 0.1662\t validation accuracy: 0.9533\n",
      "iteration number: 1595\t training loss: 0.0798\tvalidation loss: 0.1640\t validation accuracy: 0.9511\n",
      "iteration number: 1596\t training loss: 0.0788\tvalidation loss: 0.1636\t validation accuracy: 0.9533\n",
      "iteration number: 1597\t training loss: 0.0791\tvalidation loss: 0.1628\t validation accuracy: 0.9511\n",
      "iteration number: 1598\t training loss: 0.0795\tvalidation loss: 0.1632\t validation accuracy: 0.9489\n",
      "iteration number: 1599\t training loss: 0.0796\tvalidation loss: 0.1636\t validation accuracy: 0.9511\n",
      "iteration number: 1600\t training loss: 0.0788\tvalidation loss: 0.1641\t validation accuracy: 0.9511\n",
      "iteration number: 1601\t training loss: 0.0793\tvalidation loss: 0.1662\t validation accuracy: 0.9489\n",
      "iteration number: 1602\t training loss: 0.0782\tvalidation loss: 0.1651\t validation accuracy: 0.9489\n",
      "iteration number: 1603\t training loss: 0.0786\tvalidation loss: 0.1654\t validation accuracy: 0.9467\n",
      "iteration number: 1604\t training loss: 0.0792\tvalidation loss: 0.1671\t validation accuracy: 0.9467\n",
      "iteration number: 1605\t training loss: 0.0786\tvalidation loss: 0.1656\t validation accuracy: 0.9444\n",
      "iteration number: 1606\t training loss: 0.0785\tvalidation loss: 0.1666\t validation accuracy: 0.9489\n",
      "iteration number: 1607\t training loss: 0.0784\tvalidation loss: 0.1664\t validation accuracy: 0.9467\n",
      "iteration number: 1608\t training loss: 0.0782\tvalidation loss: 0.1644\t validation accuracy: 0.9444\n",
      "iteration number: 1609\t training loss: 0.0779\tvalidation loss: 0.1641\t validation accuracy: 0.9444\n",
      "iteration number: 1610\t training loss: 0.0789\tvalidation loss: 0.1656\t validation accuracy: 0.9467\n",
      "iteration number: 1611\t training loss: 0.0780\tvalidation loss: 0.1631\t validation accuracy: 0.9444\n",
      "iteration number: 1612\t training loss: 0.0776\tvalidation loss: 0.1619\t validation accuracy: 0.9467\n",
      "iteration number: 1613\t training loss: 0.0778\tvalidation loss: 0.1652\t validation accuracy: 0.9511\n",
      "iteration number: 1614\t training loss: 0.0797\tvalidation loss: 0.1693\t validation accuracy: 0.9467\n",
      "iteration number: 1615\t training loss: 0.0782\tvalidation loss: 0.1659\t validation accuracy: 0.9489\n",
      "iteration number: 1616\t training loss: 0.0769\tvalidation loss: 0.1621\t validation accuracy: 0.9511\n",
      "iteration number: 1617\t training loss: 0.0768\tvalidation loss: 0.1624\t validation accuracy: 0.9511\n",
      "iteration number: 1618\t training loss: 0.0768\tvalidation loss: 0.1640\t validation accuracy: 0.9511\n",
      "iteration number: 1619\t training loss: 0.0768\tvalidation loss: 0.1648\t validation accuracy: 0.9511\n",
      "iteration number: 1620\t training loss: 0.0777\tvalidation loss: 0.1649\t validation accuracy: 0.9467\n",
      "iteration number: 1621\t training loss: 0.0783\tvalidation loss: 0.1676\t validation accuracy: 0.9444\n",
      "iteration number: 1622\t training loss: 0.0774\tvalidation loss: 0.1647\t validation accuracy: 0.9467\n",
      "iteration number: 1623\t training loss: 0.0772\tvalidation loss: 0.1655\t validation accuracy: 0.9533\n",
      "iteration number: 1624\t training loss: 0.0770\tvalidation loss: 0.1657\t validation accuracy: 0.9511\n",
      "iteration number: 1625\t training loss: 0.0767\tvalidation loss: 0.1634\t validation accuracy: 0.9511\n",
      "iteration number: 1626\t training loss: 0.0774\tvalidation loss: 0.1639\t validation accuracy: 0.9489\n",
      "iteration number: 1627\t training loss: 0.0777\tvalidation loss: 0.1636\t validation accuracy: 0.9467\n",
      "iteration number: 1628\t training loss: 0.0771\tvalidation loss: 0.1615\t validation accuracy: 0.9533\n",
      "iteration number: 1629\t training loss: 0.0769\tvalidation loss: 0.1613\t validation accuracy: 0.9533\n",
      "iteration number: 1630\t training loss: 0.0770\tvalidation loss: 0.1615\t validation accuracy: 0.9511\n",
      "iteration number: 1631\t training loss: 0.0764\tvalidation loss: 0.1629\t validation accuracy: 0.9533\n",
      "iteration number: 1632\t training loss: 0.0764\tvalidation loss: 0.1635\t validation accuracy: 0.9511\n",
      "iteration number: 1633\t training loss: 0.0761\tvalidation loss: 0.1645\t validation accuracy: 0.9533\n",
      "iteration number: 1634\t training loss: 0.0760\tvalidation loss: 0.1647\t validation accuracy: 0.9511\n",
      "iteration number: 1635\t training loss: 0.0765\tvalidation loss: 0.1665\t validation accuracy: 0.9511\n",
      "iteration number: 1636\t training loss: 0.0772\tvalidation loss: 0.1683\t validation accuracy: 0.9489\n",
      "iteration number: 1637\t training loss: 0.0765\tvalidation loss: 0.1671\t validation accuracy: 0.9533\n",
      "iteration number: 1638\t training loss: 0.0762\tvalidation loss: 0.1660\t validation accuracy: 0.9511\n",
      "iteration number: 1639\t training loss: 0.0762\tvalidation loss: 0.1644\t validation accuracy: 0.9489\n",
      "iteration number: 1640\t training loss: 0.0765\tvalidation loss: 0.1662\t validation accuracy: 0.9511\n",
      "iteration number: 1641\t training loss: 0.0760\tvalidation loss: 0.1643\t validation accuracy: 0.9489\n",
      "iteration number: 1642\t training loss: 0.0765\tvalidation loss: 0.1654\t validation accuracy: 0.9511\n",
      "iteration number: 1643\t training loss: 0.0762\tvalidation loss: 0.1647\t validation accuracy: 0.9489\n",
      "iteration number: 1644\t training loss: 0.0764\tvalidation loss: 0.1642\t validation accuracy: 0.9467\n",
      "iteration number: 1645\t training loss: 0.0766\tvalidation loss: 0.1639\t validation accuracy: 0.9444\n",
      "iteration number: 1646\t training loss: 0.0768\tvalidation loss: 0.1643\t validation accuracy: 0.9444\n",
      "iteration number: 1647\t training loss: 0.0757\tvalidation loss: 0.1622\t validation accuracy: 0.9511\n",
      "iteration number: 1648\t training loss: 0.0762\tvalidation loss: 0.1647\t validation accuracy: 0.9489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1649\t training loss: 0.0764\tvalidation loss: 0.1653\t validation accuracy: 0.9467\n",
      "iteration number: 1650\t training loss: 0.0763\tvalidation loss: 0.1648\t validation accuracy: 0.9489\n",
      "iteration number: 1651\t training loss: 0.0750\tvalidation loss: 0.1623\t validation accuracy: 0.9489\n",
      "iteration number: 1652\t training loss: 0.0762\tvalidation loss: 0.1647\t validation accuracy: 0.9444\n",
      "iteration number: 1653\t training loss: 0.0755\tvalidation loss: 0.1628\t validation accuracy: 0.9467\n",
      "iteration number: 1654\t training loss: 0.0756\tvalidation loss: 0.1622\t validation accuracy: 0.9444\n",
      "iteration number: 1655\t training loss: 0.0756\tvalidation loss: 0.1628\t validation accuracy: 0.9444\n",
      "iteration number: 1656\t training loss: 0.0754\tvalidation loss: 0.1633\t validation accuracy: 0.9489\n",
      "iteration number: 1657\t training loss: 0.0756\tvalidation loss: 0.1630\t validation accuracy: 0.9467\n",
      "iteration number: 1658\t training loss: 0.0753\tvalidation loss: 0.1631\t validation accuracy: 0.9489\n",
      "iteration number: 1659\t training loss: 0.0748\tvalidation loss: 0.1629\t validation accuracy: 0.9533\n",
      "iteration number: 1660\t training loss: 0.0747\tvalidation loss: 0.1619\t validation accuracy: 0.9533\n",
      "iteration number: 1661\t training loss: 0.0746\tvalidation loss: 0.1618\t validation accuracy: 0.9511\n",
      "iteration number: 1662\t training loss: 0.0755\tvalidation loss: 0.1609\t validation accuracy: 0.9467\n",
      "iteration number: 1663\t training loss: 0.0751\tvalidation loss: 0.1608\t validation accuracy: 0.9444\n",
      "iteration number: 1664\t training loss: 0.0749\tvalidation loss: 0.1611\t validation accuracy: 0.9489\n",
      "iteration number: 1665\t training loss: 0.0751\tvalidation loss: 0.1615\t validation accuracy: 0.9511\n",
      "iteration number: 1666\t training loss: 0.0758\tvalidation loss: 0.1638\t validation accuracy: 0.9533\n",
      "iteration number: 1667\t training loss: 0.0749\tvalidation loss: 0.1637\t validation accuracy: 0.9533\n",
      "iteration number: 1668\t training loss: 0.0743\tvalidation loss: 0.1630\t validation accuracy: 0.9578\n",
      "iteration number: 1669\t training loss: 0.0741\tvalidation loss: 0.1614\t validation accuracy: 0.9533\n",
      "iteration number: 1670\t training loss: 0.0745\tvalidation loss: 0.1618\t validation accuracy: 0.9556\n",
      "iteration number: 1671\t training loss: 0.0744\tvalidation loss: 0.1610\t validation accuracy: 0.9556\n",
      "iteration number: 1672\t training loss: 0.0739\tvalidation loss: 0.1598\t validation accuracy: 0.9533\n",
      "iteration number: 1673\t training loss: 0.0742\tvalidation loss: 0.1600\t validation accuracy: 0.9556\n",
      "iteration number: 1674\t training loss: 0.0753\tvalidation loss: 0.1619\t validation accuracy: 0.9556\n",
      "iteration number: 1675\t training loss: 0.0744\tvalidation loss: 0.1601\t validation accuracy: 0.9556\n",
      "iteration number: 1676\t training loss: 0.0740\tvalidation loss: 0.1618\t validation accuracy: 0.9556\n",
      "iteration number: 1677\t training loss: 0.0741\tvalidation loss: 0.1623\t validation accuracy: 0.9556\n",
      "iteration number: 1678\t training loss: 0.0738\tvalidation loss: 0.1610\t validation accuracy: 0.9533\n",
      "iteration number: 1679\t training loss: 0.0748\tvalidation loss: 0.1618\t validation accuracy: 0.9533\n",
      "iteration number: 1680\t training loss: 0.0740\tvalidation loss: 0.1601\t validation accuracy: 0.9533\n",
      "iteration number: 1681\t training loss: 0.0747\tvalidation loss: 0.1649\t validation accuracy: 0.9533\n",
      "iteration number: 1682\t training loss: 0.0736\tvalidation loss: 0.1616\t validation accuracy: 0.9533\n",
      "iteration number: 1683\t training loss: 0.0736\tvalidation loss: 0.1620\t validation accuracy: 0.9556\n",
      "iteration number: 1684\t training loss: 0.0734\tvalidation loss: 0.1605\t validation accuracy: 0.9533\n",
      "iteration number: 1685\t training loss: 0.0733\tvalidation loss: 0.1588\t validation accuracy: 0.9511\n",
      "iteration number: 1686\t training loss: 0.0733\tvalidation loss: 0.1607\t validation accuracy: 0.9533\n",
      "iteration number: 1687\t training loss: 0.0735\tvalidation loss: 0.1609\t validation accuracy: 0.9533\n",
      "iteration number: 1688\t training loss: 0.0732\tvalidation loss: 0.1597\t validation accuracy: 0.9533\n",
      "iteration number: 1689\t training loss: 0.0740\tvalidation loss: 0.1628\t validation accuracy: 0.9556\n",
      "iteration number: 1690\t training loss: 0.0743\tvalidation loss: 0.1641\t validation accuracy: 0.9556\n",
      "iteration number: 1691\t training loss: 0.0741\tvalidation loss: 0.1629\t validation accuracy: 0.9533\n",
      "iteration number: 1692\t training loss: 0.0746\tvalidation loss: 0.1638\t validation accuracy: 0.9533\n",
      "iteration number: 1693\t training loss: 0.0740\tvalidation loss: 0.1634\t validation accuracy: 0.9556\n",
      "iteration number: 1694\t training loss: 0.0745\tvalidation loss: 0.1653\t validation accuracy: 0.9556\n",
      "iteration number: 1695\t training loss: 0.0751\tvalidation loss: 0.1658\t validation accuracy: 0.9556\n",
      "iteration number: 1696\t training loss: 0.0754\tvalidation loss: 0.1663\t validation accuracy: 0.9511\n",
      "iteration number: 1697\t training loss: 0.0742\tvalidation loss: 0.1638\t validation accuracy: 0.9533\n",
      "iteration number: 1698\t training loss: 0.0732\tvalidation loss: 0.1621\t validation accuracy: 0.9556\n",
      "iteration number: 1699\t training loss: 0.0737\tvalidation loss: 0.1631\t validation accuracy: 0.9556\n",
      "iteration number: 1700\t training loss: 0.0738\tvalidation loss: 0.1638\t validation accuracy: 0.9556\n",
      "iteration number: 1701\t training loss: 0.0737\tvalidation loss: 0.1636\t validation accuracy: 0.9533\n",
      "iteration number: 1702\t training loss: 0.0736\tvalidation loss: 0.1637\t validation accuracy: 0.9556\n",
      "iteration number: 1703\t training loss: 0.0729\tvalidation loss: 0.1618\t validation accuracy: 0.9533\n",
      "iteration number: 1704\t training loss: 0.0731\tvalidation loss: 0.1623\t validation accuracy: 0.9533\n",
      "iteration number: 1705\t training loss: 0.0731\tvalidation loss: 0.1626\t validation accuracy: 0.9511\n",
      "iteration number: 1706\t training loss: 0.0726\tvalidation loss: 0.1613\t validation accuracy: 0.9511\n",
      "iteration number: 1707\t training loss: 0.0729\tvalidation loss: 0.1613\t validation accuracy: 0.9511\n",
      "iteration number: 1708\t training loss: 0.0723\tvalidation loss: 0.1593\t validation accuracy: 0.9511\n",
      "iteration number: 1709\t training loss: 0.0724\tvalidation loss: 0.1582\t validation accuracy: 0.9533\n",
      "iteration number: 1710\t training loss: 0.0719\tvalidation loss: 0.1594\t validation accuracy: 0.9556\n",
      "iteration number: 1711\t training loss: 0.0719\tvalidation loss: 0.1594\t validation accuracy: 0.9533\n",
      "iteration number: 1712\t training loss: 0.0718\tvalidation loss: 0.1598\t validation accuracy: 0.9533\n",
      "iteration number: 1713\t training loss: 0.0718\tvalidation loss: 0.1602\t validation accuracy: 0.9533\n",
      "iteration number: 1714\t training loss: 0.0737\tvalidation loss: 0.1590\t validation accuracy: 0.9511\n",
      "iteration number: 1715\t training loss: 0.0725\tvalidation loss: 0.1597\t validation accuracy: 0.9533\n",
      "iteration number: 1716\t training loss: 0.0728\tvalidation loss: 0.1611\t validation accuracy: 0.9489\n",
      "iteration number: 1717\t training loss: 0.0723\tvalidation loss: 0.1626\t validation accuracy: 0.9489\n",
      "iteration number: 1718\t training loss: 0.0729\tvalidation loss: 0.1630\t validation accuracy: 0.9489\n",
      "iteration number: 1719\t training loss: 0.0726\tvalidation loss: 0.1619\t validation accuracy: 0.9511\n",
      "iteration number: 1720\t training loss: 0.0741\tvalidation loss: 0.1647\t validation accuracy: 0.9444\n",
      "iteration number: 1721\t training loss: 0.0745\tvalidation loss: 0.1656\t validation accuracy: 0.9444\n",
      "iteration number: 1722\t training loss: 0.0745\tvalidation loss: 0.1658\t validation accuracy: 0.9444\n",
      "iteration number: 1723\t training loss: 0.0737\tvalidation loss: 0.1654\t validation accuracy: 0.9489\n",
      "iteration number: 1724\t training loss: 0.0733\tvalidation loss: 0.1641\t validation accuracy: 0.9467\n",
      "iteration number: 1725\t training loss: 0.0733\tvalidation loss: 0.1614\t validation accuracy: 0.9467\n",
      "iteration number: 1726\t training loss: 0.0724\tvalidation loss: 0.1626\t validation accuracy: 0.9511\n",
      "iteration number: 1727\t training loss: 0.0736\tvalidation loss: 0.1641\t validation accuracy: 0.9467\n",
      "iteration number: 1728\t training loss: 0.0721\tvalidation loss: 0.1601\t validation accuracy: 0.9467\n",
      "iteration number: 1729\t training loss: 0.0715\tvalidation loss: 0.1581\t validation accuracy: 0.9489\n",
      "iteration number: 1730\t training loss: 0.0712\tvalidation loss: 0.1580\t validation accuracy: 0.9511\n",
      "iteration number: 1731\t training loss: 0.0717\tvalidation loss: 0.1576\t validation accuracy: 0.9511\n",
      "iteration number: 1732\t training loss: 0.0721\tvalidation loss: 0.1573\t validation accuracy: 0.9489\n",
      "iteration number: 1733\t training loss: 0.0713\tvalidation loss: 0.1578\t validation accuracy: 0.9533\n",
      "iteration number: 1734\t training loss: 0.0711\tvalidation loss: 0.1584\t validation accuracy: 0.9556\n",
      "iteration number: 1735\t training loss: 0.0716\tvalidation loss: 0.1577\t validation accuracy: 0.9533\n",
      "iteration number: 1736\t training loss: 0.0713\tvalidation loss: 0.1572\t validation accuracy: 0.9533\n",
      "iteration number: 1737\t training loss: 0.0708\tvalidation loss: 0.1584\t validation accuracy: 0.9489\n",
      "iteration number: 1738\t training loss: 0.0711\tvalidation loss: 0.1579\t validation accuracy: 0.9556\n",
      "iteration number: 1739\t training loss: 0.0717\tvalidation loss: 0.1598\t validation accuracy: 0.9556\n",
      "iteration number: 1740\t training loss: 0.0714\tvalidation loss: 0.1599\t validation accuracy: 0.9556\n",
      "iteration number: 1741\t training loss: 0.0716\tvalidation loss: 0.1620\t validation accuracy: 0.9556\n",
      "iteration number: 1742\t training loss: 0.0710\tvalidation loss: 0.1611\t validation accuracy: 0.9556\n",
      "iteration number: 1743\t training loss: 0.0715\tvalidation loss: 0.1589\t validation accuracy: 0.9533\n",
      "iteration number: 1744\t training loss: 0.0712\tvalidation loss: 0.1582\t validation accuracy: 0.9556\n",
      "iteration number: 1745\t training loss: 0.0711\tvalidation loss: 0.1582\t validation accuracy: 0.9533\n",
      "iteration number: 1746\t training loss: 0.0708\tvalidation loss: 0.1577\t validation accuracy: 0.9556\n",
      "iteration number: 1747\t training loss: 0.0719\tvalidation loss: 0.1611\t validation accuracy: 0.9533\n",
      "iteration number: 1748\t training loss: 0.0719\tvalidation loss: 0.1599\t validation accuracy: 0.9511\n",
      "iteration number: 1749\t training loss: 0.0721\tvalidation loss: 0.1604\t validation accuracy: 0.9489\n",
      "iteration number: 1750\t training loss: 0.0730\tvalidation loss: 0.1618\t validation accuracy: 0.9556\n",
      "iteration number: 1751\t training loss: 0.0712\tvalidation loss: 0.1601\t validation accuracy: 0.9511\n",
      "iteration number: 1752\t training loss: 0.0709\tvalidation loss: 0.1588\t validation accuracy: 0.9511\n",
      "iteration number: 1753\t training loss: 0.0702\tvalidation loss: 0.1593\t validation accuracy: 0.9533\n",
      "iteration number: 1754\t training loss: 0.0701\tvalidation loss: 0.1598\t validation accuracy: 0.9533\n",
      "iteration number: 1755\t training loss: 0.0703\tvalidation loss: 0.1584\t validation accuracy: 0.9533\n",
      "iteration number: 1756\t training loss: 0.0707\tvalidation loss: 0.1594\t validation accuracy: 0.9511\n",
      "iteration number: 1757\t training loss: 0.0707\tvalidation loss: 0.1602\t validation accuracy: 0.9511\n",
      "iteration number: 1758\t training loss: 0.0707\tvalidation loss: 0.1593\t validation accuracy: 0.9511\n",
      "iteration number: 1759\t training loss: 0.0709\tvalidation loss: 0.1607\t validation accuracy: 0.9511\n",
      "iteration number: 1760\t training loss: 0.0711\tvalidation loss: 0.1603\t validation accuracy: 0.9511\n",
      "iteration number: 1761\t training loss: 0.0721\tvalidation loss: 0.1636\t validation accuracy: 0.9533\n",
      "iteration number: 1762\t training loss: 0.0705\tvalidation loss: 0.1611\t validation accuracy: 0.9556\n",
      "iteration number: 1763\t training loss: 0.0703\tvalidation loss: 0.1606\t validation accuracy: 0.9556\n",
      "iteration number: 1764\t training loss: 0.0710\tvalidation loss: 0.1616\t validation accuracy: 0.9533\n",
      "iteration number: 1765\t training loss: 0.0710\tvalidation loss: 0.1629\t validation accuracy: 0.9556\n",
      "iteration number: 1766\t training loss: 0.0716\tvalidation loss: 0.1625\t validation accuracy: 0.9578\n",
      "iteration number: 1767\t training loss: 0.0708\tvalidation loss: 0.1626\t validation accuracy: 0.9533\n",
      "iteration number: 1768\t training loss: 0.0704\tvalidation loss: 0.1592\t validation accuracy: 0.9556\n",
      "iteration number: 1769\t training loss: 0.0707\tvalidation loss: 0.1602\t validation accuracy: 0.9556\n",
      "iteration number: 1770\t training loss: 0.0735\tvalidation loss: 0.1605\t validation accuracy: 0.9533\n",
      "iteration number: 1771\t training loss: 0.0739\tvalidation loss: 0.1599\t validation accuracy: 0.9556\n",
      "iteration number: 1772\t training loss: 0.0713\tvalidation loss: 0.1615\t validation accuracy: 0.9556\n",
      "iteration number: 1773\t training loss: 0.0708\tvalidation loss: 0.1615\t validation accuracy: 0.9533\n",
      "iteration number: 1774\t training loss: 0.0705\tvalidation loss: 0.1623\t validation accuracy: 0.9533\n",
      "iteration number: 1775\t training loss: 0.0712\tvalidation loss: 0.1634\t validation accuracy: 0.9511\n",
      "iteration number: 1776\t training loss: 0.0703\tvalidation loss: 0.1624\t validation accuracy: 0.9533\n",
      "iteration number: 1777\t training loss: 0.0696\tvalidation loss: 0.1620\t validation accuracy: 0.9533\n",
      "iteration number: 1778\t training loss: 0.0700\tvalidation loss: 0.1627\t validation accuracy: 0.9533\n",
      "iteration number: 1779\t training loss: 0.0706\tvalidation loss: 0.1629\t validation accuracy: 0.9533\n",
      "iteration number: 1780\t training loss: 0.0691\tvalidation loss: 0.1586\t validation accuracy: 0.9533\n",
      "iteration number: 1781\t training loss: 0.0691\tvalidation loss: 0.1579\t validation accuracy: 0.9533\n",
      "iteration number: 1782\t training loss: 0.0701\tvalidation loss: 0.1582\t validation accuracy: 0.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1783\t training loss: 0.0696\tvalidation loss: 0.1581\t validation accuracy: 0.9511\n",
      "iteration number: 1784\t training loss: 0.0695\tvalidation loss: 0.1589\t validation accuracy: 0.9533\n",
      "iteration number: 1785\t training loss: 0.0701\tvalidation loss: 0.1575\t validation accuracy: 0.9533\n",
      "iteration number: 1786\t training loss: 0.0701\tvalidation loss: 0.1575\t validation accuracy: 0.9556\n",
      "iteration number: 1787\t training loss: 0.0700\tvalidation loss: 0.1575\t validation accuracy: 0.9556\n",
      "iteration number: 1788\t training loss: 0.0695\tvalidation loss: 0.1579\t validation accuracy: 0.9533\n",
      "iteration number: 1789\t training loss: 0.0694\tvalidation loss: 0.1581\t validation accuracy: 0.9511\n",
      "iteration number: 1790\t training loss: 0.0691\tvalidation loss: 0.1572\t validation accuracy: 0.9511\n",
      "iteration number: 1791\t training loss: 0.0690\tvalidation loss: 0.1565\t validation accuracy: 0.9489\n",
      "iteration number: 1792\t training loss: 0.0685\tvalidation loss: 0.1566\t validation accuracy: 0.9533\n",
      "iteration number: 1793\t training loss: 0.0694\tvalidation loss: 0.1571\t validation accuracy: 0.9511\n",
      "iteration number: 1794\t training loss: 0.0694\tvalidation loss: 0.1580\t validation accuracy: 0.9489\n",
      "iteration number: 1795\t training loss: 0.0690\tvalidation loss: 0.1585\t validation accuracy: 0.9511\n",
      "iteration number: 1796\t training loss: 0.0685\tvalidation loss: 0.1578\t validation accuracy: 0.9489\n",
      "iteration number: 1797\t training loss: 0.0684\tvalidation loss: 0.1567\t validation accuracy: 0.9511\n",
      "iteration number: 1798\t training loss: 0.0684\tvalidation loss: 0.1561\t validation accuracy: 0.9533\n",
      "iteration number: 1799\t training loss: 0.0684\tvalidation loss: 0.1572\t validation accuracy: 0.9489\n",
      "iteration number: 1800\t training loss: 0.0678\tvalidation loss: 0.1569\t validation accuracy: 0.9533\n",
      "iteration number: 1801\t training loss: 0.0679\tvalidation loss: 0.1587\t validation accuracy: 0.9533\n",
      "iteration number: 1802\t training loss: 0.0688\tvalidation loss: 0.1613\t validation accuracy: 0.9489\n",
      "iteration number: 1803\t training loss: 0.0682\tvalidation loss: 0.1601\t validation accuracy: 0.9511\n",
      "iteration number: 1804\t training loss: 0.0680\tvalidation loss: 0.1585\t validation accuracy: 0.9489\n",
      "iteration number: 1805\t training loss: 0.0684\tvalidation loss: 0.1591\t validation accuracy: 0.9489\n",
      "iteration number: 1806\t training loss: 0.0678\tvalidation loss: 0.1583\t validation accuracy: 0.9556\n",
      "iteration number: 1807\t training loss: 0.0679\tvalidation loss: 0.1591\t validation accuracy: 0.9556\n",
      "iteration number: 1808\t training loss: 0.0678\tvalidation loss: 0.1591\t validation accuracy: 0.9533\n",
      "iteration number: 1809\t training loss: 0.0684\tvalidation loss: 0.1600\t validation accuracy: 0.9511\n",
      "iteration number: 1810\t training loss: 0.0694\tvalidation loss: 0.1621\t validation accuracy: 0.9511\n",
      "iteration number: 1811\t training loss: 0.0683\tvalidation loss: 0.1585\t validation accuracy: 0.9489\n",
      "iteration number: 1812\t training loss: 0.0677\tvalidation loss: 0.1581\t validation accuracy: 0.9489\n",
      "iteration number: 1813\t training loss: 0.0677\tvalidation loss: 0.1581\t validation accuracy: 0.9533\n",
      "iteration number: 1814\t training loss: 0.0678\tvalidation loss: 0.1581\t validation accuracy: 0.9489\n",
      "iteration number: 1815\t training loss: 0.0678\tvalidation loss: 0.1585\t validation accuracy: 0.9511\n",
      "iteration number: 1816\t training loss: 0.0694\tvalidation loss: 0.1613\t validation accuracy: 0.9489\n",
      "iteration number: 1817\t training loss: 0.0694\tvalidation loss: 0.1618\t validation accuracy: 0.9511\n",
      "iteration number: 1818\t training loss: 0.0695\tvalidation loss: 0.1616\t validation accuracy: 0.9489\n",
      "iteration number: 1819\t training loss: 0.0674\tvalidation loss: 0.1582\t validation accuracy: 0.9511\n",
      "iteration number: 1820\t training loss: 0.0674\tvalidation loss: 0.1581\t validation accuracy: 0.9489\n",
      "iteration number: 1821\t training loss: 0.0672\tvalidation loss: 0.1573\t validation accuracy: 0.9511\n",
      "iteration number: 1822\t training loss: 0.0672\tvalidation loss: 0.1574\t validation accuracy: 0.9511\n",
      "iteration number: 1823\t training loss: 0.0681\tvalidation loss: 0.1592\t validation accuracy: 0.9467\n",
      "iteration number: 1824\t training loss: 0.0679\tvalidation loss: 0.1584\t validation accuracy: 0.9489\n",
      "iteration number: 1825\t training loss: 0.0670\tvalidation loss: 0.1562\t validation accuracy: 0.9467\n",
      "iteration number: 1826\t training loss: 0.0671\tvalidation loss: 0.1546\t validation accuracy: 0.9489\n",
      "iteration number: 1827\t training loss: 0.0668\tvalidation loss: 0.1538\t validation accuracy: 0.9533\n",
      "iteration number: 1828\t training loss: 0.0667\tvalidation loss: 0.1542\t validation accuracy: 0.9533\n",
      "iteration number: 1829\t training loss: 0.0672\tvalidation loss: 0.1549\t validation accuracy: 0.9578\n",
      "iteration number: 1830\t training loss: 0.0668\tvalidation loss: 0.1553\t validation accuracy: 0.9556\n",
      "iteration number: 1831\t training loss: 0.0672\tvalidation loss: 0.1571\t validation accuracy: 0.9533\n",
      "iteration number: 1832\t training loss: 0.0678\tvalidation loss: 0.1586\t validation accuracy: 0.9533\n",
      "iteration number: 1833\t training loss: 0.0679\tvalidation loss: 0.1585\t validation accuracy: 0.9533\n",
      "iteration number: 1834\t training loss: 0.0672\tvalidation loss: 0.1570\t validation accuracy: 0.9533\n",
      "iteration number: 1835\t training loss: 0.0667\tvalidation loss: 0.1555\t validation accuracy: 0.9511\n",
      "iteration number: 1836\t training loss: 0.0670\tvalidation loss: 0.1569\t validation accuracy: 0.9556\n",
      "iteration number: 1837\t training loss: 0.0669\tvalidation loss: 0.1567\t validation accuracy: 0.9556\n",
      "iteration number: 1838\t training loss: 0.0683\tvalidation loss: 0.1589\t validation accuracy: 0.9533\n",
      "iteration number: 1839\t training loss: 0.0690\tvalidation loss: 0.1598\t validation accuracy: 0.9511\n",
      "iteration number: 1840\t training loss: 0.0688\tvalidation loss: 0.1592\t validation accuracy: 0.9511\n",
      "iteration number: 1841\t training loss: 0.0684\tvalidation loss: 0.1581\t validation accuracy: 0.9556\n",
      "iteration number: 1842\t training loss: 0.0678\tvalidation loss: 0.1573\t validation accuracy: 0.9533\n",
      "iteration number: 1843\t training loss: 0.0681\tvalidation loss: 0.1581\t validation accuracy: 0.9556\n",
      "iteration number: 1844\t training loss: 0.0676\tvalidation loss: 0.1563\t validation accuracy: 0.9533\n",
      "iteration number: 1845\t training loss: 0.0689\tvalidation loss: 0.1598\t validation accuracy: 0.9556\n",
      "iteration number: 1846\t training loss: 0.0671\tvalidation loss: 0.1564\t validation accuracy: 0.9511\n",
      "iteration number: 1847\t training loss: 0.0672\tvalidation loss: 0.1563\t validation accuracy: 0.9533\n",
      "iteration number: 1848\t training loss: 0.0669\tvalidation loss: 0.1547\t validation accuracy: 0.9533\n",
      "iteration number: 1849\t training loss: 0.0664\tvalidation loss: 0.1532\t validation accuracy: 0.9511\n",
      "iteration number: 1850\t training loss: 0.0662\tvalidation loss: 0.1543\t validation accuracy: 0.9489\n",
      "iteration number: 1851\t training loss: 0.0664\tvalidation loss: 0.1555\t validation accuracy: 0.9511\n",
      "iteration number: 1852\t training loss: 0.0669\tvalidation loss: 0.1576\t validation accuracy: 0.9533\n",
      "iteration number: 1853\t training loss: 0.0662\tvalidation loss: 0.1560\t validation accuracy: 0.9533\n",
      "iteration number: 1854\t training loss: 0.0665\tvalidation loss: 0.1558\t validation accuracy: 0.9511\n",
      "iteration number: 1855\t training loss: 0.0664\tvalidation loss: 0.1554\t validation accuracy: 0.9533\n",
      "iteration number: 1856\t training loss: 0.0666\tvalidation loss: 0.1557\t validation accuracy: 0.9533\n",
      "iteration number: 1857\t training loss: 0.0659\tvalidation loss: 0.1550\t validation accuracy: 0.9533\n",
      "iteration number: 1858\t training loss: 0.0677\tvalidation loss: 0.1594\t validation accuracy: 0.9556\n",
      "iteration number: 1859\t training loss: 0.0668\tvalidation loss: 0.1575\t validation accuracy: 0.9556\n",
      "iteration number: 1860\t training loss: 0.0657\tvalidation loss: 0.1542\t validation accuracy: 0.9578\n",
      "iteration number: 1861\t training loss: 0.0658\tvalidation loss: 0.1528\t validation accuracy: 0.9556\n",
      "iteration number: 1862\t training loss: 0.0657\tvalidation loss: 0.1518\t validation accuracy: 0.9533\n",
      "iteration number: 1863\t training loss: 0.0655\tvalidation loss: 0.1531\t validation accuracy: 0.9556\n",
      "iteration number: 1864\t training loss: 0.0655\tvalidation loss: 0.1549\t validation accuracy: 0.9511\n",
      "iteration number: 1865\t training loss: 0.0659\tvalidation loss: 0.1570\t validation accuracy: 0.9489\n",
      "iteration number: 1866\t training loss: 0.0658\tvalidation loss: 0.1575\t validation accuracy: 0.9511\n",
      "iteration number: 1867\t training loss: 0.0662\tvalidation loss: 0.1578\t validation accuracy: 0.9489\n",
      "iteration number: 1868\t training loss: 0.0657\tvalidation loss: 0.1577\t validation accuracy: 0.9511\n",
      "iteration number: 1869\t training loss: 0.0666\tvalidation loss: 0.1588\t validation accuracy: 0.9533\n",
      "iteration number: 1870\t training loss: 0.0666\tvalidation loss: 0.1589\t validation accuracy: 0.9533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1871\t training loss: 0.0667\tvalidation loss: 0.1572\t validation accuracy: 0.9533\n",
      "iteration number: 1872\t training loss: 0.0674\tvalidation loss: 0.1576\t validation accuracy: 0.9489\n",
      "iteration number: 1873\t training loss: 0.0676\tvalidation loss: 0.1588\t validation accuracy: 0.9489\n",
      "iteration number: 1874\t training loss: 0.0668\tvalidation loss: 0.1592\t validation accuracy: 0.9489\n",
      "iteration number: 1875\t training loss: 0.0664\tvalidation loss: 0.1613\t validation accuracy: 0.9533\n",
      "iteration number: 1876\t training loss: 0.0656\tvalidation loss: 0.1595\t validation accuracy: 0.9533\n",
      "iteration number: 1877\t training loss: 0.0647\tvalidation loss: 0.1566\t validation accuracy: 0.9533\n",
      "iteration number: 1878\t training loss: 0.0651\tvalidation loss: 0.1570\t validation accuracy: 0.9511\n",
      "iteration number: 1879\t training loss: 0.0648\tvalidation loss: 0.1569\t validation accuracy: 0.9511\n",
      "iteration number: 1880\t training loss: 0.0655\tvalidation loss: 0.1594\t validation accuracy: 0.9533\n",
      "iteration number: 1881\t training loss: 0.0650\tvalidation loss: 0.1576\t validation accuracy: 0.9533\n",
      "iteration number: 1882\t training loss: 0.0647\tvalidation loss: 0.1556\t validation accuracy: 0.9533\n",
      "iteration number: 1883\t training loss: 0.0648\tvalidation loss: 0.1545\t validation accuracy: 0.9533\n",
      "iteration number: 1884\t training loss: 0.0649\tvalidation loss: 0.1545\t validation accuracy: 0.9511\n",
      "iteration number: 1885\t training loss: 0.0647\tvalidation loss: 0.1557\t validation accuracy: 0.9511\n",
      "iteration number: 1886\t training loss: 0.0648\tvalidation loss: 0.1569\t validation accuracy: 0.9533\n",
      "iteration number: 1887\t training loss: 0.0646\tvalidation loss: 0.1566\t validation accuracy: 0.9533\n",
      "iteration number: 1888\t training loss: 0.0667\tvalidation loss: 0.1561\t validation accuracy: 0.9489\n",
      "iteration number: 1889\t training loss: 0.0659\tvalidation loss: 0.1559\t validation accuracy: 0.9533\n",
      "iteration number: 1890\t training loss: 0.0655\tvalidation loss: 0.1599\t validation accuracy: 0.9511\n",
      "iteration number: 1891\t training loss: 0.0651\tvalidation loss: 0.1587\t validation accuracy: 0.9533\n",
      "iteration number: 1892\t training loss: 0.0646\tvalidation loss: 0.1574\t validation accuracy: 0.9533\n",
      "iteration number: 1893\t training loss: 0.0644\tvalidation loss: 0.1568\t validation accuracy: 0.9489\n",
      "iteration number: 1894\t training loss: 0.0650\tvalidation loss: 0.1588\t validation accuracy: 0.9489\n",
      "iteration number: 1895\t training loss: 0.0644\tvalidation loss: 0.1574\t validation accuracy: 0.9511\n",
      "iteration number: 1896\t training loss: 0.0643\tvalidation loss: 0.1566\t validation accuracy: 0.9533\n",
      "iteration number: 1897\t training loss: 0.0647\tvalidation loss: 0.1573\t validation accuracy: 0.9489\n",
      "iteration number: 1898\t training loss: 0.0657\tvalidation loss: 0.1591\t validation accuracy: 0.9533\n",
      "iteration number: 1899\t training loss: 0.0653\tvalidation loss: 0.1589\t validation accuracy: 0.9489\n",
      "iteration number: 1900\t training loss: 0.0646\tvalidation loss: 0.1559\t validation accuracy: 0.9489\n",
      "iteration number: 1901\t training loss: 0.0653\tvalidation loss: 0.1578\t validation accuracy: 0.9489\n",
      "iteration number: 1902\t training loss: 0.0651\tvalidation loss: 0.1560\t validation accuracy: 0.9467\n",
      "iteration number: 1903\t training loss: 0.0644\tvalidation loss: 0.1549\t validation accuracy: 0.9489\n",
      "iteration number: 1904\t training loss: 0.0645\tvalidation loss: 0.1550\t validation accuracy: 0.9511\n",
      "iteration number: 1905\t training loss: 0.0654\tvalidation loss: 0.1569\t validation accuracy: 0.9511\n",
      "iteration number: 1906\t training loss: 0.0658\tvalidation loss: 0.1573\t validation accuracy: 0.9511\n",
      "iteration number: 1907\t training loss: 0.0657\tvalidation loss: 0.1570\t validation accuracy: 0.9511\n",
      "iteration number: 1908\t training loss: 0.0647\tvalidation loss: 0.1560\t validation accuracy: 0.9511\n",
      "iteration number: 1909\t training loss: 0.0642\tvalidation loss: 0.1541\t validation accuracy: 0.9533\n",
      "iteration number: 1910\t training loss: 0.0645\tvalidation loss: 0.1550\t validation accuracy: 0.9511\n",
      "iteration number: 1911\t training loss: 0.0659\tvalidation loss: 0.1566\t validation accuracy: 0.9489\n",
      "iteration number: 1912\t training loss: 0.0648\tvalidation loss: 0.1546\t validation accuracy: 0.9533\n",
      "iteration number: 1913\t training loss: 0.0656\tvalidation loss: 0.1568\t validation accuracy: 0.9511\n",
      "iteration number: 1914\t training loss: 0.0649\tvalidation loss: 0.1558\t validation accuracy: 0.9533\n",
      "iteration number: 1915\t training loss: 0.0647\tvalidation loss: 0.1546\t validation accuracy: 0.9511\n",
      "iteration number: 1916\t training loss: 0.0643\tvalidation loss: 0.1550\t validation accuracy: 0.9533\n",
      "iteration number: 1917\t training loss: 0.0647\tvalidation loss: 0.1556\t validation accuracy: 0.9511\n",
      "iteration number: 1918\t training loss: 0.0648\tvalidation loss: 0.1582\t validation accuracy: 0.9556\n",
      "iteration number: 1919\t training loss: 0.0648\tvalidation loss: 0.1569\t validation accuracy: 0.9556\n",
      "iteration number: 1920\t training loss: 0.0645\tvalidation loss: 0.1551\t validation accuracy: 0.9556\n",
      "iteration number: 1921\t training loss: 0.0646\tvalidation loss: 0.1547\t validation accuracy: 0.9578\n",
      "iteration number: 1922\t training loss: 0.0645\tvalidation loss: 0.1557\t validation accuracy: 0.9556\n",
      "iteration number: 1923\t training loss: 0.0648\tvalidation loss: 0.1564\t validation accuracy: 0.9556\n",
      "iteration number: 1924\t training loss: 0.0651\tvalidation loss: 0.1543\t validation accuracy: 0.9578\n",
      "iteration number: 1925\t training loss: 0.0648\tvalidation loss: 0.1542\t validation accuracy: 0.9578\n",
      "iteration number: 1926\t training loss: 0.0643\tvalidation loss: 0.1533\t validation accuracy: 0.9578\n",
      "iteration number: 1927\t training loss: 0.0647\tvalidation loss: 0.1531\t validation accuracy: 0.9578\n",
      "iteration number: 1928\t training loss: 0.0645\tvalidation loss: 0.1546\t validation accuracy: 0.9533\n",
      "iteration number: 1929\t training loss: 0.0638\tvalidation loss: 0.1522\t validation accuracy: 0.9556\n",
      "iteration number: 1930\t training loss: 0.0637\tvalidation loss: 0.1523\t validation accuracy: 0.9556\n",
      "iteration number: 1931\t training loss: 0.0632\tvalidation loss: 0.1545\t validation accuracy: 0.9556\n",
      "iteration number: 1932\t training loss: 0.0633\tvalidation loss: 0.1546\t validation accuracy: 0.9533\n",
      "iteration number: 1933\t training loss: 0.0633\tvalidation loss: 0.1536\t validation accuracy: 0.9556\n",
      "iteration number: 1934\t training loss: 0.0639\tvalidation loss: 0.1546\t validation accuracy: 0.9533\n",
      "iteration number: 1935\t training loss: 0.0642\tvalidation loss: 0.1553\t validation accuracy: 0.9533\n",
      "iteration number: 1936\t training loss: 0.0642\tvalidation loss: 0.1555\t validation accuracy: 0.9533\n",
      "iteration number: 1937\t training loss: 0.0638\tvalidation loss: 0.1542\t validation accuracy: 0.9511\n",
      "iteration number: 1938\t training loss: 0.0639\tvalidation loss: 0.1531\t validation accuracy: 0.9533\n",
      "iteration number: 1939\t training loss: 0.0647\tvalidation loss: 0.1539\t validation accuracy: 0.9556\n",
      "iteration number: 1940\t training loss: 0.0650\tvalidation loss: 0.1559\t validation accuracy: 0.9533\n",
      "iteration number: 1941\t training loss: 0.0645\tvalidation loss: 0.1559\t validation accuracy: 0.9533\n",
      "iteration number: 1942\t training loss: 0.0636\tvalidation loss: 0.1518\t validation accuracy: 0.9556\n",
      "iteration number: 1943\t training loss: 0.0633\tvalidation loss: 0.1522\t validation accuracy: 0.9556\n",
      "iteration number: 1944\t training loss: 0.0636\tvalidation loss: 0.1526\t validation accuracy: 0.9556\n",
      "iteration number: 1945\t training loss: 0.0630\tvalidation loss: 0.1536\t validation accuracy: 0.9556\n",
      "iteration number: 1946\t training loss: 0.0635\tvalidation loss: 0.1539\t validation accuracy: 0.9556\n",
      "iteration number: 1947\t training loss: 0.0630\tvalidation loss: 0.1531\t validation accuracy: 0.9556\n",
      "iteration number: 1948\t training loss: 0.0635\tvalidation loss: 0.1526\t validation accuracy: 0.9556\n",
      "iteration number: 1949\t training loss: 0.0636\tvalidation loss: 0.1524\t validation accuracy: 0.9533\n",
      "iteration number: 1950\t training loss: 0.0625\tvalidation loss: 0.1542\t validation accuracy: 0.9578\n",
      "iteration number: 1951\t training loss: 0.0624\tvalidation loss: 0.1555\t validation accuracy: 0.9556\n",
      "iteration number: 1952\t training loss: 0.0625\tvalidation loss: 0.1547\t validation accuracy: 0.9578\n",
      "iteration number: 1953\t training loss: 0.0627\tvalidation loss: 0.1552\t validation accuracy: 0.9578\n",
      "iteration number: 1954\t training loss: 0.0626\tvalidation loss: 0.1537\t validation accuracy: 0.9578\n",
      "iteration number: 1955\t training loss: 0.0623\tvalidation loss: 0.1532\t validation accuracy: 0.9578\n",
      "iteration number: 1956\t training loss: 0.0621\tvalidation loss: 0.1526\t validation accuracy: 0.9578\n",
      "iteration number: 1957\t training loss: 0.0623\tvalidation loss: 0.1518\t validation accuracy: 0.9556\n",
      "iteration number: 1958\t training loss: 0.0623\tvalidation loss: 0.1524\t validation accuracy: 0.9556\n",
      "iteration number: 1959\t training loss: 0.0626\tvalidation loss: 0.1521\t validation accuracy: 0.9533\n",
      "iteration number: 1960\t training loss: 0.0626\tvalidation loss: 0.1532\t validation accuracy: 0.9511\n",
      "iteration number: 1961\t training loss: 0.0620\tvalidation loss: 0.1543\t validation accuracy: 0.9556\n",
      "iteration number: 1962\t training loss: 0.0622\tvalidation loss: 0.1542\t validation accuracy: 0.9533\n",
      "**MLP with early stopping**\n",
      "Training accuracy: 0.9889\n",
      "Validation accuracy: 0.9533\n",
      "Training loss: 0.0622\n",
      "Validation loss: 0.1542\n",
      "Number of iterations: 1962\n"
     ]
    }
   ],
   "source": [
    "mlp_early_stopping = MultiLayerPerceptron(\n",
    "    X, Y, hidden_size=50, \n",
    "    activation='relu', \n",
    "    dropout=False, \n",
    "    dropout_rate=0\n",
    ")\n",
    "mlp_early_stopping.train(\n",
    "    optimizer='sgd',\n",
    "    min_iterations=500,\n",
    "    max_iterations=5000,\n",
    "    initial_step=1e-1,\n",
    "    batch_size=64,\n",
    "    early_stopping=True,\n",
    "    early_stopping_lookbehind=100,\n",
    "    early_stopping_delta=1e-4, \n",
    "    vectorized=True,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"**MLP with early stopping**\")\n",
    "print(\"Training accuracy: {:.4f}\".format(mlp_early_stopping.accuracy_on_train()))\n",
    "print(\"Validation accuracy: {:.4f}\".format(mlp_early_stopping.accuracy_on_validation()))\n",
    "print(\"Training loss: {:.4f}\".format(mlp_early_stopping.training_losses_history[-1]))\n",
    "print(\"Validation loss: {:.4f}\".format(mlp_early_stopping.validation_losses_history[-1]))\n",
    "print(\"Number of iterations: {:d}\".format(len(mlp_early_stopping.training_losses_history)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_early_stopping.plot_validation_prediction(sample_id=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â MLP with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\t training loss: 2.3025\tvalidation loss: 2.3028\t validation accuracy: 0.0911\n",
      "iteration number: 2\t training loss: 2.3024\tvalidation loss: 2.3029\t validation accuracy: 0.0911\n",
      "iteration number: 3\t training loss: 2.3023\tvalidation loss: 2.3032\t validation accuracy: 0.0911\n",
      "iteration number: 4\t training loss: 2.3022\tvalidation loss: 2.3036\t validation accuracy: 0.0844\n",
      "iteration number: 5\t training loss: 2.3021\tvalidation loss: 2.3042\t validation accuracy: 0.0844\n",
      "iteration number: 6\t training loss: 2.3020\tvalidation loss: 2.3046\t validation accuracy: 0.0844\n",
      "iteration number: 7\t training loss: 2.3020\tvalidation loss: 2.3050\t validation accuracy: 0.0844\n",
      "iteration number: 8\t training loss: 2.3019\tvalidation loss: 2.3054\t validation accuracy: 0.0844\n",
      "iteration number: 9\t training loss: 2.3020\tvalidation loss: 2.3059\t validation accuracy: 0.0844\n",
      "iteration number: 10\t training loss: 2.3020\tvalidation loss: 2.3062\t validation accuracy: 0.0844\n",
      "iteration number: 11\t training loss: 2.3020\tvalidation loss: 2.3063\t validation accuracy: 0.0844\n",
      "iteration number: 12\t training loss: 2.3021\tvalidation loss: 2.3066\t validation accuracy: 0.0844\n",
      "iteration number: 13\t training loss: 2.3022\tvalidation loss: 2.3069\t validation accuracy: 0.0844\n",
      "iteration number: 14\t training loss: 2.3022\tvalidation loss: 2.3072\t validation accuracy: 0.0844\n",
      "iteration number: 15\t training loss: 2.3020\tvalidation loss: 2.3074\t validation accuracy: 0.0844\n",
      "iteration number: 16\t training loss: 2.3017\tvalidation loss: 2.3077\t validation accuracy: 0.0844\n",
      "iteration number: 17\t training loss: 2.3013\tvalidation loss: 2.3077\t validation accuracy: 0.0844\n",
      "iteration number: 18\t training loss: 2.3004\tvalidation loss: 2.3070\t validation accuracy: 0.0844\n",
      "iteration number: 19\t training loss: 2.2993\tvalidation loss: 2.3059\t validation accuracy: 0.0844\n",
      "iteration number: 20\t training loss: 2.2976\tvalidation loss: 2.3042\t validation accuracy: 0.0844\n",
      "iteration number: 21\t training loss: 2.2956\tvalidation loss: 2.3018\t validation accuracy: 0.0844\n",
      "iteration number: 22\t training loss: 2.2929\tvalidation loss: 2.2989\t validation accuracy: 0.0844\n",
      "iteration number: 23\t training loss: 2.2897\tvalidation loss: 2.2955\t validation accuracy: 0.0844\n",
      "iteration number: 24\t training loss: 2.2859\tvalidation loss: 2.2916\t validation accuracy: 0.0844\n",
      "iteration number: 25\t training loss: 2.2815\tvalidation loss: 2.2872\t validation accuracy: 0.0844\n",
      "iteration number: 26\t training loss: 2.2762\tvalidation loss: 2.2824\t validation accuracy: 0.0844\n",
      "iteration number: 27\t training loss: 2.2699\tvalidation loss: 2.2766\t validation accuracy: 0.1089\n",
      "iteration number: 28\t training loss: 2.2621\tvalidation loss: 2.2695\t validation accuracy: 0.1444\n",
      "iteration number: 29\t training loss: 2.2529\tvalidation loss: 2.2606\t validation accuracy: 0.1867\n",
      "iteration number: 30\t training loss: 2.2420\tvalidation loss: 2.2505\t validation accuracy: 0.1978\n",
      "iteration number: 31\t training loss: 2.2287\tvalidation loss: 2.2377\t validation accuracy: 0.2356\n",
      "iteration number: 32\t training loss: 2.2134\tvalidation loss: 2.2231\t validation accuracy: 0.2844\n",
      "iteration number: 33\t training loss: 2.1953\tvalidation loss: 2.2053\t validation accuracy: 0.3867\n",
      "iteration number: 34\t training loss: 2.1743\tvalidation loss: 2.1850\t validation accuracy: 0.4489\n",
      "iteration number: 35\t training loss: 2.1491\tvalidation loss: 2.1605\t validation accuracy: 0.4600\n",
      "iteration number: 36\t training loss: 2.1191\tvalidation loss: 2.1319\t validation accuracy: 0.4400\n",
      "iteration number: 37\t training loss: 2.0819\tvalidation loss: 2.0963\t validation accuracy: 0.4311\n",
      "iteration number: 38\t training loss: 2.0388\tvalidation loss: 2.0550\t validation accuracy: 0.4667\n",
      "iteration number: 39\t training loss: 1.9875\tvalidation loss: 2.0058\t validation accuracy: 0.5222\n",
      "iteration number: 40\t training loss: 1.9296\tvalidation loss: 1.9494\t validation accuracy: 0.5800\n",
      "iteration number: 41\t training loss: 1.8636\tvalidation loss: 1.8849\t validation accuracy: 0.6067\n",
      "iteration number: 42\t training loss: 1.7907\tvalidation loss: 1.8147\t validation accuracy: 0.5889\n",
      "iteration number: 43\t training loss: 1.7139\tvalidation loss: 1.7428\t validation accuracy: 0.5556\n",
      "iteration number: 44\t training loss: 1.6313\tvalidation loss: 1.6645\t validation accuracy: 0.5378\n",
      "iteration number: 45\t training loss: 1.5452\tvalidation loss: 1.5813\t validation accuracy: 0.5244\n",
      "iteration number: 46\t training loss: 1.4542\tvalidation loss: 1.4904\t validation accuracy: 0.5422\n",
      "iteration number: 47\t training loss: 1.3619\tvalidation loss: 1.3959\t validation accuracy: 0.5933\n",
      "iteration number: 48\t training loss: 1.2724\tvalidation loss: 1.3038\t validation accuracy: 0.6667\n",
      "iteration number: 49\t training loss: 1.1921\tvalidation loss: 1.2199\t validation accuracy: 0.7067\n",
      "iteration number: 50\t training loss: 1.1138\tvalidation loss: 1.1411\t validation accuracy: 0.7044\n",
      "iteration number: 51\t training loss: 1.0366\tvalidation loss: 1.0650\t validation accuracy: 0.7244\n",
      "iteration number: 52\t training loss: 0.9632\tvalidation loss: 0.9922\t validation accuracy: 0.7311\n",
      "iteration number: 53\t training loss: 0.8870\tvalidation loss: 0.9160\t validation accuracy: 0.7778\n",
      "iteration number: 54\t training loss: 0.8225\tvalidation loss: 0.8575\t validation accuracy: 0.7978\n",
      "iteration number: 55\t training loss: 0.7733\tvalidation loss: 0.8082\t validation accuracy: 0.7867\n",
      "iteration number: 56\t training loss: 0.7426\tvalidation loss: 0.7784\t validation accuracy: 0.7800\n",
      "iteration number: 57\t training loss: 0.6918\tvalidation loss: 0.7329\t validation accuracy: 0.7978\n",
      "iteration number: 58\t training loss: 0.6498\tvalidation loss: 0.7019\t validation accuracy: 0.7889\n",
      "iteration number: 59\t training loss: 0.6177\tvalidation loss: 0.6749\t validation accuracy: 0.7844\n",
      "iteration number: 60\t training loss: 0.5800\tvalidation loss: 0.6337\t validation accuracy: 0.8000\n",
      "iteration number: 61\t training loss: 0.5433\tvalidation loss: 0.5939\t validation accuracy: 0.8289\n",
      "iteration number: 62\t training loss: 0.5098\tvalidation loss: 0.5591\t validation accuracy: 0.8333\n",
      "iteration number: 63\t training loss: 0.4965\tvalidation loss: 0.5523\t validation accuracy: 0.8289\n",
      "iteration number: 64\t training loss: 0.4920\tvalidation loss: 0.5556\t validation accuracy: 0.8222\n",
      "iteration number: 65\t training loss: 0.4576\tvalidation loss: 0.5177\t validation accuracy: 0.8311\n",
      "iteration number: 66\t training loss: 0.4292\tvalidation loss: 0.4870\t validation accuracy: 0.8378\n",
      "iteration number: 67\t training loss: 0.4281\tvalidation loss: 0.4761\t validation accuracy: 0.8467\n",
      "iteration number: 68\t training loss: 0.4249\tvalidation loss: 0.4725\t validation accuracy: 0.8422\n",
      "iteration number: 69\t training loss: 0.3998\tvalidation loss: 0.4561\t validation accuracy: 0.8489\n",
      "iteration number: 70\t training loss: 0.3652\tvalidation loss: 0.4325\t validation accuracy: 0.8533\n",
      "iteration number: 71\t training loss: 0.3519\tvalidation loss: 0.4255\t validation accuracy: 0.8600\n",
      "iteration number: 72\t training loss: 0.3444\tvalidation loss: 0.4209\t validation accuracy: 0.8489\n",
      "iteration number: 73\t training loss: 0.3460\tvalidation loss: 0.4311\t validation accuracy: 0.8400\n",
      "iteration number: 74\t training loss: 0.3560\tvalidation loss: 0.4512\t validation accuracy: 0.8400\n",
      "iteration number: 75\t training loss: 0.3651\tvalidation loss: 0.4683\t validation accuracy: 0.8333\n",
      "iteration number: 76\t training loss: 0.3513\tvalidation loss: 0.4499\t validation accuracy: 0.8444\n",
      "iteration number: 77\t training loss: 0.3255\tvalidation loss: 0.4144\t validation accuracy: 0.8556\n",
      "iteration number: 78\t training loss: 0.3043\tvalidation loss: 0.3846\t validation accuracy: 0.8622\n",
      "iteration number: 79\t training loss: 0.2971\tvalidation loss: 0.3822\t validation accuracy: 0.8511\n",
      "iteration number: 80\t training loss: 0.2987\tvalidation loss: 0.3927\t validation accuracy: 0.8467\n",
      "iteration number: 81\t training loss: 0.3017\tvalidation loss: 0.4002\t validation accuracy: 0.8556\n",
      "iteration number: 82\t training loss: 0.2945\tvalidation loss: 0.3793\t validation accuracy: 0.8600\n",
      "iteration number: 83\t training loss: 0.2939\tvalidation loss: 0.3639\t validation accuracy: 0.8644\n",
      "iteration number: 84\t training loss: 0.2809\tvalidation loss: 0.3371\t validation accuracy: 0.8711\n",
      "iteration number: 85\t training loss: 0.2768\tvalidation loss: 0.3180\t validation accuracy: 0.8867\n",
      "iteration number: 86\t training loss: 0.2727\tvalidation loss: 0.3111\t validation accuracy: 0.8867\n",
      "iteration number: 87\t training loss: 0.2598\tvalidation loss: 0.2994\t validation accuracy: 0.8978\n",
      "iteration number: 88\t training loss: 0.2389\tvalidation loss: 0.2924\t validation accuracy: 0.9000\n",
      "iteration number: 89\t training loss: 0.2398\tvalidation loss: 0.3111\t validation accuracy: 0.8844\n",
      "iteration number: 90\t training loss: 0.2559\tvalidation loss: 0.3485\t validation accuracy: 0.8800\n",
      "iteration number: 91\t training loss: 0.2656\tvalidation loss: 0.3700\t validation accuracy: 0.8711\n",
      "iteration number: 92\t training loss: 0.2564\tvalidation loss: 0.3505\t validation accuracy: 0.8667\n",
      "iteration number: 93\t training loss: 0.2558\tvalidation loss: 0.3368\t validation accuracy: 0.8867\n",
      "iteration number: 94\t training loss: 0.2562\tvalidation loss: 0.3295\t validation accuracy: 0.8933\n",
      "iteration number: 95\t training loss: 0.2313\tvalidation loss: 0.2922\t validation accuracy: 0.8933\n",
      "iteration number: 96\t training loss: 0.2210\tvalidation loss: 0.2673\t validation accuracy: 0.9044\n",
      "iteration number: 97\t training loss: 0.2253\tvalidation loss: 0.2669\t validation accuracy: 0.9067\n",
      "iteration number: 98\t training loss: 0.2484\tvalidation loss: 0.2951\t validation accuracy: 0.8889\n",
      "iteration number: 99\t training loss: 0.2439\tvalidation loss: 0.3031\t validation accuracy: 0.8733\n",
      "iteration number: 100\t training loss: 0.2236\tvalidation loss: 0.3011\t validation accuracy: 0.8689\n",
      "iteration number: 101\t training loss: 0.2048\tvalidation loss: 0.2923\t validation accuracy: 0.8911\n",
      "iteration number: 102\t training loss: 0.2133\tvalidation loss: 0.3049\t validation accuracy: 0.9022\n",
      "iteration number: 103\t training loss: 0.2564\tvalidation loss: 0.3474\t validation accuracy: 0.8889\n",
      "iteration number: 104\t training loss: 0.2912\tvalidation loss: 0.3781\t validation accuracy: 0.8956\n",
      "iteration number: 105\t training loss: 0.2554\tvalidation loss: 0.3332\t validation accuracy: 0.9067\n",
      "iteration number: 106\t training loss: 0.2029\tvalidation loss: 0.2584\t validation accuracy: 0.9244\n",
      "iteration number: 107\t training loss: 0.1834\tvalidation loss: 0.2230\t validation accuracy: 0.9267\n",
      "iteration number: 108\t training loss: 0.2118\tvalidation loss: 0.2439\t validation accuracy: 0.9111\n",
      "iteration number: 109\t training loss: 0.2760\tvalidation loss: 0.3032\t validation accuracy: 0.8933\n",
      "iteration number: 110\t training loss: 0.2607\tvalidation loss: 0.2948\t validation accuracy: 0.8933\n",
      "iteration number: 111\t training loss: 0.2193\tvalidation loss: 0.2674\t validation accuracy: 0.9022\n",
      "iteration number: 112\t training loss: 0.1835\tvalidation loss: 0.2488\t validation accuracy: 0.9089\n",
      "iteration number: 113\t training loss: 0.1708\tvalidation loss: 0.2449\t validation accuracy: 0.9178\n",
      "iteration number: 114\t training loss: 0.1752\tvalidation loss: 0.2539\t validation accuracy: 0.9111\n",
      "iteration number: 115\t training loss: 0.1884\tvalidation loss: 0.2676\t validation accuracy: 0.9022\n",
      "iteration number: 116\t training loss: 0.1961\tvalidation loss: 0.2691\t validation accuracy: 0.9044\n",
      "iteration number: 117\t training loss: 0.1944\tvalidation loss: 0.2623\t validation accuracy: 0.9111\n",
      "iteration number: 118\t training loss: 0.1850\tvalidation loss: 0.2488\t validation accuracy: 0.9156\n",
      "iteration number: 119\t training loss: 0.1858\tvalidation loss: 0.2483\t validation accuracy: 0.9133\n",
      "iteration number: 120\t training loss: 0.1805\tvalidation loss: 0.2435\t validation accuracy: 0.9111\n",
      "iteration number: 121\t training loss: 0.1724\tvalidation loss: 0.2353\t validation accuracy: 0.9200\n",
      "iteration number: 122\t training loss: 0.1591\tvalidation loss: 0.2192\t validation accuracy: 0.9222\n",
      "iteration number: 123\t training loss: 0.1497\tvalidation loss: 0.2106\t validation accuracy: 0.9200\n",
      "iteration number: 124\t training loss: 0.1430\tvalidation loss: 0.2038\t validation accuracy: 0.9200\n",
      "iteration number: 125\t training loss: 0.1387\tvalidation loss: 0.1971\t validation accuracy: 0.9244\n",
      "iteration number: 126\t training loss: 0.1365\tvalidation loss: 0.1928\t validation accuracy: 0.9311\n",
      "iteration number: 127\t training loss: 0.1405\tvalidation loss: 0.1944\t validation accuracy: 0.9311\n",
      "iteration number: 128\t training loss: 0.1360\tvalidation loss: 0.1923\t validation accuracy: 0.9333\n",
      "iteration number: 129\t training loss: 0.1369\tvalidation loss: 0.1956\t validation accuracy: 0.9356\n",
      "iteration number: 130\t training loss: 0.1375\tvalidation loss: 0.1951\t validation accuracy: 0.9267\n",
      "iteration number: 131\t training loss: 0.1369\tvalidation loss: 0.1939\t validation accuracy: 0.9267\n",
      "iteration number: 132\t training loss: 0.1374\tvalidation loss: 0.1921\t validation accuracy: 0.9267\n",
      "iteration number: 133\t training loss: 0.1353\tvalidation loss: 0.1917\t validation accuracy: 0.9222\n",
      "iteration number: 134\t training loss: 0.1336\tvalidation loss: 0.1922\t validation accuracy: 0.9267\n",
      "iteration number: 135\t training loss: 0.1319\tvalidation loss: 0.1948\t validation accuracy: 0.9267\n",
      "iteration number: 136\t training loss: 0.1299\tvalidation loss: 0.1978\t validation accuracy: 0.9222\n",
      "iteration number: 137\t training loss: 0.1278\tvalidation loss: 0.1988\t validation accuracy: 0.9200\n",
      "iteration number: 138\t training loss: 0.1265\tvalidation loss: 0.1959\t validation accuracy: 0.9200\n",
      "iteration number: 139\t training loss: 0.1245\tvalidation loss: 0.1928\t validation accuracy: 0.9267\n",
      "iteration number: 140\t training loss: 0.1216\tvalidation loss: 0.1894\t validation accuracy: 0.9311\n",
      "iteration number: 141\t training loss: 0.1196\tvalidation loss: 0.1909\t validation accuracy: 0.9378\n",
      "iteration number: 142\t training loss: 0.1211\tvalidation loss: 0.1973\t validation accuracy: 0.9356\n",
      "iteration number: 143\t training loss: 0.1229\tvalidation loss: 0.2000\t validation accuracy: 0.9333\n",
      "iteration number: 144\t training loss: 0.1237\tvalidation loss: 0.2017\t validation accuracy: 0.9311\n",
      "iteration number: 145\t training loss: 0.1243\tvalidation loss: 0.2045\t validation accuracy: 0.9289\n",
      "iteration number: 146\t training loss: 0.1201\tvalidation loss: 0.1977\t validation accuracy: 0.9356\n",
      "iteration number: 147\t training loss: 0.1149\tvalidation loss: 0.1872\t validation accuracy: 0.9444\n",
      "iteration number: 148\t training loss: 0.1124\tvalidation loss: 0.1813\t validation accuracy: 0.9489\n",
      "iteration number: 149\t training loss: 0.1111\tvalidation loss: 0.1738\t validation accuracy: 0.9489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 150\t training loss: 0.1120\tvalidation loss: 0.1690\t validation accuracy: 0.9489\n",
      "iteration number: 151\t training loss: 0.1119\tvalidation loss: 0.1667\t validation accuracy: 0.9489\n",
      "iteration number: 152\t training loss: 0.1113\tvalidation loss: 0.1663\t validation accuracy: 0.9489\n",
      "iteration number: 153\t training loss: 0.1114\tvalidation loss: 0.1661\t validation accuracy: 0.9489\n",
      "iteration number: 154\t training loss: 0.1116\tvalidation loss: 0.1697\t validation accuracy: 0.9467\n",
      "iteration number: 155\t training loss: 0.1109\tvalidation loss: 0.1772\t validation accuracy: 0.9422\n",
      "iteration number: 156\t training loss: 0.1090\tvalidation loss: 0.1851\t validation accuracy: 0.9467\n",
      "iteration number: 157\t training loss: 0.1088\tvalidation loss: 0.1957\t validation accuracy: 0.9400\n",
      "iteration number: 158\t training loss: 0.1052\tvalidation loss: 0.1947\t validation accuracy: 0.9444\n",
      "iteration number: 159\t training loss: 0.1018\tvalidation loss: 0.1880\t validation accuracy: 0.9378\n",
      "iteration number: 160\t training loss: 0.1020\tvalidation loss: 0.1873\t validation accuracy: 0.9333\n",
      "iteration number: 161\t training loss: 0.1035\tvalidation loss: 0.1839\t validation accuracy: 0.9356\n",
      "iteration number: 162\t training loss: 0.1076\tvalidation loss: 0.1856\t validation accuracy: 0.9267\n",
      "iteration number: 163\t training loss: 0.1104\tvalidation loss: 0.1885\t validation accuracy: 0.9222\n",
      "iteration number: 164\t training loss: 0.1033\tvalidation loss: 0.1780\t validation accuracy: 0.9289\n",
      "iteration number: 165\t training loss: 0.0995\tvalidation loss: 0.1747\t validation accuracy: 0.9422\n",
      "iteration number: 166\t training loss: 0.1014\tvalidation loss: 0.1792\t validation accuracy: 0.9378\n",
      "iteration number: 167\t training loss: 0.1055\tvalidation loss: 0.1822\t validation accuracy: 0.9378\n",
      "iteration number: 168\t training loss: 0.1067\tvalidation loss: 0.1799\t validation accuracy: 0.9444\n",
      "iteration number: 169\t training loss: 0.1038\tvalidation loss: 0.1740\t validation accuracy: 0.9533\n",
      "iteration number: 170\t training loss: 0.1007\tvalidation loss: 0.1710\t validation accuracy: 0.9489\n",
      "iteration number: 171\t training loss: 0.1019\tvalidation loss: 0.1738\t validation accuracy: 0.9444\n",
      "iteration number: 172\t training loss: 0.0980\tvalidation loss: 0.1753\t validation accuracy: 0.9378\n",
      "iteration number: 173\t training loss: 0.0957\tvalidation loss: 0.1771\t validation accuracy: 0.9289\n",
      "iteration number: 174\t training loss: 0.0955\tvalidation loss: 0.1824\t validation accuracy: 0.9244\n",
      "iteration number: 175\t training loss: 0.0982\tvalidation loss: 0.1919\t validation accuracy: 0.9200\n",
      "iteration number: 176\t training loss: 0.0981\tvalidation loss: 0.1938\t validation accuracy: 0.9178\n",
      "iteration number: 177\t training loss: 0.0990\tvalidation loss: 0.1958\t validation accuracy: 0.9289\n",
      "iteration number: 178\t training loss: 0.1011\tvalidation loss: 0.1985\t validation accuracy: 0.9311\n",
      "iteration number: 179\t training loss: 0.1002\tvalidation loss: 0.1913\t validation accuracy: 0.9267\n",
      "iteration number: 180\t training loss: 0.0975\tvalidation loss: 0.1803\t validation accuracy: 0.9267\n",
      "iteration number: 181\t training loss: 0.0993\tvalidation loss: 0.1685\t validation accuracy: 0.9378\n",
      "iteration number: 182\t training loss: 0.1027\tvalidation loss: 0.1667\t validation accuracy: 0.9400\n",
      "iteration number: 183\t training loss: 0.1058\tvalidation loss: 0.1664\t validation accuracy: 0.9378\n",
      "iteration number: 184\t training loss: 0.1135\tvalidation loss: 0.1696\t validation accuracy: 0.9311\n",
      "iteration number: 185\t training loss: 0.1152\tvalidation loss: 0.1686\t validation accuracy: 0.9333\n",
      "iteration number: 186\t training loss: 0.1151\tvalidation loss: 0.1723\t validation accuracy: 0.9289\n",
      "iteration number: 187\t training loss: 0.1065\tvalidation loss: 0.1735\t validation accuracy: 0.9356\n",
      "iteration number: 188\t training loss: 0.1025\tvalidation loss: 0.1798\t validation accuracy: 0.9378\n",
      "iteration number: 189\t training loss: 0.1067\tvalidation loss: 0.1965\t validation accuracy: 0.9356\n",
      "iteration number: 190\t training loss: 0.1141\tvalidation loss: 0.2103\t validation accuracy: 0.9311\n",
      "iteration number: 191\t training loss: 0.1202\tvalidation loss: 0.2174\t validation accuracy: 0.9200\n",
      "iteration number: 192\t training loss: 0.1155\tvalidation loss: 0.2112\t validation accuracy: 0.9244\n",
      "iteration number: 193\t training loss: 0.0998\tvalidation loss: 0.1921\t validation accuracy: 0.9289\n",
      "iteration number: 194\t training loss: 0.0899\tvalidation loss: 0.1789\t validation accuracy: 0.9356\n",
      "iteration number: 195\t training loss: 0.0890\tvalidation loss: 0.1764\t validation accuracy: 0.9489\n",
      "iteration number: 196\t training loss: 0.0892\tvalidation loss: 0.1765\t validation accuracy: 0.9444\n",
      "iteration number: 197\t training loss: 0.0929\tvalidation loss: 0.1831\t validation accuracy: 0.9400\n",
      "iteration number: 198\t training loss: 0.0997\tvalidation loss: 0.1915\t validation accuracy: 0.9333\n",
      "iteration number: 199\t training loss: 0.1005\tvalidation loss: 0.1903\t validation accuracy: 0.9356\n",
      "iteration number: 200\t training loss: 0.0998\tvalidation loss: 0.1901\t validation accuracy: 0.9400\n",
      "iteration number: 201\t training loss: 0.1002\tvalidation loss: 0.1905\t validation accuracy: 0.9378\n",
      "iteration number: 202\t training loss: 0.1019\tvalidation loss: 0.1916\t validation accuracy: 0.9311\n",
      "iteration number: 203\t training loss: 0.0970\tvalidation loss: 0.1794\t validation accuracy: 0.9356\n",
      "iteration number: 204\t training loss: 0.0919\tvalidation loss: 0.1674\t validation accuracy: 0.9333\n",
      "iteration number: 205\t training loss: 0.0940\tvalidation loss: 0.1671\t validation accuracy: 0.9222\n",
      "iteration number: 206\t training loss: 0.1051\tvalidation loss: 0.1794\t validation accuracy: 0.9267\n",
      "iteration number: 207\t training loss: 0.1116\tvalidation loss: 0.1871\t validation accuracy: 0.9267\n",
      "iteration number: 208\t training loss: 0.1049\tvalidation loss: 0.1819\t validation accuracy: 0.9378\n",
      "iteration number: 209\t training loss: 0.0909\tvalidation loss: 0.1703\t validation accuracy: 0.9356\n",
      "iteration number: 210\t training loss: 0.0787\tvalidation loss: 0.1604\t validation accuracy: 0.9489\n",
      "iteration number: 211\t training loss: 0.0753\tvalidation loss: 0.1608\t validation accuracy: 0.9467\n",
      "iteration number: 212\t training loss: 0.0817\tvalidation loss: 0.1730\t validation accuracy: 0.9489\n",
      "iteration number: 213\t training loss: 0.0927\tvalidation loss: 0.1912\t validation accuracy: 0.9444\n",
      "iteration number: 214\t training loss: 0.0998\tvalidation loss: 0.1978\t validation accuracy: 0.9422\n",
      "iteration number: 215\t training loss: 0.1010\tvalidation loss: 0.1981\t validation accuracy: 0.9422\n",
      "iteration number: 216\t training loss: 0.0941\tvalidation loss: 0.1845\t validation accuracy: 0.9422\n",
      "iteration number: 217\t training loss: 0.0868\tvalidation loss: 0.1732\t validation accuracy: 0.9378\n",
      "iteration number: 218\t training loss: 0.0817\tvalidation loss: 0.1669\t validation accuracy: 0.9378\n",
      "iteration number: 219\t training loss: 0.0780\tvalidation loss: 0.1630\t validation accuracy: 0.9467\n",
      "iteration number: 220\t training loss: 0.0799\tvalidation loss: 0.1631\t validation accuracy: 0.9333\n",
      "iteration number: 221\t training loss: 0.0914\tvalidation loss: 0.1749\t validation accuracy: 0.9267\n",
      "iteration number: 222\t training loss: 0.1083\tvalidation loss: 0.1932\t validation accuracy: 0.9289\n",
      "iteration number: 223\t training loss: 0.1200\tvalidation loss: 0.2043\t validation accuracy: 0.9289\n",
      "iteration number: 224\t training loss: 0.1057\tvalidation loss: 0.1870\t validation accuracy: 0.9267\n",
      "iteration number: 225\t training loss: 0.0910\tvalidation loss: 0.1721\t validation accuracy: 0.9356\n",
      "iteration number: 226\t training loss: 0.0793\tvalidation loss: 0.1586\t validation accuracy: 0.9400\n",
      "iteration number: 227\t training loss: 0.0770\tvalidation loss: 0.1579\t validation accuracy: 0.9422\n",
      "iteration number: 228\t training loss: 0.0800\tvalidation loss: 0.1649\t validation accuracy: 0.9467\n",
      "iteration number: 229\t training loss: 0.0863\tvalidation loss: 0.1727\t validation accuracy: 0.9467\n",
      "iteration number: 230\t training loss: 0.0901\tvalidation loss: 0.1774\t validation accuracy: 0.9444\n",
      "iteration number: 231\t training loss: 0.0922\tvalidation loss: 0.1802\t validation accuracy: 0.9422\n",
      "iteration number: 232\t training loss: 0.0900\tvalidation loss: 0.1772\t validation accuracy: 0.9400\n",
      "iteration number: 233\t training loss: 0.0865\tvalidation loss: 0.1739\t validation accuracy: 0.9444\n",
      "iteration number: 234\t training loss: 0.0751\tvalidation loss: 0.1575\t validation accuracy: 0.9400\n",
      "iteration number: 235\t training loss: 0.0709\tvalidation loss: 0.1476\t validation accuracy: 0.9489\n",
      "iteration number: 236\t training loss: 0.0727\tvalidation loss: 0.1487\t validation accuracy: 0.9422\n",
      "iteration number: 237\t training loss: 0.0788\tvalidation loss: 0.1583\t validation accuracy: 0.9400\n",
      "iteration number: 238\t training loss: 0.0834\tvalidation loss: 0.1662\t validation accuracy: 0.9378\n",
      "iteration number: 239\t training loss: 0.0839\tvalidation loss: 0.1680\t validation accuracy: 0.9356\n",
      "iteration number: 240\t training loss: 0.0827\tvalidation loss: 0.1685\t validation accuracy: 0.9378\n",
      "iteration number: 241\t training loss: 0.0784\tvalidation loss: 0.1636\t validation accuracy: 0.9400\n",
      "iteration number: 242\t training loss: 0.0719\tvalidation loss: 0.1562\t validation accuracy: 0.9444\n",
      "iteration number: 243\t training loss: 0.0654\tvalidation loss: 0.1538\t validation accuracy: 0.9489\n",
      "iteration number: 244\t training loss: 0.0649\tvalidation loss: 0.1605\t validation accuracy: 0.9467\n",
      "iteration number: 245\t training loss: 0.0714\tvalidation loss: 0.1752\t validation accuracy: 0.9467\n",
      "iteration number: 246\t training loss: 0.0798\tvalidation loss: 0.1891\t validation accuracy: 0.9400\n",
      "iteration number: 247\t training loss: 0.0851\tvalidation loss: 0.1962\t validation accuracy: 0.9400\n",
      "iteration number: 248\t training loss: 0.0886\tvalidation loss: 0.1979\t validation accuracy: 0.9422\n",
      "iteration number: 249\t training loss: 0.0927\tvalidation loss: 0.2004\t validation accuracy: 0.9378\n",
      "iteration number: 250\t training loss: 0.0822\tvalidation loss: 0.1827\t validation accuracy: 0.9422\n",
      "iteration number: 251\t training loss: 0.0728\tvalidation loss: 0.1695\t validation accuracy: 0.9422\n",
      "iteration number: 252\t training loss: 0.0657\tvalidation loss: 0.1592\t validation accuracy: 0.9422\n",
      "iteration number: 253\t training loss: 0.0619\tvalidation loss: 0.1539\t validation accuracy: 0.9444\n",
      "iteration number: 254\t training loss: 0.0623\tvalidation loss: 0.1560\t validation accuracy: 0.9400\n",
      "iteration number: 255\t training loss: 0.0640\tvalidation loss: 0.1587\t validation accuracy: 0.9378\n",
      "iteration number: 256\t training loss: 0.0638\tvalidation loss: 0.1572\t validation accuracy: 0.9400\n",
      "iteration number: 257\t training loss: 0.0649\tvalidation loss: 0.1576\t validation accuracy: 0.9422\n",
      "iteration number: 258\t training loss: 0.0647\tvalidation loss: 0.1549\t validation accuracy: 0.9467\n",
      "iteration number: 259\t training loss: 0.0622\tvalidation loss: 0.1526\t validation accuracy: 0.9467\n",
      "iteration number: 260\t training loss: 0.0604\tvalidation loss: 0.1501\t validation accuracy: 0.9489\n",
      "iteration number: 261\t training loss: 0.0605\tvalidation loss: 0.1497\t validation accuracy: 0.9489\n",
      "iteration number: 262\t training loss: 0.0617\tvalidation loss: 0.1566\t validation accuracy: 0.9511\n",
      "iteration number: 263\t training loss: 0.0633\tvalidation loss: 0.1644\t validation accuracy: 0.9467\n",
      "iteration number: 264\t training loss: 0.0628\tvalidation loss: 0.1665\t validation accuracy: 0.9467\n",
      "iteration number: 265\t training loss: 0.0596\tvalidation loss: 0.1593\t validation accuracy: 0.9489\n",
      "iteration number: 266\t training loss: 0.0592\tvalidation loss: 0.1559\t validation accuracy: 0.9511\n",
      "iteration number: 267\t training loss: 0.0620\tvalidation loss: 0.1592\t validation accuracy: 0.9489\n",
      "iteration number: 268\t training loss: 0.0679\tvalidation loss: 0.1673\t validation accuracy: 0.9422\n",
      "iteration number: 269\t training loss: 0.0747\tvalidation loss: 0.1765\t validation accuracy: 0.9422\n",
      "iteration number: 270\t training loss: 0.0802\tvalidation loss: 0.1838\t validation accuracy: 0.9378\n",
      "iteration number: 271\t training loss: 0.0810\tvalidation loss: 0.1852\t validation accuracy: 0.9356\n",
      "iteration number: 272\t training loss: 0.0753\tvalidation loss: 0.1749\t validation accuracy: 0.9356\n",
      "iteration number: 273\t training loss: 0.0672\tvalidation loss: 0.1621\t validation accuracy: 0.9400\n",
      "iteration number: 274\t training loss: 0.0583\tvalidation loss: 0.1481\t validation accuracy: 0.9422\n",
      "iteration number: 275\t training loss: 0.0548\tvalidation loss: 0.1413\t validation accuracy: 0.9511\n",
      "iteration number: 276\t training loss: 0.0552\tvalidation loss: 0.1399\t validation accuracy: 0.9533\n",
      "iteration number: 277\t training loss: 0.0567\tvalidation loss: 0.1454\t validation accuracy: 0.9511\n",
      "iteration number: 278\t training loss: 0.0607\tvalidation loss: 0.1546\t validation accuracy: 0.9444\n",
      "iteration number: 279\t training loss: 0.0641\tvalidation loss: 0.1589\t validation accuracy: 0.9422\n",
      "iteration number: 280\t training loss: 0.0664\tvalidation loss: 0.1613\t validation accuracy: 0.9444\n",
      "iteration number: 281\t training loss: 0.0644\tvalidation loss: 0.1576\t validation accuracy: 0.9422\n",
      "iteration number: 282\t training loss: 0.0596\tvalidation loss: 0.1531\t validation accuracy: 0.9422\n",
      "iteration number: 283\t training loss: 0.0536\tvalidation loss: 0.1451\t validation accuracy: 0.9467\n",
      "iteration number: 284\t training loss: 0.0511\tvalidation loss: 0.1395\t validation accuracy: 0.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 285\t training loss: 0.0523\tvalidation loss: 0.1397\t validation accuracy: 0.9489\n",
      "iteration number: 286\t training loss: 0.0547\tvalidation loss: 0.1446\t validation accuracy: 0.9467\n",
      "iteration number: 287\t training loss: 0.0593\tvalidation loss: 0.1534\t validation accuracy: 0.9467\n",
      "iteration number: 288\t training loss: 0.0658\tvalidation loss: 0.1634\t validation accuracy: 0.9400\n",
      "iteration number: 289\t training loss: 0.0670\tvalidation loss: 0.1654\t validation accuracy: 0.9422\n",
      "iteration number: 290\t training loss: 0.0648\tvalidation loss: 0.1629\t validation accuracy: 0.9489\n",
      "iteration number: 291\t training loss: 0.0591\tvalidation loss: 0.1547\t validation accuracy: 0.9467\n",
      "iteration number: 292\t training loss: 0.0558\tvalidation loss: 0.1494\t validation accuracy: 0.9533\n",
      "iteration number: 293\t training loss: 0.0563\tvalidation loss: 0.1500\t validation accuracy: 0.9533\n",
      "iteration number: 294\t training loss: 0.0596\tvalidation loss: 0.1575\t validation accuracy: 0.9511\n",
      "iteration number: 295\t training loss: 0.0593\tvalidation loss: 0.1563\t validation accuracy: 0.9511\n",
      "iteration number: 296\t training loss: 0.0587\tvalidation loss: 0.1548\t validation accuracy: 0.9489\n",
      "iteration number: 297\t training loss: 0.0589\tvalidation loss: 0.1560\t validation accuracy: 0.9489\n",
      "iteration number: 298\t training loss: 0.0604\tvalidation loss: 0.1607\t validation accuracy: 0.9400\n",
      "iteration number: 299\t training loss: 0.0612\tvalidation loss: 0.1627\t validation accuracy: 0.9400\n",
      "iteration number: 300\t training loss: 0.0601\tvalidation loss: 0.1648\t validation accuracy: 0.9378\n",
      "iteration number: 301\t training loss: 0.0583\tvalidation loss: 0.1641\t validation accuracy: 0.9378\n",
      "iteration number: 302\t training loss: 0.0551\tvalidation loss: 0.1582\t validation accuracy: 0.9356\n",
      "iteration number: 303\t training loss: 0.0531\tvalidation loss: 0.1482\t validation accuracy: 0.9444\n",
      "iteration number: 304\t training loss: 0.0557\tvalidation loss: 0.1448\t validation accuracy: 0.9467\n",
      "iteration number: 305\t training loss: 0.0579\tvalidation loss: 0.1432\t validation accuracy: 0.9467\n",
      "iteration number: 306\t training loss: 0.0631\tvalidation loss: 0.1446\t validation accuracy: 0.9467\n",
      "iteration number: 307\t training loss: 0.0678\tvalidation loss: 0.1450\t validation accuracy: 0.9489\n",
      "iteration number: 308\t training loss: 0.0666\tvalidation loss: 0.1395\t validation accuracy: 0.9489\n",
      "iteration number: 309\t training loss: 0.0583\tvalidation loss: 0.1330\t validation accuracy: 0.9600\n",
      "iteration number: 310\t training loss: 0.0546\tvalidation loss: 0.1346\t validation accuracy: 0.9578\n",
      "iteration number: 311\t training loss: 0.0548\tvalidation loss: 0.1407\t validation accuracy: 0.9600\n",
      "iteration number: 312\t training loss: 0.0542\tvalidation loss: 0.1453\t validation accuracy: 0.9533\n",
      "iteration number: 313\t training loss: 0.0562\tvalidation loss: 0.1557\t validation accuracy: 0.9556\n",
      "iteration number: 314\t training loss: 0.0596\tvalidation loss: 0.1672\t validation accuracy: 0.9533\n",
      "iteration number: 315\t training loss: 0.0625\tvalidation loss: 0.1761\t validation accuracy: 0.9489\n",
      "iteration number: 316\t training loss: 0.0621\tvalidation loss: 0.1781\t validation accuracy: 0.9444\n",
      "iteration number: 317\t training loss: 0.0577\tvalidation loss: 0.1722\t validation accuracy: 0.9489\n",
      "iteration number: 318\t training loss: 0.0516\tvalidation loss: 0.1605\t validation accuracy: 0.9489\n",
      "iteration number: 319\t training loss: 0.0497\tvalidation loss: 0.1530\t validation accuracy: 0.9467\n",
      "iteration number: 320\t training loss: 0.0489\tvalidation loss: 0.1493\t validation accuracy: 0.9533\n",
      "iteration number: 321\t training loss: 0.0509\tvalidation loss: 0.1488\t validation accuracy: 0.9511\n",
      "iteration number: 322\t training loss: 0.0545\tvalidation loss: 0.1508\t validation accuracy: 0.9489\n",
      "iteration number: 323\t training loss: 0.0593\tvalidation loss: 0.1544\t validation accuracy: 0.9400\n",
      "iteration number: 324\t training loss: 0.0594\tvalidation loss: 0.1523\t validation accuracy: 0.9422\n",
      "iteration number: 325\t training loss: 0.0557\tvalidation loss: 0.1419\t validation accuracy: 0.9489\n",
      "iteration number: 326\t training loss: 0.0514\tvalidation loss: 0.1337\t validation accuracy: 0.9489\n",
      "iteration number: 327\t training loss: 0.0467\tvalidation loss: 0.1287\t validation accuracy: 0.9578\n",
      "iteration number: 328\t training loss: 0.0451\tvalidation loss: 0.1301\t validation accuracy: 0.9578\n",
      "iteration number: 329\t training loss: 0.0467\tvalidation loss: 0.1369\t validation accuracy: 0.9600\n",
      "iteration number: 330\t training loss: 0.0453\tvalidation loss: 0.1360\t validation accuracy: 0.9556\n",
      "iteration number: 331\t training loss: 0.0442\tvalidation loss: 0.1344\t validation accuracy: 0.9533\n",
      "iteration number: 332\t training loss: 0.0441\tvalidation loss: 0.1331\t validation accuracy: 0.9533\n",
      "iteration number: 333\t training loss: 0.0453\tvalidation loss: 0.1330\t validation accuracy: 0.9511\n",
      "iteration number: 334\t training loss: 0.0457\tvalidation loss: 0.1313\t validation accuracy: 0.9556\n",
      "iteration number: 335\t training loss: 0.0459\tvalidation loss: 0.1276\t validation accuracy: 0.9533\n",
      "iteration number: 336\t training loss: 0.0472\tvalidation loss: 0.1261\t validation accuracy: 0.9556\n",
      "iteration number: 337\t training loss: 0.0480\tvalidation loss: 0.1270\t validation accuracy: 0.9533\n",
      "iteration number: 338\t training loss: 0.0453\tvalidation loss: 0.1270\t validation accuracy: 0.9556\n",
      "iteration number: 339\t training loss: 0.0411\tvalidation loss: 0.1264\t validation accuracy: 0.9533\n",
      "iteration number: 340\t training loss: 0.0394\tvalidation loss: 0.1292\t validation accuracy: 0.9511\n",
      "iteration number: 341\t training loss: 0.0397\tvalidation loss: 0.1344\t validation accuracy: 0.9489\n",
      "iteration number: 342\t training loss: 0.0407\tvalidation loss: 0.1388\t validation accuracy: 0.9400\n",
      "iteration number: 343\t training loss: 0.0420\tvalidation loss: 0.1441\t validation accuracy: 0.9422\n",
      "iteration number: 344\t training loss: 0.0431\tvalidation loss: 0.1504\t validation accuracy: 0.9422\n",
      "iteration number: 345\t training loss: 0.0446\tvalidation loss: 0.1531\t validation accuracy: 0.9444\n",
      "iteration number: 346\t training loss: 0.0442\tvalidation loss: 0.1530\t validation accuracy: 0.9444\n",
      "iteration number: 347\t training loss: 0.0425\tvalidation loss: 0.1481\t validation accuracy: 0.9467\n",
      "iteration number: 348\t training loss: 0.0403\tvalidation loss: 0.1411\t validation accuracy: 0.9489\n",
      "iteration number: 349\t training loss: 0.0390\tvalidation loss: 0.1351\t validation accuracy: 0.9511\n",
      "iteration number: 350\t training loss: 0.0392\tvalidation loss: 0.1310\t validation accuracy: 0.9533\n",
      "iteration number: 351\t training loss: 0.0403\tvalidation loss: 0.1272\t validation accuracy: 0.9533\n",
      "iteration number: 352\t training loss: 0.0437\tvalidation loss: 0.1238\t validation accuracy: 0.9533\n",
      "iteration number: 353\t training loss: 0.0477\tvalidation loss: 0.1241\t validation accuracy: 0.9556\n",
      "iteration number: 354\t training loss: 0.0466\tvalidation loss: 0.1200\t validation accuracy: 0.9556\n",
      "iteration number: 355\t training loss: 0.0470\tvalidation loss: 0.1196\t validation accuracy: 0.9556\n",
      "iteration number: 356\t training loss: 0.0444\tvalidation loss: 0.1210\t validation accuracy: 0.9578\n",
      "iteration number: 357\t training loss: 0.0427\tvalidation loss: 0.1247\t validation accuracy: 0.9533\n",
      "iteration number: 358\t training loss: 0.0419\tvalidation loss: 0.1305\t validation accuracy: 0.9511\n",
      "iteration number: 359\t training loss: 0.0419\tvalidation loss: 0.1380\t validation accuracy: 0.9511\n",
      "iteration number: 360\t training loss: 0.0416\tvalidation loss: 0.1436\t validation accuracy: 0.9533\n",
      "iteration number: 361\t training loss: 0.0422\tvalidation loss: 0.1496\t validation accuracy: 0.9511\n",
      "iteration number: 362\t training loss: 0.0446\tvalidation loss: 0.1586\t validation accuracy: 0.9489\n",
      "iteration number: 363\t training loss: 0.0469\tvalidation loss: 0.1652\t validation accuracy: 0.9489\n",
      "iteration number: 364\t training loss: 0.0439\tvalidation loss: 0.1570\t validation accuracy: 0.9489\n",
      "iteration number: 365\t training loss: 0.0395\tvalidation loss: 0.1434\t validation accuracy: 0.9533\n",
      "iteration number: 366\t training loss: 0.0373\tvalidation loss: 0.1322\t validation accuracy: 0.9578\n",
      "iteration number: 367\t training loss: 0.0389\tvalidation loss: 0.1277\t validation accuracy: 0.9556\n",
      "iteration number: 368\t training loss: 0.0435\tvalidation loss: 0.1272\t validation accuracy: 0.9556\n",
      "iteration number: 369\t training loss: 0.0443\tvalidation loss: 0.1241\t validation accuracy: 0.9533\n",
      "iteration number: 370\t training loss: 0.0442\tvalidation loss: 0.1218\t validation accuracy: 0.9533\n",
      "iteration number: 371\t training loss: 0.0445\tvalidation loss: 0.1204\t validation accuracy: 0.9533\n",
      "iteration number: 372\t training loss: 0.0436\tvalidation loss: 0.1211\t validation accuracy: 0.9511\n",
      "iteration number: 373\t training loss: 0.0427\tvalidation loss: 0.1230\t validation accuracy: 0.9511\n",
      "iteration number: 374\t training loss: 0.0428\tvalidation loss: 0.1259\t validation accuracy: 0.9489\n",
      "iteration number: 375\t training loss: 0.0420\tvalidation loss: 0.1276\t validation accuracy: 0.9556\n",
      "iteration number: 376\t training loss: 0.0408\tvalidation loss: 0.1267\t validation accuracy: 0.9600\n",
      "iteration number: 377\t training loss: 0.0384\tvalidation loss: 0.1246\t validation accuracy: 0.9600\n",
      "iteration number: 378\t training loss: 0.0370\tvalidation loss: 0.1243\t validation accuracy: 0.9511\n",
      "iteration number: 379\t training loss: 0.0365\tvalidation loss: 0.1253\t validation accuracy: 0.9556\n",
      "iteration number: 380\t training loss: 0.0389\tvalidation loss: 0.1277\t validation accuracy: 0.9578\n",
      "iteration number: 381\t training loss: 0.0433\tvalidation loss: 0.1346\t validation accuracy: 0.9600\n",
      "iteration number: 382\t training loss: 0.0482\tvalidation loss: 0.1443\t validation accuracy: 0.9489\n",
      "iteration number: 383\t training loss: 0.0527\tvalidation loss: 0.1532\t validation accuracy: 0.9444\n",
      "iteration number: 384\t training loss: 0.0553\tvalidation loss: 0.1601\t validation accuracy: 0.9378\n",
      "iteration number: 385\t training loss: 0.0477\tvalidation loss: 0.1541\t validation accuracy: 0.9444\n",
      "iteration number: 386\t training loss: 0.0404\tvalidation loss: 0.1504\t validation accuracy: 0.9444\n",
      "iteration number: 387\t training loss: 0.0364\tvalidation loss: 0.1514\t validation accuracy: 0.9489\n",
      "iteration number: 388\t training loss: 0.0360\tvalidation loss: 0.1567\t validation accuracy: 0.9511\n",
      "iteration number: 389\t training loss: 0.0381\tvalidation loss: 0.1660\t validation accuracy: 0.9489\n",
      "iteration number: 390\t training loss: 0.0408\tvalidation loss: 0.1731\t validation accuracy: 0.9511\n",
      "iteration number: 391\t training loss: 0.0424\tvalidation loss: 0.1758\t validation accuracy: 0.9511\n",
      "iteration number: 392\t training loss: 0.0446\tvalidation loss: 0.1793\t validation accuracy: 0.9489\n",
      "iteration number: 393\t training loss: 0.0455\tvalidation loss: 0.1787\t validation accuracy: 0.9489\n",
      "iteration number: 394\t training loss: 0.0399\tvalidation loss: 0.1628\t validation accuracy: 0.9467\n",
      "iteration number: 395\t training loss: 0.0360\tvalidation loss: 0.1508\t validation accuracy: 0.9511\n",
      "iteration number: 396\t training loss: 0.0325\tvalidation loss: 0.1384\t validation accuracy: 0.9533\n",
      "iteration number: 397\t training loss: 0.0315\tvalidation loss: 0.1292\t validation accuracy: 0.9556\n",
      "iteration number: 398\t training loss: 0.0330\tvalidation loss: 0.1257\t validation accuracy: 0.9578\n",
      "iteration number: 399\t training loss: 0.0356\tvalidation loss: 0.1261\t validation accuracy: 0.9533\n",
      "iteration number: 400\t training loss: 0.0356\tvalidation loss: 0.1282\t validation accuracy: 0.9578\n",
      "iteration number: 401\t training loss: 0.0338\tvalidation loss: 0.1307\t validation accuracy: 0.9556\n",
      "iteration number: 402\t training loss: 0.0327\tvalidation loss: 0.1335\t validation accuracy: 0.9533\n",
      "iteration number: 403\t training loss: 0.0322\tvalidation loss: 0.1367\t validation accuracy: 0.9489\n",
      "iteration number: 404\t training loss: 0.0314\tvalidation loss: 0.1401\t validation accuracy: 0.9533\n",
      "iteration number: 405\t training loss: 0.0312\tvalidation loss: 0.1445\t validation accuracy: 0.9533\n",
      "iteration number: 406\t training loss: 0.0308\tvalidation loss: 0.1456\t validation accuracy: 0.9489\n",
      "iteration number: 407\t training loss: 0.0309\tvalidation loss: 0.1463\t validation accuracy: 0.9467\n",
      "iteration number: 408\t training loss: 0.0325\tvalidation loss: 0.1460\t validation accuracy: 0.9511\n",
      "iteration number: 409\t training loss: 0.0351\tvalidation loss: 0.1456\t validation accuracy: 0.9533\n",
      "iteration number: 410\t training loss: 0.0384\tvalidation loss: 0.1468\t validation accuracy: 0.9533\n",
      "iteration number: 411\t training loss: 0.0394\tvalidation loss: 0.1447\t validation accuracy: 0.9556\n",
      "iteration number: 412\t training loss: 0.0352\tvalidation loss: 0.1412\t validation accuracy: 0.9556\n",
      "iteration number: 413\t training loss: 0.0316\tvalidation loss: 0.1413\t validation accuracy: 0.9533\n",
      "iteration number: 414\t training loss: 0.0320\tvalidation loss: 0.1503\t validation accuracy: 0.9489\n",
      "iteration number: 415\t training loss: 0.0369\tvalidation loss: 0.1659\t validation accuracy: 0.9489\n",
      "iteration number: 416\t training loss: 0.0409\tvalidation loss: 0.1759\t validation accuracy: 0.9489\n",
      "iteration number: 417\t training loss: 0.0388\tvalidation loss: 0.1710\t validation accuracy: 0.9489\n",
      "iteration number: 418\t training loss: 0.0358\tvalidation loss: 0.1635\t validation accuracy: 0.9489\n",
      "iteration number: 419\t training loss: 0.0320\tvalidation loss: 0.1531\t validation accuracy: 0.9489\n",
      "iteration number: 420\t training loss: 0.0302\tvalidation loss: 0.1468\t validation accuracy: 0.9489\n",
      "iteration number: 421\t training loss: 0.0304\tvalidation loss: 0.1440\t validation accuracy: 0.9489\n",
      "iteration number: 422\t training loss: 0.0313\tvalidation loss: 0.1436\t validation accuracy: 0.9489\n",
      "iteration number: 423\t training loss: 0.0313\tvalidation loss: 0.1410\t validation accuracy: 0.9489\n",
      "iteration number: 424\t training loss: 0.0309\tvalidation loss: 0.1373\t validation accuracy: 0.9489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 425\t training loss: 0.0301\tvalidation loss: 0.1329\t validation accuracy: 0.9489\n",
      "iteration number: 426\t training loss: 0.0296\tvalidation loss: 0.1294\t validation accuracy: 0.9533\n",
      "iteration number: 427\t training loss: 0.0295\tvalidation loss: 0.1267\t validation accuracy: 0.9511\n",
      "iteration number: 428\t training loss: 0.0301\tvalidation loss: 0.1249\t validation accuracy: 0.9556\n",
      "iteration number: 429\t training loss: 0.0310\tvalidation loss: 0.1248\t validation accuracy: 0.9556\n",
      "iteration number: 430\t training loss: 0.0319\tvalidation loss: 0.1261\t validation accuracy: 0.9533\n",
      "iteration number: 431\t training loss: 0.0325\tvalidation loss: 0.1303\t validation accuracy: 0.9533\n",
      "iteration number: 432\t training loss: 0.0339\tvalidation loss: 0.1360\t validation accuracy: 0.9533\n",
      "iteration number: 433\t training loss: 0.0324\tvalidation loss: 0.1382\t validation accuracy: 0.9533\n",
      "iteration number: 434\t training loss: 0.0298\tvalidation loss: 0.1409\t validation accuracy: 0.9556\n",
      "iteration number: 435\t training loss: 0.0288\tvalidation loss: 0.1431\t validation accuracy: 0.9511\n",
      "iteration number: 436\t training loss: 0.0294\tvalidation loss: 0.1483\t validation accuracy: 0.9511\n",
      "iteration number: 437\t training loss: 0.0314\tvalidation loss: 0.1541\t validation accuracy: 0.9422\n",
      "iteration number: 438\t training loss: 0.0343\tvalidation loss: 0.1617\t validation accuracy: 0.9378\n",
      "iteration number: 439\t training loss: 0.0367\tvalidation loss: 0.1707\t validation accuracy: 0.9356\n",
      "iteration number: 440\t training loss: 0.0344\tvalidation loss: 0.1674\t validation accuracy: 0.9356\n",
      "iteration number: 441\t training loss: 0.0296\tvalidation loss: 0.1522\t validation accuracy: 0.9444\n",
      "iteration number: 442\t training loss: 0.0292\tvalidation loss: 0.1437\t validation accuracy: 0.9489\n",
      "iteration number: 443\t training loss: 0.0316\tvalidation loss: 0.1413\t validation accuracy: 0.9533\n",
      "iteration number: 444\t training loss: 0.0333\tvalidation loss: 0.1381\t validation accuracy: 0.9533\n",
      "iteration number: 445\t training loss: 0.0341\tvalidation loss: 0.1361\t validation accuracy: 0.9533\n",
      "iteration number: 446\t training loss: 0.0337\tvalidation loss: 0.1328\t validation accuracy: 0.9578\n",
      "iteration number: 447\t training loss: 0.0334\tvalidation loss: 0.1307\t validation accuracy: 0.9600\n",
      "iteration number: 448\t training loss: 0.0299\tvalidation loss: 0.1276\t validation accuracy: 0.9622\n",
      "iteration number: 449\t training loss: 0.0276\tvalidation loss: 0.1259\t validation accuracy: 0.9644\n",
      "iteration number: 450\t training loss: 0.0264\tvalidation loss: 0.1269\t validation accuracy: 0.9556\n",
      "iteration number: 451\t training loss: 0.0263\tvalidation loss: 0.1291\t validation accuracy: 0.9467\n",
      "iteration number: 452\t training loss: 0.0280\tvalidation loss: 0.1346\t validation accuracy: 0.9467\n",
      "iteration number: 453\t training loss: 0.0305\tvalidation loss: 0.1421\t validation accuracy: 0.9467\n",
      "iteration number: 454\t training loss: 0.0323\tvalidation loss: 0.1465\t validation accuracy: 0.9444\n",
      "iteration number: 455\t training loss: 0.0338\tvalidation loss: 0.1506\t validation accuracy: 0.9400\n",
      "iteration number: 456\t training loss: 0.0342\tvalidation loss: 0.1507\t validation accuracy: 0.9356\n",
      "iteration number: 457\t training loss: 0.0336\tvalidation loss: 0.1485\t validation accuracy: 0.9400\n",
      "iteration number: 458\t training loss: 0.0307\tvalidation loss: 0.1419\t validation accuracy: 0.9444\n",
      "iteration number: 459\t training loss: 0.0293\tvalidation loss: 0.1380\t validation accuracy: 0.9533\n",
      "iteration number: 460\t training loss: 0.0279\tvalidation loss: 0.1343\t validation accuracy: 0.9578\n",
      "iteration number: 461\t training loss: 0.0272\tvalidation loss: 0.1336\t validation accuracy: 0.9556\n",
      "iteration number: 462\t training loss: 0.0278\tvalidation loss: 0.1353\t validation accuracy: 0.9556\n",
      "iteration number: 463\t training loss: 0.0293\tvalidation loss: 0.1396\t validation accuracy: 0.9533\n",
      "iteration number: 464\t training loss: 0.0310\tvalidation loss: 0.1436\t validation accuracy: 0.9533\n",
      "iteration number: 465\t training loss: 0.0316\tvalidation loss: 0.1425\t validation accuracy: 0.9533\n",
      "iteration number: 466\t training loss: 0.0306\tvalidation loss: 0.1372\t validation accuracy: 0.9533\n",
      "iteration number: 467\t training loss: 0.0294\tvalidation loss: 0.1311\t validation accuracy: 0.9556\n",
      "iteration number: 468\t training loss: 0.0269\tvalidation loss: 0.1244\t validation accuracy: 0.9556\n",
      "iteration number: 469\t training loss: 0.0251\tvalidation loss: 0.1198\t validation accuracy: 0.9622\n",
      "iteration number: 470\t training loss: 0.0242\tvalidation loss: 0.1186\t validation accuracy: 0.9578\n",
      "iteration number: 471\t training loss: 0.0242\tvalidation loss: 0.1178\t validation accuracy: 0.9622\n",
      "iteration number: 472\t training loss: 0.0260\tvalidation loss: 0.1208\t validation accuracy: 0.9578\n",
      "iteration number: 473\t training loss: 0.0298\tvalidation loss: 0.1274\t validation accuracy: 0.9511\n",
      "iteration number: 474\t training loss: 0.0354\tvalidation loss: 0.1372\t validation accuracy: 0.9533\n",
      "iteration number: 475\t training loss: 0.0405\tvalidation loss: 0.1464\t validation accuracy: 0.9489\n",
      "iteration number: 476\t training loss: 0.0370\tvalidation loss: 0.1474\t validation accuracy: 0.9511\n",
      "iteration number: 477\t training loss: 0.0327\tvalidation loss: 0.1483\t validation accuracy: 0.9489\n",
      "iteration number: 478\t training loss: 0.0288\tvalidation loss: 0.1493\t validation accuracy: 0.9533\n",
      "iteration number: 479\t training loss: 0.0268\tvalidation loss: 0.1528\t validation accuracy: 0.9511\n",
      "iteration number: 480\t training loss: 0.0276\tvalidation loss: 0.1594\t validation accuracy: 0.9467\n",
      "iteration number: 481\t training loss: 0.0307\tvalidation loss: 0.1667\t validation accuracy: 0.9511\n",
      "iteration number: 482\t training loss: 0.0361\tvalidation loss: 0.1750\t validation accuracy: 0.9467\n",
      "iteration number: 483\t training loss: 0.0428\tvalidation loss: 0.1832\t validation accuracy: 0.9467\n",
      "iteration number: 484\t training loss: 0.0493\tvalidation loss: 0.1889\t validation accuracy: 0.9444\n",
      "iteration number: 485\t training loss: 0.0537\tvalidation loss: 0.1927\t validation accuracy: 0.9422\n",
      "iteration number: 486\t training loss: 0.0491\tvalidation loss: 0.1865\t validation accuracy: 0.9444\n",
      "iteration number: 487\t training loss: 0.0421\tvalidation loss: 0.1776\t validation accuracy: 0.9444\n",
      "iteration number: 488\t training loss: 0.0363\tvalidation loss: 0.1690\t validation accuracy: 0.9489\n",
      "iteration number: 489\t training loss: 0.0315\tvalidation loss: 0.1599\t validation accuracy: 0.9467\n",
      "iteration number: 490\t training loss: 0.0279\tvalidation loss: 0.1506\t validation accuracy: 0.9489\n",
      "iteration number: 491\t training loss: 0.0267\tvalidation loss: 0.1406\t validation accuracy: 0.9467\n",
      "iteration number: 492\t training loss: 0.0298\tvalidation loss: 0.1372\t validation accuracy: 0.9467\n",
      "iteration number: 493\t training loss: 0.0344\tvalidation loss: 0.1369\t validation accuracy: 0.9489\n",
      "iteration number: 494\t training loss: 0.0391\tvalidation loss: 0.1390\t validation accuracy: 0.9467\n",
      "iteration number: 495\t training loss: 0.0383\tvalidation loss: 0.1361\t validation accuracy: 0.9444\n",
      "iteration number: 496\t training loss: 0.0330\tvalidation loss: 0.1312\t validation accuracy: 0.9556\n",
      "iteration number: 497\t training loss: 0.0308\tvalidation loss: 0.1322\t validation accuracy: 0.9533\n",
      "iteration number: 498\t training loss: 0.0307\tvalidation loss: 0.1377\t validation accuracy: 0.9533\n",
      "iteration number: 499\t training loss: 0.0293\tvalidation loss: 0.1415\t validation accuracy: 0.9511\n",
      "iteration number: 500\t training loss: 0.0300\tvalidation loss: 0.1470\t validation accuracy: 0.9556\n",
      "iteration number: 501\t training loss: 0.0321\tvalidation loss: 0.1532\t validation accuracy: 0.9578\n",
      "iteration number: 502\t training loss: 0.0352\tvalidation loss: 0.1610\t validation accuracy: 0.9578\n",
      "iteration number: 503\t training loss: 0.0388\tvalidation loss: 0.1706\t validation accuracy: 0.9600\n",
      "iteration number: 504\t training loss: 0.0403\tvalidation loss: 0.1783\t validation accuracy: 0.9578\n",
      "iteration number: 505\t training loss: 0.0409\tvalidation loss: 0.1822\t validation accuracy: 0.9533\n",
      "iteration number: 506\t training loss: 0.0415\tvalidation loss: 0.1852\t validation accuracy: 0.9533\n",
      "iteration number: 507\t training loss: 0.0374\tvalidation loss: 0.1765\t validation accuracy: 0.9556\n",
      "iteration number: 508\t training loss: 0.0337\tvalidation loss: 0.1673\t validation accuracy: 0.9511\n",
      "iteration number: 509\t training loss: 0.0313\tvalidation loss: 0.1599\t validation accuracy: 0.9511\n",
      "iteration number: 510\t training loss: 0.0287\tvalidation loss: 0.1509\t validation accuracy: 0.9511\n",
      "iteration number: 511\t training loss: 0.0271\tvalidation loss: 0.1412\t validation accuracy: 0.9556\n",
      "iteration number: 512\t training loss: 0.0272\tvalidation loss: 0.1308\t validation accuracy: 0.9511\n",
      "iteration number: 513\t training loss: 0.0286\tvalidation loss: 0.1248\t validation accuracy: 0.9467\n",
      "iteration number: 514\t training loss: 0.0277\tvalidation loss: 0.1206\t validation accuracy: 0.9489\n",
      "iteration number: 515\t training loss: 0.0272\tvalidation loss: 0.1177\t validation accuracy: 0.9511\n",
      "iteration number: 516\t training loss: 0.0266\tvalidation loss: 0.1166\t validation accuracy: 0.9533\n",
      "iteration number: 517\t training loss: 0.0262\tvalidation loss: 0.1173\t validation accuracy: 0.9533\n",
      "iteration number: 518\t training loss: 0.0264\tvalidation loss: 0.1192\t validation accuracy: 0.9556\n",
      "iteration number: 519\t training loss: 0.0269\tvalidation loss: 0.1241\t validation accuracy: 0.9533\n",
      "iteration number: 520\t training loss: 0.0296\tvalidation loss: 0.1332\t validation accuracy: 0.9556\n",
      "iteration number: 521\t training loss: 0.0304\tvalidation loss: 0.1393\t validation accuracy: 0.9600\n",
      "iteration number: 522\t training loss: 0.0301\tvalidation loss: 0.1388\t validation accuracy: 0.9600\n",
      "iteration number: 523\t training loss: 0.0288\tvalidation loss: 0.1338\t validation accuracy: 0.9578\n",
      "iteration number: 524\t training loss: 0.0282\tvalidation loss: 0.1287\t validation accuracy: 0.9556\n",
      "iteration number: 525\t training loss: 0.0287\tvalidation loss: 0.1246\t validation accuracy: 0.9578\n",
      "iteration number: 526\t training loss: 0.0300\tvalidation loss: 0.1230\t validation accuracy: 0.9600\n",
      "iteration number: 527\t training loss: 0.0317\tvalidation loss: 0.1236\t validation accuracy: 0.9644\n",
      "iteration number: 528\t training loss: 0.0338\tvalidation loss: 0.1266\t validation accuracy: 0.9600\n",
      "iteration number: 529\t training loss: 0.0349\tvalidation loss: 0.1307\t validation accuracy: 0.9600\n",
      "iteration number: 530\t training loss: 0.0356\tvalidation loss: 0.1338\t validation accuracy: 0.9578\n",
      "iteration number: 531\t training loss: 0.0323\tvalidation loss: 0.1343\t validation accuracy: 0.9556\n",
      "iteration number: 532\t training loss: 0.0318\tvalidation loss: 0.1393\t validation accuracy: 0.9489\n",
      "iteration number: 533\t training loss: 0.0314\tvalidation loss: 0.1473\t validation accuracy: 0.9533\n",
      "iteration number: 534\t training loss: 0.0332\tvalidation loss: 0.1568\t validation accuracy: 0.9489\n",
      "iteration number: 535\t training loss: 0.0315\tvalidation loss: 0.1581\t validation accuracy: 0.9489\n",
      "iteration number: 536\t training loss: 0.0338\tvalidation loss: 0.1623\t validation accuracy: 0.9511\n",
      "iteration number: 537\t training loss: 0.0321\tvalidation loss: 0.1537\t validation accuracy: 0.9511\n",
      "iteration number: 538\t training loss: 0.0293\tvalidation loss: 0.1394\t validation accuracy: 0.9600\n",
      "iteration number: 539\t training loss: 0.0298\tvalidation loss: 0.1317\t validation accuracy: 0.9578\n",
      "iteration number: 540\t training loss: 0.0317\tvalidation loss: 0.1274\t validation accuracy: 0.9556\n",
      "iteration number: 541\t training loss: 0.0329\tvalidation loss: 0.1234\t validation accuracy: 0.9622\n",
      "iteration number: 542\t training loss: 0.0312\tvalidation loss: 0.1194\t validation accuracy: 0.9622\n",
      "iteration number: 543\t training loss: 0.0306\tvalidation loss: 0.1176\t validation accuracy: 0.9578\n",
      "iteration number: 544\t training loss: 0.0278\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 545\t training loss: 0.0239\tvalidation loss: 0.1135\t validation accuracy: 0.9622\n",
      "iteration number: 546\t training loss: 0.0211\tvalidation loss: 0.1139\t validation accuracy: 0.9644\n",
      "iteration number: 547\t training loss: 0.0202\tvalidation loss: 0.1189\t validation accuracy: 0.9622\n",
      "iteration number: 548\t training loss: 0.0213\tvalidation loss: 0.1260\t validation accuracy: 0.9578\n",
      "iteration number: 549\t training loss: 0.0234\tvalidation loss: 0.1339\t validation accuracy: 0.9556\n",
      "iteration number: 550\t training loss: 0.0253\tvalidation loss: 0.1404\t validation accuracy: 0.9533\n",
      "iteration number: 551\t training loss: 0.0274\tvalidation loss: 0.1467\t validation accuracy: 0.9533\n",
      "iteration number: 552\t training loss: 0.0277\tvalidation loss: 0.1500\t validation accuracy: 0.9489\n",
      "iteration number: 553\t training loss: 0.0284\tvalidation loss: 0.1526\t validation accuracy: 0.9489\n",
      "iteration number: 554\t training loss: 0.0272\tvalidation loss: 0.1531\t validation accuracy: 0.9467\n",
      "iteration number: 555\t training loss: 0.0255\tvalidation loss: 0.1531\t validation accuracy: 0.9467\n",
      "iteration number: 556\t training loss: 0.0244\tvalidation loss: 0.1526\t validation accuracy: 0.9422\n",
      "iteration number: 557\t training loss: 0.0245\tvalidation loss: 0.1532\t validation accuracy: 0.9400\n",
      "iteration number: 558\t training loss: 0.0249\tvalidation loss: 0.1534\t validation accuracy: 0.9422\n",
      "iteration number: 559\t training loss: 0.0254\tvalidation loss: 0.1528\t validation accuracy: 0.9400\n",
      "iteration number: 560\t training loss: 0.0255\tvalidation loss: 0.1504\t validation accuracy: 0.9400\n",
      "iteration number: 561\t training loss: 0.0251\tvalidation loss: 0.1460\t validation accuracy: 0.9422\n",
      "iteration number: 562\t training loss: 0.0240\tvalidation loss: 0.1402\t validation accuracy: 0.9444\n",
      "iteration number: 563\t training loss: 0.0232\tvalidation loss: 0.1343\t validation accuracy: 0.9533\n",
      "iteration number: 564\t training loss: 0.0225\tvalidation loss: 0.1317\t validation accuracy: 0.9511\n",
      "iteration number: 565\t training loss: 0.0226\tvalidation loss: 0.1296\t validation accuracy: 0.9556\n",
      "iteration number: 566\t training loss: 0.0224\tvalidation loss: 0.1262\t validation accuracy: 0.9556\n",
      "iteration number: 567\t training loss: 0.0226\tvalidation loss: 0.1234\t validation accuracy: 0.9556\n",
      "iteration number: 568\t training loss: 0.0227\tvalidation loss: 0.1192\t validation accuracy: 0.9600\n",
      "iteration number: 569\t training loss: 0.0227\tvalidation loss: 0.1167\t validation accuracy: 0.9600\n",
      "iteration number: 570\t training loss: 0.0226\tvalidation loss: 0.1161\t validation accuracy: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 571\t training loss: 0.0225\tvalidation loss: 0.1157\t validation accuracy: 0.9578\n",
      "iteration number: 572\t training loss: 0.0223\tvalidation loss: 0.1164\t validation accuracy: 0.9578\n",
      "iteration number: 573\t training loss: 0.0217\tvalidation loss: 0.1177\t validation accuracy: 0.9578\n",
      "iteration number: 574\t training loss: 0.0210\tvalidation loss: 0.1188\t validation accuracy: 0.9600\n",
      "iteration number: 575\t training loss: 0.0208\tvalidation loss: 0.1221\t validation accuracy: 0.9533\n",
      "iteration number: 576\t training loss: 0.0204\tvalidation loss: 0.1246\t validation accuracy: 0.9533\n",
      "iteration number: 577\t training loss: 0.0192\tvalidation loss: 0.1262\t validation accuracy: 0.9600\n",
      "iteration number: 578\t training loss: 0.0184\tvalidation loss: 0.1282\t validation accuracy: 0.9578\n",
      "iteration number: 579\t training loss: 0.0179\tvalidation loss: 0.1310\t validation accuracy: 0.9556\n",
      "iteration number: 580\t training loss: 0.0177\tvalidation loss: 0.1338\t validation accuracy: 0.9556\n",
      "iteration number: 581\t training loss: 0.0176\tvalidation loss: 0.1345\t validation accuracy: 0.9533\n",
      "iteration number: 582\t training loss: 0.0175\tvalidation loss: 0.1346\t validation accuracy: 0.9489\n",
      "iteration number: 583\t training loss: 0.0175\tvalidation loss: 0.1352\t validation accuracy: 0.9489\n",
      "iteration number: 584\t training loss: 0.0177\tvalidation loss: 0.1355\t validation accuracy: 0.9489\n",
      "iteration number: 585\t training loss: 0.0182\tvalidation loss: 0.1363\t validation accuracy: 0.9444\n",
      "iteration number: 586\t training loss: 0.0191\tvalidation loss: 0.1379\t validation accuracy: 0.9444\n",
      "iteration number: 587\t training loss: 0.0211\tvalidation loss: 0.1396\t validation accuracy: 0.9467\n",
      "iteration number: 588\t training loss: 0.0235\tvalidation loss: 0.1425\t validation accuracy: 0.9489\n",
      "iteration number: 589\t training loss: 0.0257\tvalidation loss: 0.1452\t validation accuracy: 0.9467\n",
      "iteration number: 590\t training loss: 0.0257\tvalidation loss: 0.1464\t validation accuracy: 0.9489\n",
      "iteration number: 591\t training loss: 0.0251\tvalidation loss: 0.1476\t validation accuracy: 0.9489\n",
      "iteration number: 592\t training loss: 0.0222\tvalidation loss: 0.1456\t validation accuracy: 0.9467\n",
      "iteration number: 593\t training loss: 0.0198\tvalidation loss: 0.1423\t validation accuracy: 0.9422\n",
      "iteration number: 594\t training loss: 0.0183\tvalidation loss: 0.1390\t validation accuracy: 0.9422\n",
      "iteration number: 595\t training loss: 0.0183\tvalidation loss: 0.1391\t validation accuracy: 0.9489\n",
      "iteration number: 596\t training loss: 0.0192\tvalidation loss: 0.1404\t validation accuracy: 0.9511\n",
      "iteration number: 597\t training loss: 0.0205\tvalidation loss: 0.1423\t validation accuracy: 0.9511\n",
      "iteration number: 598\t training loss: 0.0222\tvalidation loss: 0.1449\t validation accuracy: 0.9578\n",
      "iteration number: 599\t training loss: 0.0224\tvalidation loss: 0.1453\t validation accuracy: 0.9578\n",
      "iteration number: 600\t training loss: 0.0226\tvalidation loss: 0.1457\t validation accuracy: 0.9578\n",
      "iteration number: 601\t training loss: 0.0214\tvalidation loss: 0.1445\t validation accuracy: 0.9578\n",
      "iteration number: 602\t training loss: 0.0207\tvalidation loss: 0.1442\t validation accuracy: 0.9556\n",
      "iteration number: 603\t training loss: 0.0193\tvalidation loss: 0.1410\t validation accuracy: 0.9556\n",
      "iteration number: 604\t training loss: 0.0187\tvalidation loss: 0.1387\t validation accuracy: 0.9511\n",
      "iteration number: 605\t training loss: 0.0188\tvalidation loss: 0.1376\t validation accuracy: 0.9511\n",
      "iteration number: 606\t training loss: 0.0188\tvalidation loss: 0.1341\t validation accuracy: 0.9511\n",
      "iteration number: 607\t training loss: 0.0185\tvalidation loss: 0.1300\t validation accuracy: 0.9556\n",
      "iteration number: 608\t training loss: 0.0187\tvalidation loss: 0.1279\t validation accuracy: 0.9578\n",
      "iteration number: 609\t training loss: 0.0191\tvalidation loss: 0.1268\t validation accuracy: 0.9578\n",
      "iteration number: 610\t training loss: 0.0194\tvalidation loss: 0.1272\t validation accuracy: 0.9622\n",
      "iteration number: 611\t training loss: 0.0197\tvalidation loss: 0.1277\t validation accuracy: 0.9600\n",
      "iteration number: 612\t training loss: 0.0191\tvalidation loss: 0.1252\t validation accuracy: 0.9600\n",
      "iteration number: 613\t training loss: 0.0188\tvalidation loss: 0.1245\t validation accuracy: 0.9556\n",
      "iteration number: 614\t training loss: 0.0190\tvalidation loss: 0.1248\t validation accuracy: 0.9556\n",
      "iteration number: 615\t training loss: 0.0187\tvalidation loss: 0.1216\t validation accuracy: 0.9600\n",
      "iteration number: 616\t training loss: 0.0186\tvalidation loss: 0.1204\t validation accuracy: 0.9622\n",
      "iteration number: 617\t training loss: 0.0187\tvalidation loss: 0.1192\t validation accuracy: 0.9600\n",
      "iteration number: 618\t training loss: 0.0191\tvalidation loss: 0.1189\t validation accuracy: 0.9622\n",
      "iteration number: 619\t training loss: 0.0177\tvalidation loss: 0.1171\t validation accuracy: 0.9622\n",
      "iteration number: 620\t training loss: 0.0164\tvalidation loss: 0.1165\t validation accuracy: 0.9622\n",
      "iteration number: 621\t training loss: 0.0160\tvalidation loss: 0.1190\t validation accuracy: 0.9556\n",
      "iteration number: 622\t training loss: 0.0167\tvalidation loss: 0.1240\t validation accuracy: 0.9489\n",
      "iteration number: 623\t training loss: 0.0169\tvalidation loss: 0.1271\t validation accuracy: 0.9511\n",
      "iteration number: 624\t training loss: 0.0174\tvalidation loss: 0.1299\t validation accuracy: 0.9511\n",
      "iteration number: 625\t training loss: 0.0179\tvalidation loss: 0.1321\t validation accuracy: 0.9511\n",
      "iteration number: 626\t training loss: 0.0187\tvalidation loss: 0.1329\t validation accuracy: 0.9511\n",
      "iteration number: 627\t training loss: 0.0196\tvalidation loss: 0.1344\t validation accuracy: 0.9511\n",
      "iteration number: 628\t training loss: 0.0179\tvalidation loss: 0.1256\t validation accuracy: 0.9578\n",
      "iteration number: 629\t training loss: 0.0168\tvalidation loss: 0.1194\t validation accuracy: 0.9556\n",
      "iteration number: 630\t training loss: 0.0162\tvalidation loss: 0.1147\t validation accuracy: 0.9578\n",
      "iteration number: 631\t training loss: 0.0161\tvalidation loss: 0.1121\t validation accuracy: 0.9600\n",
      "iteration number: 632\t training loss: 0.0163\tvalidation loss: 0.1102\t validation accuracy: 0.9622\n",
      "iteration number: 633\t training loss: 0.0165\tvalidation loss: 0.1103\t validation accuracy: 0.9622\n",
      "iteration number: 634\t training loss: 0.0167\tvalidation loss: 0.1110\t validation accuracy: 0.9622\n",
      "iteration number: 635\t training loss: 0.0165\tvalidation loss: 0.1118\t validation accuracy: 0.9556\n",
      "iteration number: 636\t training loss: 0.0158\tvalidation loss: 0.1124\t validation accuracy: 0.9556\n",
      "iteration number: 637\t training loss: 0.0151\tvalidation loss: 0.1138\t validation accuracy: 0.9556\n",
      "iteration number: 638\t training loss: 0.0146\tvalidation loss: 0.1154\t validation accuracy: 0.9578\n",
      "iteration number: 639\t training loss: 0.0142\tvalidation loss: 0.1186\t validation accuracy: 0.9600\n",
      "iteration number: 640\t training loss: 0.0142\tvalidation loss: 0.1223\t validation accuracy: 0.9600\n",
      "iteration number: 641\t training loss: 0.0145\tvalidation loss: 0.1259\t validation accuracy: 0.9578\n",
      "iteration number: 642\t training loss: 0.0149\tvalidation loss: 0.1303\t validation accuracy: 0.9533\n",
      "iteration number: 643\t training loss: 0.0152\tvalidation loss: 0.1336\t validation accuracy: 0.9511\n",
      "iteration number: 644\t training loss: 0.0154\tvalidation loss: 0.1358\t validation accuracy: 0.9511\n",
      "iteration number: 645\t training loss: 0.0156\tvalidation loss: 0.1378\t validation accuracy: 0.9533\n",
      "iteration number: 646\t training loss: 0.0161\tvalidation loss: 0.1399\t validation accuracy: 0.9533\n",
      "iteration number: 647\t training loss: 0.0162\tvalidation loss: 0.1381\t validation accuracy: 0.9533\n",
      "iteration number: 648\t training loss: 0.0163\tvalidation loss: 0.1343\t validation accuracy: 0.9556\n",
      "iteration number: 649\t training loss: 0.0166\tvalidation loss: 0.1314\t validation accuracy: 0.9578\n",
      "iteration number: 650\t training loss: 0.0172\tvalidation loss: 0.1274\t validation accuracy: 0.9556\n",
      "iteration number: 651\t training loss: 0.0184\tvalidation loss: 0.1254\t validation accuracy: 0.9600\n",
      "iteration number: 652\t training loss: 0.0196\tvalidation loss: 0.1246\t validation accuracy: 0.9600\n",
      "iteration number: 653\t training loss: 0.0186\tvalidation loss: 0.1193\t validation accuracy: 0.9600\n",
      "iteration number: 654\t training loss: 0.0183\tvalidation loss: 0.1165\t validation accuracy: 0.9600\n",
      "iteration number: 655\t training loss: 0.0164\tvalidation loss: 0.1161\t validation accuracy: 0.9578\n",
      "iteration number: 656\t training loss: 0.0166\tvalidation loss: 0.1190\t validation accuracy: 0.9556\n",
      "iteration number: 657\t training loss: 0.0191\tvalidation loss: 0.1238\t validation accuracy: 0.9556\n",
      "iteration number: 658\t training loss: 0.0211\tvalidation loss: 0.1291\t validation accuracy: 0.9511\n",
      "iteration number: 659\t training loss: 0.0210\tvalidation loss: 0.1325\t validation accuracy: 0.9556\n",
      "iteration number: 660\t training loss: 0.0196\tvalidation loss: 0.1351\t validation accuracy: 0.9556\n",
      "iteration number: 661\t training loss: 0.0179\tvalidation loss: 0.1382\t validation accuracy: 0.9556\n",
      "iteration number: 662\t training loss: 0.0171\tvalidation loss: 0.1419\t validation accuracy: 0.9533\n",
      "iteration number: 663\t training loss: 0.0169\tvalidation loss: 0.1462\t validation accuracy: 0.9489\n",
      "iteration number: 664\t training loss: 0.0171\tvalidation loss: 0.1508\t validation accuracy: 0.9489\n",
      "iteration number: 665\t training loss: 0.0171\tvalidation loss: 0.1532\t validation accuracy: 0.9467\n",
      "iteration number: 666\t training loss: 0.0172\tvalidation loss: 0.1537\t validation accuracy: 0.9511\n",
      "iteration number: 667\t training loss: 0.0174\tvalidation loss: 0.1540\t validation accuracy: 0.9511\n",
      "iteration number: 668\t training loss: 0.0178\tvalidation loss: 0.1522\t validation accuracy: 0.9556\n",
      "iteration number: 669\t training loss: 0.0182\tvalidation loss: 0.1498\t validation accuracy: 0.9533\n",
      "iteration number: 670\t training loss: 0.0193\tvalidation loss: 0.1486\t validation accuracy: 0.9556\n",
      "iteration number: 671\t training loss: 0.0197\tvalidation loss: 0.1454\t validation accuracy: 0.9578\n",
      "iteration number: 672\t training loss: 0.0199\tvalidation loss: 0.1420\t validation accuracy: 0.9578\n",
      "iteration number: 673\t training loss: 0.0196\tvalidation loss: 0.1380\t validation accuracy: 0.9600\n",
      "iteration number: 674\t training loss: 0.0173\tvalidation loss: 0.1316\t validation accuracy: 0.9578\n",
      "iteration number: 675\t training loss: 0.0153\tvalidation loss: 0.1260\t validation accuracy: 0.9600\n",
      "iteration number: 676\t training loss: 0.0143\tvalidation loss: 0.1246\t validation accuracy: 0.9578\n",
      "iteration number: 677\t training loss: 0.0147\tvalidation loss: 0.1263\t validation accuracy: 0.9556\n",
      "iteration number: 678\t training loss: 0.0157\tvalidation loss: 0.1304\t validation accuracy: 0.9467\n",
      "iteration number: 679\t training loss: 0.0172\tvalidation loss: 0.1343\t validation accuracy: 0.9489\n",
      "iteration number: 680\t training loss: 0.0187\tvalidation loss: 0.1410\t validation accuracy: 0.9489\n",
      "iteration number: 681\t training loss: 0.0199\tvalidation loss: 0.1468\t validation accuracy: 0.9511\n",
      "iteration number: 682\t training loss: 0.0201\tvalidation loss: 0.1479\t validation accuracy: 0.9511\n",
      "iteration number: 683\t training loss: 0.0198\tvalidation loss: 0.1473\t validation accuracy: 0.9600\n",
      "iteration number: 684\t training loss: 0.0201\tvalidation loss: 0.1469\t validation accuracy: 0.9600\n",
      "iteration number: 685\t training loss: 0.0208\tvalidation loss: 0.1466\t validation accuracy: 0.9578\n",
      "iteration number: 686\t training loss: 0.0194\tvalidation loss: 0.1385\t validation accuracy: 0.9578\n",
      "iteration number: 687\t training loss: 0.0187\tvalidation loss: 0.1285\t validation accuracy: 0.9578\n",
      "iteration number: 688\t training loss: 0.0195\tvalidation loss: 0.1225\t validation accuracy: 0.9622\n",
      "iteration number: 689\t training loss: 0.0204\tvalidation loss: 0.1189\t validation accuracy: 0.9622\n",
      "iteration number: 690\t training loss: 0.0216\tvalidation loss: 0.1166\t validation accuracy: 0.9622\n",
      "iteration number: 691\t training loss: 0.0218\tvalidation loss: 0.1147\t validation accuracy: 0.9667\n",
      "iteration number: 692\t training loss: 0.0196\tvalidation loss: 0.1126\t validation accuracy: 0.9689\n",
      "iteration number: 693\t training loss: 0.0175\tvalidation loss: 0.1108\t validation accuracy: 0.9689\n",
      "iteration number: 694\t training loss: 0.0165\tvalidation loss: 0.1109\t validation accuracy: 0.9689\n",
      "iteration number: 695\t training loss: 0.0158\tvalidation loss: 0.1111\t validation accuracy: 0.9689\n",
      "iteration number: 696\t training loss: 0.0156\tvalidation loss: 0.1119\t validation accuracy: 0.9689\n",
      "iteration number: 697\t training loss: 0.0150\tvalidation loss: 0.1137\t validation accuracy: 0.9622\n",
      "iteration number: 698\t training loss: 0.0146\tvalidation loss: 0.1167\t validation accuracy: 0.9600\n",
      "iteration number: 699\t training loss: 0.0146\tvalidation loss: 0.1209\t validation accuracy: 0.9600\n",
      "iteration number: 700\t training loss: 0.0150\tvalidation loss: 0.1253\t validation accuracy: 0.9600\n",
      "iteration number: 701\t training loss: 0.0151\tvalidation loss: 0.1312\t validation accuracy: 0.9556\n",
      "iteration number: 702\t training loss: 0.0152\tvalidation loss: 0.1366\t validation accuracy: 0.9556\n",
      "iteration number: 703\t training loss: 0.0145\tvalidation loss: 0.1376\t validation accuracy: 0.9489\n",
      "iteration number: 704\t training loss: 0.0144\tvalidation loss: 0.1409\t validation accuracy: 0.9489\n",
      "iteration number: 705\t training loss: 0.0147\tvalidation loss: 0.1449\t validation accuracy: 0.9489\n",
      "iteration number: 706\t training loss: 0.0153\tvalidation loss: 0.1485\t validation accuracy: 0.9467\n",
      "iteration number: 707\t training loss: 0.0159\tvalidation loss: 0.1525\t validation accuracy: 0.9467\n",
      "iteration number: 708\t training loss: 0.0166\tvalidation loss: 0.1560\t validation accuracy: 0.9467\n",
      "iteration number: 709\t training loss: 0.0159\tvalidation loss: 0.1569\t validation accuracy: 0.9467\n",
      "iteration number: 710\t training loss: 0.0157\tvalidation loss: 0.1594\t validation accuracy: 0.9489\n",
      "iteration number: 711\t training loss: 0.0155\tvalidation loss: 0.1603\t validation accuracy: 0.9444\n",
      "iteration number: 712\t training loss: 0.0151\tvalidation loss: 0.1605\t validation accuracy: 0.9444\n",
      "iteration number: 713\t training loss: 0.0146\tvalidation loss: 0.1585\t validation accuracy: 0.9467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 714\t training loss: 0.0143\tvalidation loss: 0.1569\t validation accuracy: 0.9489\n",
      "iteration number: 715\t training loss: 0.0143\tvalidation loss: 0.1557\t validation accuracy: 0.9444\n",
      "iteration number: 716\t training loss: 0.0146\tvalidation loss: 0.1544\t validation accuracy: 0.9422\n",
      "iteration number: 717\t training loss: 0.0140\tvalidation loss: 0.1510\t validation accuracy: 0.9444\n",
      "iteration number: 718\t training loss: 0.0133\tvalidation loss: 0.1469\t validation accuracy: 0.9489\n",
      "iteration number: 719\t training loss: 0.0128\tvalidation loss: 0.1428\t validation accuracy: 0.9489\n",
      "iteration number: 720\t training loss: 0.0124\tvalidation loss: 0.1381\t validation accuracy: 0.9489\n",
      "iteration number: 721\t training loss: 0.0121\tvalidation loss: 0.1329\t validation accuracy: 0.9511\n",
      "iteration number: 722\t training loss: 0.0120\tvalidation loss: 0.1282\t validation accuracy: 0.9533\n",
      "iteration number: 723\t training loss: 0.0120\tvalidation loss: 0.1238\t validation accuracy: 0.9556\n",
      "iteration number: 724\t training loss: 0.0121\tvalidation loss: 0.1203\t validation accuracy: 0.9600\n",
      "iteration number: 725\t training loss: 0.0122\tvalidation loss: 0.1175\t validation accuracy: 0.9578\n",
      "iteration number: 726\t training loss: 0.0123\tvalidation loss: 0.1150\t validation accuracy: 0.9622\n",
      "iteration number: 727\t training loss: 0.0126\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 728\t training loss: 0.0130\tvalidation loss: 0.1118\t validation accuracy: 0.9644\n",
      "iteration number: 729\t training loss: 0.0135\tvalidation loss: 0.1108\t validation accuracy: 0.9644\n",
      "iteration number: 730\t training loss: 0.0141\tvalidation loss: 0.1102\t validation accuracy: 0.9644\n",
      "iteration number: 731\t training loss: 0.0145\tvalidation loss: 0.1097\t validation accuracy: 0.9644\n",
      "iteration number: 732\t training loss: 0.0149\tvalidation loss: 0.1098\t validation accuracy: 0.9644\n",
      "iteration number: 733\t training loss: 0.0148\tvalidation loss: 0.1093\t validation accuracy: 0.9644\n",
      "iteration number: 734\t training loss: 0.0144\tvalidation loss: 0.1091\t validation accuracy: 0.9644\n",
      "iteration number: 735\t training loss: 0.0141\tvalidation loss: 0.1090\t validation accuracy: 0.9667\n",
      "iteration number: 736\t training loss: 0.0140\tvalidation loss: 0.1095\t validation accuracy: 0.9667\n",
      "iteration number: 737\t training loss: 0.0139\tvalidation loss: 0.1108\t validation accuracy: 0.9644\n",
      "iteration number: 738\t training loss: 0.0139\tvalidation loss: 0.1136\t validation accuracy: 0.9622\n",
      "iteration number: 739\t training loss: 0.0135\tvalidation loss: 0.1157\t validation accuracy: 0.9622\n",
      "iteration number: 740\t training loss: 0.0132\tvalidation loss: 0.1184\t validation accuracy: 0.9578\n",
      "iteration number: 741\t training loss: 0.0130\tvalidation loss: 0.1210\t validation accuracy: 0.9578\n",
      "iteration number: 742\t training loss: 0.0128\tvalidation loss: 0.1231\t validation accuracy: 0.9578\n",
      "iteration number: 743\t training loss: 0.0127\tvalidation loss: 0.1245\t validation accuracy: 0.9556\n",
      "iteration number: 744\t training loss: 0.0129\tvalidation loss: 0.1261\t validation accuracy: 0.9533\n",
      "iteration number: 745\t training loss: 0.0128\tvalidation loss: 0.1272\t validation accuracy: 0.9533\n",
      "iteration number: 746\t training loss: 0.0128\tvalidation loss: 0.1290\t validation accuracy: 0.9533\n",
      "iteration number: 747\t training loss: 0.0127\tvalidation loss: 0.1308\t validation accuracy: 0.9533\n",
      "iteration number: 748\t training loss: 0.0126\tvalidation loss: 0.1324\t validation accuracy: 0.9533\n",
      "iteration number: 749\t training loss: 0.0124\tvalidation loss: 0.1323\t validation accuracy: 0.9533\n",
      "iteration number: 750\t training loss: 0.0122\tvalidation loss: 0.1319\t validation accuracy: 0.9511\n",
      "iteration number: 751\t training loss: 0.0121\tvalidation loss: 0.1305\t validation accuracy: 0.9533\n",
      "iteration number: 752\t training loss: 0.0120\tvalidation loss: 0.1289\t validation accuracy: 0.9511\n",
      "iteration number: 753\t training loss: 0.0121\tvalidation loss: 0.1273\t validation accuracy: 0.9533\n",
      "iteration number: 754\t training loss: 0.0123\tvalidation loss: 0.1256\t validation accuracy: 0.9533\n",
      "iteration number: 755\t training loss: 0.0125\tvalidation loss: 0.1251\t validation accuracy: 0.9533\n",
      "iteration number: 756\t training loss: 0.0126\tvalidation loss: 0.1250\t validation accuracy: 0.9556\n",
      "iteration number: 757\t training loss: 0.0127\tvalidation loss: 0.1254\t validation accuracy: 0.9556\n",
      "iteration number: 758\t training loss: 0.0127\tvalidation loss: 0.1261\t validation accuracy: 0.9556\n",
      "iteration number: 759\t training loss: 0.0123\tvalidation loss: 0.1270\t validation accuracy: 0.9533\n",
      "iteration number: 760\t training loss: 0.0119\tvalidation loss: 0.1278\t validation accuracy: 0.9511\n",
      "iteration number: 761\t training loss: 0.0116\tvalidation loss: 0.1287\t validation accuracy: 0.9511\n",
      "iteration number: 762\t training loss: 0.0116\tvalidation loss: 0.1278\t validation accuracy: 0.9511\n",
      "iteration number: 763\t training loss: 0.0116\tvalidation loss: 0.1281\t validation accuracy: 0.9511\n",
      "iteration number: 764\t training loss: 0.0116\tvalidation loss: 0.1283\t validation accuracy: 0.9511\n",
      "iteration number: 765\t training loss: 0.0116\tvalidation loss: 0.1286\t validation accuracy: 0.9511\n",
      "iteration number: 766\t training loss: 0.0116\tvalidation loss: 0.1288\t validation accuracy: 0.9511\n",
      "iteration number: 767\t training loss: 0.0115\tvalidation loss: 0.1294\t validation accuracy: 0.9511\n",
      "iteration number: 768\t training loss: 0.0113\tvalidation loss: 0.1297\t validation accuracy: 0.9511\n",
      "iteration number: 769\t training loss: 0.0111\tvalidation loss: 0.1302\t validation accuracy: 0.9489\n",
      "iteration number: 770\t training loss: 0.0110\tvalidation loss: 0.1308\t validation accuracy: 0.9489\n",
      "iteration number: 771\t training loss: 0.0110\tvalidation loss: 0.1311\t validation accuracy: 0.9489\n",
      "iteration number: 772\t training loss: 0.0109\tvalidation loss: 0.1314\t validation accuracy: 0.9511\n",
      "iteration number: 773\t training loss: 0.0109\tvalidation loss: 0.1318\t validation accuracy: 0.9511\n",
      "iteration number: 774\t training loss: 0.0109\tvalidation loss: 0.1326\t validation accuracy: 0.9489\n",
      "iteration number: 775\t training loss: 0.0111\tvalidation loss: 0.1331\t validation accuracy: 0.9489\n",
      "iteration number: 776\t training loss: 0.0110\tvalidation loss: 0.1318\t validation accuracy: 0.9511\n",
      "iteration number: 777\t training loss: 0.0110\tvalidation loss: 0.1310\t validation accuracy: 0.9511\n",
      "iteration number: 778\t training loss: 0.0109\tvalidation loss: 0.1305\t validation accuracy: 0.9511\n",
      "iteration number: 779\t training loss: 0.0109\tvalidation loss: 0.1301\t validation accuracy: 0.9511\n",
      "iteration number: 780\t training loss: 0.0109\tvalidation loss: 0.1303\t validation accuracy: 0.9533\n",
      "iteration number: 781\t training loss: 0.0109\tvalidation loss: 0.1297\t validation accuracy: 0.9533\n",
      "iteration number: 782\t training loss: 0.0108\tvalidation loss: 0.1299\t validation accuracy: 0.9556\n",
      "iteration number: 783\t training loss: 0.0108\tvalidation loss: 0.1293\t validation accuracy: 0.9533\n",
      "iteration number: 784\t training loss: 0.0108\tvalidation loss: 0.1298\t validation accuracy: 0.9533\n",
      "iteration number: 785\t training loss: 0.0109\tvalidation loss: 0.1307\t validation accuracy: 0.9511\n",
      "iteration number: 786\t training loss: 0.0111\tvalidation loss: 0.1314\t validation accuracy: 0.9533\n",
      "iteration number: 787\t training loss: 0.0113\tvalidation loss: 0.1326\t validation accuracy: 0.9533\n",
      "iteration number: 788\t training loss: 0.0112\tvalidation loss: 0.1313\t validation accuracy: 0.9533\n",
      "iteration number: 789\t training loss: 0.0111\tvalidation loss: 0.1301\t validation accuracy: 0.9533\n",
      "iteration number: 790\t training loss: 0.0111\tvalidation loss: 0.1288\t validation accuracy: 0.9533\n",
      "iteration number: 791\t training loss: 0.0110\tvalidation loss: 0.1269\t validation accuracy: 0.9556\n",
      "iteration number: 792\t training loss: 0.0110\tvalidation loss: 0.1255\t validation accuracy: 0.9533\n",
      "iteration number: 793\t training loss: 0.0109\tvalidation loss: 0.1238\t validation accuracy: 0.9533\n",
      "iteration number: 794\t training loss: 0.0107\tvalidation loss: 0.1250\t validation accuracy: 0.9556\n",
      "iteration number: 795\t training loss: 0.0108\tvalidation loss: 0.1251\t validation accuracy: 0.9533\n",
      "iteration number: 796\t training loss: 0.0112\tvalidation loss: 0.1263\t validation accuracy: 0.9511\n",
      "iteration number: 797\t training loss: 0.0119\tvalidation loss: 0.1282\t validation accuracy: 0.9511\n",
      "iteration number: 798\t training loss: 0.0131\tvalidation loss: 0.1308\t validation accuracy: 0.9511\n",
      "iteration number: 799\t training loss: 0.0138\tvalidation loss: 0.1333\t validation accuracy: 0.9511\n",
      "iteration number: 800\t training loss: 0.0139\tvalidation loss: 0.1352\t validation accuracy: 0.9489\n",
      "iteration number: 801\t training loss: 0.0140\tvalidation loss: 0.1374\t validation accuracy: 0.9467\n",
      "iteration number: 802\t training loss: 0.0140\tvalidation loss: 0.1395\t validation accuracy: 0.9444\n",
      "iteration number: 803\t training loss: 0.0133\tvalidation loss: 0.1424\t validation accuracy: 0.9422\n",
      "iteration number: 804\t training loss: 0.0132\tvalidation loss: 0.1462\t validation accuracy: 0.9422\n",
      "iteration number: 805\t training loss: 0.0133\tvalidation loss: 0.1494\t validation accuracy: 0.9422\n",
      "iteration number: 806\t training loss: 0.0134\tvalidation loss: 0.1529\t validation accuracy: 0.9400\n",
      "iteration number: 807\t training loss: 0.0136\tvalidation loss: 0.1564\t validation accuracy: 0.9400\n",
      "iteration number: 808\t training loss: 0.0139\tvalidation loss: 0.1561\t validation accuracy: 0.9444\n",
      "iteration number: 809\t training loss: 0.0145\tvalidation loss: 0.1556\t validation accuracy: 0.9489\n",
      "iteration number: 810\t training loss: 0.0145\tvalidation loss: 0.1525\t validation accuracy: 0.9511\n",
      "iteration number: 811\t training loss: 0.0146\tvalidation loss: 0.1507\t validation accuracy: 0.9489\n",
      "iteration number: 812\t training loss: 0.0143\tvalidation loss: 0.1474\t validation accuracy: 0.9489\n",
      "iteration number: 813\t training loss: 0.0140\tvalidation loss: 0.1442\t validation accuracy: 0.9467\n",
      "iteration number: 814\t training loss: 0.0129\tvalidation loss: 0.1398\t validation accuracy: 0.9489\n",
      "iteration number: 815\t training loss: 0.0118\tvalidation loss: 0.1358\t validation accuracy: 0.9511\n",
      "iteration number: 816\t training loss: 0.0113\tvalidation loss: 0.1333\t validation accuracy: 0.9511\n",
      "iteration number: 817\t training loss: 0.0108\tvalidation loss: 0.1314\t validation accuracy: 0.9511\n",
      "iteration number: 818\t training loss: 0.0103\tvalidation loss: 0.1301\t validation accuracy: 0.9511\n",
      "iteration number: 819\t training loss: 0.0101\tvalidation loss: 0.1290\t validation accuracy: 0.9556\n",
      "iteration number: 820\t training loss: 0.0100\tvalidation loss: 0.1287\t validation accuracy: 0.9578\n",
      "iteration number: 821\t training loss: 0.0101\tvalidation loss: 0.1284\t validation accuracy: 0.9556\n",
      "iteration number: 822\t training loss: 0.0102\tvalidation loss: 0.1277\t validation accuracy: 0.9556\n",
      "iteration number: 823\t training loss: 0.0104\tvalidation loss: 0.1273\t validation accuracy: 0.9556\n",
      "iteration number: 824\t training loss: 0.0105\tvalidation loss: 0.1247\t validation accuracy: 0.9556\n",
      "iteration number: 825\t training loss: 0.0107\tvalidation loss: 0.1225\t validation accuracy: 0.9578\n",
      "iteration number: 826\t training loss: 0.0110\tvalidation loss: 0.1206\t validation accuracy: 0.9622\n",
      "iteration number: 827\t training loss: 0.0113\tvalidation loss: 0.1192\t validation accuracy: 0.9622\n",
      "iteration number: 828\t training loss: 0.0116\tvalidation loss: 0.1189\t validation accuracy: 0.9644\n",
      "iteration number: 829\t training loss: 0.0120\tvalidation loss: 0.1189\t validation accuracy: 0.9644\n",
      "iteration number: 830\t training loss: 0.0121\tvalidation loss: 0.1191\t validation accuracy: 0.9622\n",
      "iteration number: 831\t training loss: 0.0121\tvalidation loss: 0.1193\t validation accuracy: 0.9622\n",
      "iteration number: 832\t training loss: 0.0116\tvalidation loss: 0.1181\t validation accuracy: 0.9600\n",
      "iteration number: 833\t training loss: 0.0109\tvalidation loss: 0.1174\t validation accuracy: 0.9600\n",
      "iteration number: 834\t training loss: 0.0104\tvalidation loss: 0.1171\t validation accuracy: 0.9578\n",
      "iteration number: 835\t training loss: 0.0100\tvalidation loss: 0.1174\t validation accuracy: 0.9622\n",
      "**MLP with momentum**\n",
      "Training accuracy: 0.9993\n",
      "Validation accuracy: 0.9622\n",
      "Training loss: 0.0100\n",
      "Validation loss: 0.1174\n",
      "Number of iterations: 835\n"
     ]
    }
   ],
   "source": [
    "mlp_with_momentum = MultiLayerPerceptron(\n",
    "    X, Y, hidden_size=50, \n",
    "    activation='relu', \n",
    "    dropout=False, \n",
    "    dropout_rate=0\n",
    ")\n",
    "mlp_with_momentum.train(\n",
    "    optimizer='sgd',\n",
    "    momentum=True,\n",
    "    min_iterations=500,\n",
    "    max_iterations=5000,\n",
    "    initial_step=1e-1,\n",
    "    batch_size=64,\n",
    "    early_stopping=True,\n",
    "    early_stopping_lookbehind=100,\n",
    "    early_stopping_delta=1e-4, \n",
    "    vectorized=True,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"**MLP with momentum**\")\n",
    "print(\"Training accuracy: {:.4f}\".format(mlp_with_momentum.accuracy_on_train()))\n",
    "print(\"Validation accuracy: {:.4f}\".format(mlp_with_momentum.accuracy_on_validation()))\n",
    "print(\"Training loss: {:.4f}\".format(mlp_with_momentum.training_losses_history[-1]))\n",
    "print(\"Validation loss: {:.4f}\".format(mlp_with_momentum.validation_losses_history[-1]))\n",
    "print(\"Number of iterations: {:d}\".format(len(mlp_with_momentum.training_losses_history)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_early_stopping.plot_loss_history(add_to_title=\"early stopping\")\n",
    "mlp_with_momentum.plot_loss_history(add_to_title=\"early stopping and momentum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Comparing without and with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**MLP without early stopping nor dropout**\n",
      "Training accuracy: 0.9785\n",
      "Validation accuracy: 0.9511\n",
      "Training loss: 0.0915\n",
      "Validation loss: 0.1491\n"
     ]
    }
   ],
   "source": [
    "mlp_without_dropout = MultiLayerPerceptron(\n",
    "    X, Y, hidden_size=50, \n",
    "    activation='relu', \n",
    "    dropout=False\n",
    ")\n",
    "mlp_without_dropout.train(\n",
    "    optimizer='sgd',\n",
    "    min_iterations=500,\n",
    "    max_iterations=5000,\n",
    "    initial_step=1e-1,\n",
    "    batch_size=64,\n",
    "    early_stopping=True,\n",
    "    early_stopping_lookbehind=100,\n",
    "    early_stopping_delta=1e-4, \n",
    "    vectorized=True,\n",
    "    verbose=False\n",
    ")\n",
    "print(\"**MLP without early stopping nor dropout**\")\n",
    "print(\"Training accuracy: {:.4f}\".format(mlp_without_dropout.accuracy_on_train()))\n",
    "print(\"Validation accuracy: {:.4f}\".format(mlp_without_dropout.accuracy_on_validation()))\n",
    "print(\"Training loss: {:.4f}\".format(mlp_without_dropout.training_losses_history[-1]))\n",
    "print(\"Validation loss: {:.4f}\".format(mlp_without_dropout.validation_losses_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**MLP with dropout**\n",
      "Training accuracy: 0.9681\n",
      "Validation accuracy: 0.9556\n",
      "Training loss: 0.1078\n",
      "Validation loss: 0.1353\n"
     ]
    }
   ],
   "source": [
    "mlp_with_dropout = MultiLayerPerceptron(\n",
    "    X, Y, hidden_size=50,\n",
    "    activation='relu',\n",
    "    dropout=True,\n",
    "    dropout_rate=0.25\n",
    ")\n",
    "mlp_with_dropout.train(\n",
    "    optimizer='sgd',\n",
    "    min_iterations=500,\n",
    "    max_iterations=5000,\n",
    "    initial_step=1e-1,\n",
    "    batch_size=64,\n",
    "    early_stopping=True,\n",
    "    early_stopping_lookbehind=100,\n",
    "    early_stopping_delta=1e-4, \n",
    "    vectorized=True,\n",
    "    verbose=False\n",
    ")\n",
    "print(\"**MLP with dropout**\")\n",
    "print(\"Training accuracy: {:.4f}\".format(mlp_with_dropout.accuracy_on_train()))\n",
    "print(\"Validation accuracy: {:.4f}\".format(mlp_with_dropout.accuracy_on_validation()))\n",
    "print(\"Training loss: {:.4f}\".format(mlp_with_dropout.training_losses_history[-1]))\n",
    "print(\"Validation loss: {:.4f}\".format(mlp_with_dropout.validation_losses_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_without_dropout.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_with_dropout.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Multiclass classification MLP with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Implement the same network architecture with Keras;\n",
    "    - First using the Sequential API\n",
    "    - Secondly using the functional API\n",
    "\n",
    "#### - Check that the Keras model can approximately reproduce the behavior of the Numpy model.\n",
    "\n",
    "#### - Compute the negative log likelihood of a sample 42 in the validation set (can use `model.predict_proba`).\n",
    "\n",
    "#### - Compute the average negative log-likelihood on the full validation set.\n",
    "\n",
    "#### - Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "#### - Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 500 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X[0].shape[0]\n",
    "n_classes = len(np.unique(Y_tr))\n",
    "n_hidden = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with relu activation\n",
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/100\n",
      "1347/1347 [==============================] - 1s 487us/sample - loss: 2.2349 - accuracy: 0.1440 - val_loss: 2.1766 - val_accuracy: 0.2089\n",
      "Epoch 2/100\n",
      "1347/1347 [==============================] - 0s 58us/sample - loss: 2.1276 - accuracy: 0.2546 - val_loss: 2.0694 - val_accuracy: 0.3444\n",
      "Epoch 3/100\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 2.0016 - accuracy: 0.3534 - val_loss: 1.9285 - val_accuracy: 0.3867\n",
      "Epoch 4/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 1.8556 - accuracy: 0.4662 - val_loss: 1.7849 - val_accuracy: 0.4911\n",
      "Epoch 5/100\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.7109 - accuracy: 0.5316 - val_loss: 1.6395 - val_accuracy: 0.5689\n",
      "Epoch 6/100\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.5459 - accuracy: 0.6244 - val_loss: 1.4737 - val_accuracy: 0.6311\n",
      "Epoch 7/100\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.3805 - accuracy: 0.6778 - val_loss: 1.3218 - val_accuracy: 0.6489\n",
      "Epoch 8/100\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.2350 - accuracy: 0.6927 - val_loss: 1.1840 - val_accuracy: 0.6867\n",
      "Epoch 9/100\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.1095 - accuracy: 0.7342 - val_loss: 1.0696 - val_accuracy: 0.7222\n",
      "Epoch 10/100\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.0061 - accuracy: 0.7647 - val_loss: 0.9723 - val_accuracy: 0.7511\n",
      "Epoch 11/100\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.9158 - accuracy: 0.7854 - val_loss: 0.8899 - val_accuracy: 0.7822\n",
      "Epoch 12/100\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.8413 - accuracy: 0.8077 - val_loss: 0.8192 - val_accuracy: 0.8044\n",
      "Epoch 13/100\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.7766 - accuracy: 0.8270 - val_loss: 0.7606 - val_accuracy: 0.8222\n",
      "Epoch 14/100\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.7223 - accuracy: 0.8471 - val_loss: 0.7075 - val_accuracy: 0.8311\n",
      "Epoch 15/100\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.6727 - accuracy: 0.8515 - val_loss: 0.6610 - val_accuracy: 0.8556\n",
      "Epoch 16/100\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.6315 - accuracy: 0.8530 - val_loss: 0.6211 - val_accuracy: 0.8689\n",
      "Epoch 17/100\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.5928 - accuracy: 0.8656 - val_loss: 0.5847 - val_accuracy: 0.8778\n",
      "Epoch 18/100\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.5588 - accuracy: 0.8679 - val_loss: 0.5527 - val_accuracy: 0.8889\n",
      "Epoch 19/100\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.5296 - accuracy: 0.8723 - val_loss: 0.5231 - val_accuracy: 0.8956\n",
      "Epoch 20/100\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.5020 - accuracy: 0.8820 - val_loss: 0.4965 - val_accuracy: 0.9044\n",
      "Epoch 21/100\n",
      "1347/1347 [==============================] - 0s 55us/sample - loss: 0.4780 - accuracy: 0.8924 - val_loss: 0.4748 - val_accuracy: 0.9089\n",
      "Epoch 22/100\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.4558 - accuracy: 0.8931 - val_loss: 0.4523 - val_accuracy: 0.9133\n",
      "Epoch 23/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.4355 - accuracy: 0.8983 - val_loss: 0.4321 - val_accuracy: 0.9067\n",
      "Epoch 24/100\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.4177 - accuracy: 0.8998 - val_loss: 0.4171 - val_accuracy: 0.9133\n",
      "Epoch 25/100\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.3998 - accuracy: 0.9065 - val_loss: 0.3998 - val_accuracy: 0.9222\n",
      "Epoch 26/100\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.3817 - accuracy: 0.9079 - val_loss: 0.3837 - val_accuracy: 0.9111\n",
      "Epoch 27/100\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.3667 - accuracy: 0.9087 - val_loss: 0.3693 - val_accuracy: 0.9178\n",
      "Epoch 28/100\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3501 - accuracy: 0.9154 - val_loss: 0.3543 - val_accuracy: 0.9178\n",
      "Epoch 29/100\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3355 - accuracy: 0.9146 - val_loss: 0.3391 - val_accuracy: 0.9244\n",
      "Epoch 30/100\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3192 - accuracy: 0.9198 - val_loss: 0.3245 - val_accuracy: 0.9222\n",
      "Epoch 31/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.3053 - accuracy: 0.9206 - val_loss: 0.3090 - val_accuracy: 0.9333\n",
      "Epoch 32/100\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2940 - accuracy: 0.9258 - val_loss: 0.2999 - val_accuracy: 0.9400\n",
      "Epoch 33/100\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2824 - accuracy: 0.9258 - val_loss: 0.2867 - val_accuracy: 0.9333\n",
      "Epoch 34/100\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.2694 - accuracy: 0.9310 - val_loss: 0.2771 - val_accuracy: 0.9333\n",
      "Epoch 35/100\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2596 - accuracy: 0.9347 - val_loss: 0.2674 - val_accuracy: 0.9467\n",
      "Epoch 36/100\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2498 - accuracy: 0.9362 - val_loss: 0.2602 - val_accuracy: 0.9378\n",
      "Epoch 37/100\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2422 - accuracy: 0.9362 - val_loss: 0.2520 - val_accuracy: 0.9400\n",
      "Epoch 38/100\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2346 - accuracy: 0.9391 - val_loss: 0.2454 - val_accuracy: 0.9422\n",
      "Epoch 39/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2275 - accuracy: 0.9428 - val_loss: 0.2394 - val_accuracy: 0.9511\n",
      "Epoch 40/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2206 - accuracy: 0.9458 - val_loss: 0.2336 - val_accuracy: 0.9578\n",
      "Epoch 41/100\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.2132 - accuracy: 0.9443 - val_loss: 0.2275 - val_accuracy: 0.9533\n",
      "Epoch 42/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2085 - accuracy: 0.9458 - val_loss: 0.2238 - val_accuracy: 0.9556\n",
      "Epoch 43/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2047 - accuracy: 0.9465 - val_loss: 0.2197 - val_accuracy: 0.9511\n",
      "Epoch 44/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1971 - accuracy: 0.9503 - val_loss: 0.2151 - val_accuracy: 0.9578\n",
      "Epoch 45/100\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1919 - accuracy: 0.9503 - val_loss: 0.2115 - val_accuracy: 0.9556\n",
      "Epoch 46/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1877 - accuracy: 0.9510 - val_loss: 0.2075 - val_accuracy: 0.9511\n",
      "Epoch 47/100\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.1833 - accuracy: 0.9525 - val_loss: 0.2040 - val_accuracy: 0.9511\n",
      "Epoch 48/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1792 - accuracy: 0.9547 - val_loss: 0.2009 - val_accuracy: 0.9533\n",
      "Epoch 49/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1753 - accuracy: 0.9555 - val_loss: 0.1973 - val_accuracy: 0.9556\n",
      "Epoch 50/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1712 - accuracy: 0.9592 - val_loss: 0.1942 - val_accuracy: 0.9556\n",
      "Epoch 51/100\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1688 - accuracy: 0.9584 - val_loss: 0.1914 - val_accuracy: 0.9578\n",
      "Epoch 52/100\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1668 - accuracy: 0.9599 - val_loss: 0.1902 - val_accuracy: 0.9600\n",
      "Epoch 53/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1606 - accuracy: 0.9599 - val_loss: 0.1872 - val_accuracy: 0.9578\n",
      "Epoch 54/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1580 - accuracy: 0.9629 - val_loss: 0.1844 - val_accuracy: 0.9644\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1538 - accuracy: 0.9621 - val_loss: 0.1819 - val_accuracy: 0.9622\n",
      "Epoch 56/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1503 - accuracy: 0.9651 - val_loss: 0.1796 - val_accuracy: 0.9600\n",
      "Epoch 57/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1482 - accuracy: 0.9659 - val_loss: 0.1773 - val_accuracy: 0.9600\n",
      "Epoch 58/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1459 - accuracy: 0.9659 - val_loss: 0.1759 - val_accuracy: 0.9578\n",
      "Epoch 59/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1418 - accuracy: 0.9651 - val_loss: 0.1741 - val_accuracy: 0.9622\n",
      "Epoch 60/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1392 - accuracy: 0.9681 - val_loss: 0.1730 - val_accuracy: 0.9622\n",
      "Epoch 61/100\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1366 - accuracy: 0.9681 - val_loss: 0.1709 - val_accuracy: 0.9578\n",
      "Epoch 62/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1344 - accuracy: 0.9673 - val_loss: 0.1693 - val_accuracy: 0.9600\n",
      "Epoch 63/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1309 - accuracy: 0.9688 - val_loss: 0.1678 - val_accuracy: 0.9600\n",
      "Epoch 64/100\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1287 - accuracy: 0.9696 - val_loss: 0.1672 - val_accuracy: 0.9578\n",
      "Epoch 65/100\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.1263 - accuracy: 0.9696 - val_loss: 0.1643 - val_accuracy: 0.9600\n",
      "Epoch 66/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1245 - accuracy: 0.9733 - val_loss: 0.1631 - val_accuracy: 0.9578\n",
      "Epoch 67/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1223 - accuracy: 0.9725 - val_loss: 0.1631 - val_accuracy: 0.9622\n",
      "Epoch 68/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1185 - accuracy: 0.9733 - val_loss: 0.1614 - val_accuracy: 0.9578\n",
      "Epoch 69/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1181 - accuracy: 0.9733 - val_loss: 0.1630 - val_accuracy: 0.9644\n",
      "Epoch 70/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1147 - accuracy: 0.9748 - val_loss: 0.1600 - val_accuracy: 0.9644\n",
      "Epoch 71/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1139 - accuracy: 0.9740 - val_loss: 0.1585 - val_accuracy: 0.9600\n",
      "Epoch 72/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1104 - accuracy: 0.9770 - val_loss: 0.1569 - val_accuracy: 0.9622\n",
      "Epoch 73/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1083 - accuracy: 0.9762 - val_loss: 0.1561 - val_accuracy: 0.9600\n",
      "Epoch 74/100\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1064 - accuracy: 0.9770 - val_loss: 0.1551 - val_accuracy: 0.9600\n",
      "Epoch 75/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1056 - accuracy: 0.9777 - val_loss: 0.1553 - val_accuracy: 0.9600\n",
      "Epoch 76/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1040 - accuracy: 0.9762 - val_loss: 0.1538 - val_accuracy: 0.9600\n",
      "Epoch 77/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1020 - accuracy: 0.9762 - val_loss: 0.1521 - val_accuracy: 0.9600\n",
      "Epoch 78/100\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0998 - accuracy: 0.9800 - val_loss: 0.1530 - val_accuracy: 0.9600\n",
      "Epoch 79/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0985 - accuracy: 0.9792 - val_loss: 0.1518 - val_accuracy: 0.9578\n",
      "Epoch 80/100\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0975 - accuracy: 0.9785 - val_loss: 0.1507 - val_accuracy: 0.9622\n",
      "Epoch 81/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0973 - accuracy: 0.9785 - val_loss: 0.1515 - val_accuracy: 0.9600\n",
      "Epoch 82/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0938 - accuracy: 0.9777 - val_loss: 0.1503 - val_accuracy: 0.9622\n",
      "Epoch 83/100\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0924 - accuracy: 0.9807 - val_loss: 0.1503 - val_accuracy: 0.9600\n",
      "Epoch 84/100\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0919 - accuracy: 0.9792 - val_loss: 0.1495 - val_accuracy: 0.9600\n",
      "Epoch 85/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0904 - accuracy: 0.9807 - val_loss: 0.1491 - val_accuracy: 0.9578\n",
      "Epoch 86/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0889 - accuracy: 0.9800 - val_loss: 0.1496 - val_accuracy: 0.9644\n",
      "Epoch 87/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0873 - accuracy: 0.9822 - val_loss: 0.1485 - val_accuracy: 0.9600\n",
      "Epoch 88/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0865 - accuracy: 0.9807 - val_loss: 0.1477 - val_accuracy: 0.9622\n",
      "Epoch 89/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0850 - accuracy: 0.9807 - val_loss: 0.1483 - val_accuracy: 0.9600\n",
      "Epoch 90/100\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0834 - accuracy: 0.9814 - val_loss: 0.1479 - val_accuracy: 0.9600\n",
      "Epoch 91/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0839 - accuracy: 0.9814 - val_loss: 0.1483 - val_accuracy: 0.9644\n",
      "Epoch 92/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0811 - accuracy: 0.9814 - val_loss: 0.1481 - val_accuracy: 0.9644\n",
      "Epoch 93/100\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0807 - accuracy: 0.9822 - val_loss: 0.1462 - val_accuracy: 0.9644\n",
      "Epoch 94/100\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0790 - accuracy: 0.9822 - val_loss: 0.1482 - val_accuracy: 0.9644\n",
      "Epoch 95/100\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0781 - accuracy: 0.9814 - val_loss: 0.1469 - val_accuracy: 0.9644\n",
      "Epoch 96/100\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0768 - accuracy: 0.9807 - val_loss: 0.1463 - val_accuracy: 0.9644\n",
      "Epoch 97/100\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0756 - accuracy: 0.9822 - val_loss: 0.1465 - val_accuracy: 0.9600\n",
      "Epoch 98/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0782 - accuracy: 0.9800 - val_loss: 0.1464 - val_accuracy: 0.9578\n",
      "Epoch 99/100\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0746 - accuracy: 0.9822 - val_loss: 0.1460 - val_accuracy: 0.9600\n",
      "Epoch 100/100\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0735 - accuracy: 0.9807 - val_loss: 0.1453 - val_accuracy: 0.9644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a372627d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(n_hidden, input_dim=n_features, activation=activation))\n",
    "keras_model.add(Dense(n_classes, activation='softmax'))\n",
    "keras_model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "keras_model.fit(\n",
    "    X_tr, to_categorical(Y_tr, num_classes=n_classes),\n",
    "    epochs=100, batch_size=32, verbose=1,\n",
    "    validation_data=(X_val, to_categorical(Y_val, num_classes=n_classes))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sample 42**\n",
      "Negative loglikelihood: 0.0002235\n"
     ]
    }
   ],
   "source": [
    "index_sample = 42\n",
    "\n",
    "Y_val_one_hot_sample = to_categorical(Y_val[index_sample], num_classes=n_classes)\n",
    "Y_val_pred_sample = np.squeeze(keras_model.predict_proba(np.expand_dims(X_val[index_sample], axis=0)))\n",
    "nll_val_sample = -np.log(Y_val_one_hot_sample.dot(Y_val_pred_sample))\n",
    "print(\"**Sample {:d}**\".format(index_sample))\n",
    "print(\"Negative loglikelihood: {:.7f}\".format(nll_val_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Whole validation set**\n",
      "Negative loglikelihood: 0.1452658\n"
     ]
    }
   ],
   "source": [
    "Y_val_one_hot = to_categorical(Y_val, num_classes=n_classes)\n",
    "Y_val_pred = keras_model.predict_proba(X_val)\n",
    "nll_val = np.matmul(Y_val_one_hot.reshape(-1, 1, n_classes), Y_val_pred.reshape(-1, n_classes, 1))\n",
    "nll_val = -np.mean(np.log(np.squeeze(nll_val)))\n",
    "print(\"**Whole validation set**\")\n",
    "print(\"Negative loglikelihood: {:.7f}\".format(nll_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Whole training set**\n",
      "Our negative loglikelihood: 0.0707758\n",
      "Keras negative loglikelihood: 0.0707758\n"
     ]
    }
   ],
   "source": [
    "Y_tr_one_hot = to_categorical(Y_tr, num_classes=n_classes)\n",
    "Y_tr_pred = keras_model.predict_proba(X_tr)\n",
    "nll_tr = np.matmul(Y_tr_one_hot.reshape(-1, 1, n_classes), Y_tr_pred.reshape(-1, n_classes, 1))\n",
    "nll_tr = -np.mean(np.log(np.squeeze(nll_tr)))\n",
    "print(\"**Whole training set**\")\n",
    "print(\"Our negative loglikelihood: {:.7f}\".format(nll_tr))\n",
    "keras_nll_tr = categorical_crossentropy(Y_tr_one_hot, Y_tr_pred)\n",
    "keras_nll_tr = keras_nll_tr.numpy().mean()\n",
    "print(\"Keras negative loglikelihood: {:.7f}\".format(keras_nll_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with relu activation\n",
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/300\n",
      "1347/1347 [==============================] - 0s 360us/sample - loss: 2.2873 - accuracy: 0.0883 - val_loss: 2.2012 - val_accuracy: 0.1667\n",
      "Epoch 2/300\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 2.1358 - accuracy: 0.2146 - val_loss: 2.0691 - val_accuracy: 0.2556\n",
      "Epoch 3/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 2.0023 - accuracy: 0.3051 - val_loss: 1.9350 - val_accuracy: 0.3467\n",
      "Epoch 4/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.8665 - accuracy: 0.3764 - val_loss: 1.7939 - val_accuracy: 0.4022\n",
      "Epoch 5/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.7230 - accuracy: 0.4633 - val_loss: 1.6487 - val_accuracy: 0.4689\n",
      "Epoch 6/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.5739 - accuracy: 0.5620 - val_loss: 1.5003 - val_accuracy: 0.5600\n",
      "Epoch 7/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.4233 - accuracy: 0.6325 - val_loss: 1.3519 - val_accuracy: 0.6644\n",
      "Epoch 8/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.2725 - accuracy: 0.7068 - val_loss: 1.2086 - val_accuracy: 0.7267\n",
      "Epoch 9/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 1.1319 - accuracy: 0.7506 - val_loss: 1.0762 - val_accuracy: 0.7867\n",
      "Epoch 10/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.0068 - accuracy: 0.8040 - val_loss: 0.9601 - val_accuracy: 0.8222\n",
      "Epoch 11/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.8921 - accuracy: 0.8426 - val_loss: 0.8534 - val_accuracy: 0.8400\n",
      "Epoch 12/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.7917 - accuracy: 0.8523 - val_loss: 0.7568 - val_accuracy: 0.8644\n",
      "Epoch 13/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.7056 - accuracy: 0.8671 - val_loss: 0.6800 - val_accuracy: 0.8733\n",
      "Epoch 14/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.6374 - accuracy: 0.8790 - val_loss: 0.6169 - val_accuracy: 0.8800\n",
      "Epoch 15/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.5795 - accuracy: 0.8879 - val_loss: 0.5641 - val_accuracy: 0.8844\n",
      "Epoch 16/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.5318 - accuracy: 0.8901 - val_loss: 0.5221 - val_accuracy: 0.8889\n",
      "Epoch 17/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.4920 - accuracy: 0.8961 - val_loss: 0.4828 - val_accuracy: 0.8956\n",
      "Epoch 18/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.4585 - accuracy: 0.9020 - val_loss: 0.4537 - val_accuracy: 0.9000\n",
      "Epoch 19/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.4305 - accuracy: 0.9102 - val_loss: 0.4232 - val_accuracy: 0.9089\n",
      "Epoch 20/300\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.4054 - accuracy: 0.9109 - val_loss: 0.4002 - val_accuracy: 0.9089\n",
      "Epoch 21/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.3823 - accuracy: 0.9169 - val_loss: 0.3798 - val_accuracy: 0.9111\n",
      "Epoch 22/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.3628 - accuracy: 0.9220 - val_loss: 0.3609 - val_accuracy: 0.9244\n",
      "Epoch 23/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.3461 - accuracy: 0.9228 - val_loss: 0.3460 - val_accuracy: 0.9267\n",
      "Epoch 24/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.3300 - accuracy: 0.9272 - val_loss: 0.3369 - val_accuracy: 0.9178\n",
      "Epoch 25/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.3157 - accuracy: 0.9324 - val_loss: 0.3191 - val_accuracy: 0.9289\n",
      "Epoch 26/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.3025 - accuracy: 0.9310 - val_loss: 0.3092 - val_accuracy: 0.9244\n",
      "Epoch 27/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2944 - accuracy: 0.9347 - val_loss: 0.2985 - val_accuracy: 0.9289\n",
      "Epoch 28/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.2811 - accuracy: 0.9332 - val_loss: 0.2887 - val_accuracy: 0.9267\n",
      "Epoch 29/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.2709 - accuracy: 0.9362 - val_loss: 0.2787 - val_accuracy: 0.9311\n",
      "Epoch 30/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.2626 - accuracy: 0.9384 - val_loss: 0.2715 - val_accuracy: 0.9378\n",
      "Epoch 31/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.2536 - accuracy: 0.9399 - val_loss: 0.2644 - val_accuracy: 0.9356\n",
      "Epoch 32/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.2459 - accuracy: 0.9399 - val_loss: 0.2591 - val_accuracy: 0.9333\n",
      "Epoch 33/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2387 - accuracy: 0.9436 - val_loss: 0.2509 - val_accuracy: 0.9333\n",
      "Epoch 34/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2313 - accuracy: 0.9428 - val_loss: 0.2455 - val_accuracy: 0.9356\n",
      "Epoch 35/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.2259 - accuracy: 0.9443 - val_loss: 0.2416 - val_accuracy: 0.9378\n",
      "Epoch 36/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2192 - accuracy: 0.9458 - val_loss: 0.2346 - val_accuracy: 0.9356\n",
      "Epoch 37/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2157 - accuracy: 0.9458 - val_loss: 0.2342 - val_accuracy: 0.9400\n",
      "Epoch 38/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2115 - accuracy: 0.9473 - val_loss: 0.2292 - val_accuracy: 0.9400\n",
      "Epoch 39/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2034 - accuracy: 0.9488 - val_loss: 0.2235 - val_accuracy: 0.9400\n",
      "Epoch 40/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1982 - accuracy: 0.9480 - val_loss: 0.2192 - val_accuracy: 0.9400\n",
      "Epoch 41/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1932 - accuracy: 0.9525 - val_loss: 0.2161 - val_accuracy: 0.9400\n",
      "Epoch 42/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1886 - accuracy: 0.9525 - val_loss: 0.2118 - val_accuracy: 0.9467\n",
      "Epoch 43/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1853 - accuracy: 0.9569 - val_loss: 0.2106 - val_accuracy: 0.9444\n",
      "Epoch 44/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1823 - accuracy: 0.9540 - val_loss: 0.2045 - val_accuracy: 0.9444\n",
      "Epoch 45/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1768 - accuracy: 0.9555 - val_loss: 0.2021 - val_accuracy: 0.9489\n",
      "Epoch 46/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1726 - accuracy: 0.9577 - val_loss: 0.1997 - val_accuracy: 0.9489\n",
      "Epoch 47/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1697 - accuracy: 0.9607 - val_loss: 0.1973 - val_accuracy: 0.9467\n",
      "Epoch 48/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1662 - accuracy: 0.9592 - val_loss: 0.1942 - val_accuracy: 0.9511\n",
      "Epoch 49/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1630 - accuracy: 0.9636 - val_loss: 0.1917 - val_accuracy: 0.9444\n",
      "Epoch 50/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1607 - accuracy: 0.9636 - val_loss: 0.1902 - val_accuracy: 0.9489\n",
      "Epoch 51/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1566 - accuracy: 0.9673 - val_loss: 0.1875 - val_accuracy: 0.9533\n",
      "Epoch 52/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1534 - accuracy: 0.9659 - val_loss: 0.1853 - val_accuracy: 0.9533\n",
      "Epoch 53/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1510 - accuracy: 0.9666 - val_loss: 0.1838 - val_accuracy: 0.9556\n",
      "Epoch 54/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.1486 - accuracy: 0.9681 - val_loss: 0.1819 - val_accuracy: 0.9556\n",
      "Epoch 55/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1488 - accuracy: 0.9659 - val_loss: 0.1835 - val_accuracy: 0.9578\n",
      "Epoch 56/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1448 - accuracy: 0.9696 - val_loss: 0.1810 - val_accuracy: 0.9533\n",
      "Epoch 57/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1402 - accuracy: 0.9718 - val_loss: 0.1766 - val_accuracy: 0.9600\n",
      "Epoch 58/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1393 - accuracy: 0.9696 - val_loss: 0.1747 - val_accuracy: 0.9578\n",
      "Epoch 59/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1357 - accuracy: 0.9725 - val_loss: 0.1748 - val_accuracy: 0.9556\n",
      "Epoch 60/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1337 - accuracy: 0.9733 - val_loss: 0.1721 - val_accuracy: 0.9600\n",
      "Epoch 61/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1308 - accuracy: 0.9748 - val_loss: 0.1706 - val_accuracy: 0.9578\n",
      "Epoch 62/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1294 - accuracy: 0.9725 - val_loss: 0.1697 - val_accuracy: 0.9600\n",
      "Epoch 63/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1260 - accuracy: 0.9755 - val_loss: 0.1676 - val_accuracy: 0.9600\n",
      "Epoch 64/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1257 - accuracy: 0.9733 - val_loss: 0.1666 - val_accuracy: 0.9600\n",
      "Epoch 65/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.1242 - accuracy: 0.9740 - val_loss: 0.1653 - val_accuracy: 0.9622\n",
      "Epoch 66/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1200 - accuracy: 0.9762 - val_loss: 0.1633 - val_accuracy: 0.9600\n",
      "Epoch 67/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1184 - accuracy: 0.9755 - val_loss: 0.1623 - val_accuracy: 0.9600\n",
      "Epoch 68/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1168 - accuracy: 0.9770 - val_loss: 0.1615 - val_accuracy: 0.9622\n",
      "Epoch 69/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1147 - accuracy: 0.9777 - val_loss: 0.1597 - val_accuracy: 0.9622\n",
      "Epoch 70/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1128 - accuracy: 0.9770 - val_loss: 0.1578 - val_accuracy: 0.9622\n",
      "Epoch 71/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1108 - accuracy: 0.9770 - val_loss: 0.1581 - val_accuracy: 0.9600\n",
      "Epoch 72/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1094 - accuracy: 0.9755 - val_loss: 0.1562 - val_accuracy: 0.9578\n",
      "Epoch 73/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1078 - accuracy: 0.9785 - val_loss: 0.1551 - val_accuracy: 0.9600\n",
      "Epoch 74/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1056 - accuracy: 0.9792 - val_loss: 0.1557 - val_accuracy: 0.9578\n",
      "Epoch 75/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1044 - accuracy: 0.9800 - val_loss: 0.1535 - val_accuracy: 0.9600\n",
      "Epoch 76/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1025 - accuracy: 0.9807 - val_loss: 0.1520 - val_accuracy: 0.9600\n",
      "Epoch 77/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.1014 - accuracy: 0.9800 - val_loss: 0.1524 - val_accuracy: 0.9600\n",
      "Epoch 78/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0989 - accuracy: 0.9822 - val_loss: 0.1510 - val_accuracy: 0.9600\n",
      "Epoch 79/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0978 - accuracy: 0.9814 - val_loss: 0.1503 - val_accuracy: 0.9600\n",
      "Epoch 80/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0969 - accuracy: 0.9807 - val_loss: 0.1499 - val_accuracy: 0.9600\n",
      "Epoch 81/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0995 - accuracy: 0.9770 - val_loss: 0.1518 - val_accuracy: 0.9578\n",
      "Epoch 82/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0939 - accuracy: 0.9807 - val_loss: 0.1489 - val_accuracy: 0.9600\n",
      "Epoch 83/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0916 - accuracy: 0.9822 - val_loss: 0.1485 - val_accuracy: 0.9600\n",
      "Epoch 84/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0904 - accuracy: 0.9822 - val_loss: 0.1469 - val_accuracy: 0.9600\n",
      "Epoch 85/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0901 - accuracy: 0.9807 - val_loss: 0.1463 - val_accuracy: 0.9622\n",
      "Epoch 86/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0888 - accuracy: 0.9837 - val_loss: 0.1467 - val_accuracy: 0.9600\n",
      "Epoch 87/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0864 - accuracy: 0.9837 - val_loss: 0.1451 - val_accuracy: 0.9578\n",
      "Epoch 88/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0861 - accuracy: 0.9822 - val_loss: 0.1451 - val_accuracy: 0.9600\n",
      "Epoch 89/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0843 - accuracy: 0.9859 - val_loss: 0.1436 - val_accuracy: 0.9600\n",
      "Epoch 90/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0827 - accuracy: 0.9837 - val_loss: 0.1438 - val_accuracy: 0.9578\n",
      "Epoch 91/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0828 - accuracy: 0.9837 - val_loss: 0.1426 - val_accuracy: 0.9622\n",
      "Epoch 92/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0805 - accuracy: 0.9852 - val_loss: 0.1416 - val_accuracy: 0.9622\n",
      "Epoch 93/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0796 - accuracy: 0.9844 - val_loss: 0.1417 - val_accuracy: 0.9600\n",
      "Epoch 94/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0799 - accuracy: 0.9837 - val_loss: 0.1416 - val_accuracy: 0.9600\n",
      "Epoch 95/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0774 - accuracy: 0.9844 - val_loss: 0.1425 - val_accuracy: 0.9622\n",
      "Epoch 96/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0767 - accuracy: 0.9852 - val_loss: 0.1416 - val_accuracy: 0.9600\n",
      "Epoch 97/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0745 - accuracy: 0.9866 - val_loss: 0.1419 - val_accuracy: 0.9622\n",
      "Epoch 98/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0738 - accuracy: 0.9859 - val_loss: 0.1399 - val_accuracy: 0.9622\n",
      "Epoch 99/300\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.0734 - accuracy: 0.9852 - val_loss: 0.1389 - val_accuracy: 0.9622\n",
      "Epoch 100/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0733 - accuracy: 0.9852 - val_loss: 0.1390 - val_accuracy: 0.9600\n",
      "Epoch 101/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0706 - accuracy: 0.9844 - val_loss: 0.1381 - val_accuracy: 0.9600\n",
      "Epoch 102/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0698 - accuracy: 0.9866 - val_loss: 0.1374 - val_accuracy: 0.9600\n",
      "Epoch 103/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0689 - accuracy: 0.9881 - val_loss: 0.1369 - val_accuracy: 0.9600\n",
      "Epoch 104/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0683 - accuracy: 0.9866 - val_loss: 0.1375 - val_accuracy: 0.9622\n",
      "Epoch 105/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0667 - accuracy: 0.9881 - val_loss: 0.1371 - val_accuracy: 0.9622\n",
      "Epoch 106/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0658 - accuracy: 0.9866 - val_loss: 0.1373 - val_accuracy: 0.9644\n",
      "Epoch 107/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0651 - accuracy: 0.9874 - val_loss: 0.1362 - val_accuracy: 0.9600\n",
      "Epoch 108/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0641 - accuracy: 0.9881 - val_loss: 0.1358 - val_accuracy: 0.9622\n",
      "Epoch 109/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0632 - accuracy: 0.9889 - val_loss: 0.1350 - val_accuracy: 0.9600\n",
      "Epoch 110/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0621 - accuracy: 0.9881 - val_loss: 0.1360 - val_accuracy: 0.9622\n",
      "Epoch 111/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0614 - accuracy: 0.9889 - val_loss: 0.1360 - val_accuracy: 0.9622\n",
      "Epoch 112/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0609 - accuracy: 0.9911 - val_loss: 0.1360 - val_accuracy: 0.9600\n",
      "Epoch 113/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0598 - accuracy: 0.9918 - val_loss: 0.1350 - val_accuracy: 0.9578\n",
      "Epoch 114/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0591 - accuracy: 0.9896 - val_loss: 0.1339 - val_accuracy: 0.9622\n",
      "Epoch 115/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0581 - accuracy: 0.9918 - val_loss: 0.1333 - val_accuracy: 0.9667\n",
      "Epoch 116/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0578 - accuracy: 0.9911 - val_loss: 0.1336 - val_accuracy: 0.9644\n",
      "Epoch 117/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0571 - accuracy: 0.9903 - val_loss: 0.1345 - val_accuracy: 0.9644\n",
      "Epoch 118/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0562 - accuracy: 0.9911 - val_loss: 0.1324 - val_accuracy: 0.9600\n",
      "Epoch 119/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0550 - accuracy: 0.9911 - val_loss: 0.1366 - val_accuracy: 0.9600\n",
      "Epoch 120/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0545 - accuracy: 0.9933 - val_loss: 0.1337 - val_accuracy: 0.9667\n",
      "Epoch 121/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0551 - accuracy: 0.9903 - val_loss: 0.1334 - val_accuracy: 0.9622\n",
      "Epoch 122/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0530 - accuracy: 0.9926 - val_loss: 0.1326 - val_accuracy: 0.9667\n",
      "Epoch 123/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0525 - accuracy: 0.9918 - val_loss: 0.1332 - val_accuracy: 0.9644\n",
      "Epoch 124/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0517 - accuracy: 0.9918 - val_loss: 0.1329 - val_accuracy: 0.9644\n",
      "Epoch 125/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0510 - accuracy: 0.9918 - val_loss: 0.1330 - val_accuracy: 0.9644\n",
      "Epoch 126/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0504 - accuracy: 0.9918 - val_loss: 0.1340 - val_accuracy: 0.9644\n",
      "Epoch 127/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0500 - accuracy: 0.9941 - val_loss: 0.1317 - val_accuracy: 0.9644\n",
      "Epoch 128/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0495 - accuracy: 0.9933 - val_loss: 0.1335 - val_accuracy: 0.9600\n",
      "Epoch 129/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0537 - accuracy: 0.9889 - val_loss: 0.1320 - val_accuracy: 0.9644\n",
      "Epoch 130/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0491 - accuracy: 0.9918 - val_loss: 0.1342 - val_accuracy: 0.9644\n",
      "Epoch 131/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0480 - accuracy: 0.9933 - val_loss: 0.1334 - val_accuracy: 0.9622\n",
      "Epoch 132/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0474 - accuracy: 0.9941 - val_loss: 0.1344 - val_accuracy: 0.9644\n",
      "Epoch 133/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0465 - accuracy: 0.9941 - val_loss: 0.1319 - val_accuracy: 0.9622\n",
      "Epoch 134/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0459 - accuracy: 0.9933 - val_loss: 0.1337 - val_accuracy: 0.9644\n",
      "Epoch 135/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0455 - accuracy: 0.9948 - val_loss: 0.1329 - val_accuracy: 0.9644\n",
      "Epoch 136/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0451 - accuracy: 0.9948 - val_loss: 0.1316 - val_accuracy: 0.9644\n",
      "Epoch 137/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0446 - accuracy: 0.9933 - val_loss: 0.1326 - val_accuracy: 0.9644\n",
      "Epoch 138/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0443 - accuracy: 0.9941 - val_loss: 0.1321 - val_accuracy: 0.9622\n",
      "Epoch 139/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0434 - accuracy: 0.9948 - val_loss: 0.1320 - val_accuracy: 0.9644\n",
      "Epoch 140/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0428 - accuracy: 0.9948 - val_loss: 0.1329 - val_accuracy: 0.9644\n",
      "Epoch 141/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0422 - accuracy: 0.9941 - val_loss: 0.1309 - val_accuracy: 0.9622\n",
      "Epoch 142/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0419 - accuracy: 0.9948 - val_loss: 0.1327 - val_accuracy: 0.9622\n",
      "Epoch 143/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0419 - accuracy: 0.9948 - val_loss: 0.1320 - val_accuracy: 0.9644\n",
      "Epoch 144/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0414 - accuracy: 0.9948 - val_loss: 0.1325 - val_accuracy: 0.9644\n",
      "Epoch 145/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0404 - accuracy: 0.9948 - val_loss: 0.1327 - val_accuracy: 0.9622\n",
      "Epoch 146/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0400 - accuracy: 0.9955 - val_loss: 0.1319 - val_accuracy: 0.9644\n",
      "Epoch 147/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0402 - accuracy: 0.9955 - val_loss: 0.1307 - val_accuracy: 0.9644\n",
      "Epoch 148/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0390 - accuracy: 0.9948 - val_loss: 0.1336 - val_accuracy: 0.9644\n",
      "Epoch 149/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0391 - accuracy: 0.9941 - val_loss: 0.1324 - val_accuracy: 0.9667\n",
      "Epoch 150/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0383 - accuracy: 0.9955 - val_loss: 0.1333 - val_accuracy: 0.9644\n",
      "Epoch 151/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0384 - accuracy: 0.9948 - val_loss: 0.1335 - val_accuracy: 0.9644\n",
      "Epoch 152/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0374 - accuracy: 0.9948 - val_loss: 0.1327 - val_accuracy: 0.9667\n",
      "Epoch 153/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0370 - accuracy: 0.9955 - val_loss: 0.1326 - val_accuracy: 0.9667\n",
      "Epoch 154/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0366 - accuracy: 0.9955 - val_loss: 0.1316 - val_accuracy: 0.9644\n",
      "Epoch 155/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0361 - accuracy: 0.9963 - val_loss: 0.1329 - val_accuracy: 0.9667\n",
      "Epoch 156/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0358 - accuracy: 0.9955 - val_loss: 0.1323 - val_accuracy: 0.9644\n",
      "Epoch 157/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0360 - accuracy: 0.9963 - val_loss: 0.1329 - val_accuracy: 0.9667\n",
      "Epoch 158/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0352 - accuracy: 0.9955 - val_loss: 0.1338 - val_accuracy: 0.9667\n",
      "Epoch 159/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0345 - accuracy: 0.9963 - val_loss: 0.1334 - val_accuracy: 0.9667\n",
      "Epoch 160/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0352 - accuracy: 0.9963 - val_loss: 0.1345 - val_accuracy: 0.9644\n",
      "Epoch 161/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0344 - accuracy: 0.9963 - val_loss: 0.1321 - val_accuracy: 0.9689\n",
      "Epoch 162/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.0336 - accuracy: 0.9955 - val_loss: 0.1318 - val_accuracy: 0.9667\n",
      "Epoch 163/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0337 - accuracy: 0.9963 - val_loss: 0.1325 - val_accuracy: 0.9644\n",
      "Epoch 164/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0334 - accuracy: 0.9963 - val_loss: 0.1324 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0330 - accuracy: 0.9963 - val_loss: 0.1333 - val_accuracy: 0.9689\n",
      "Epoch 166/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0328 - accuracy: 0.9963 - val_loss: 0.1337 - val_accuracy: 0.9689\n",
      "Epoch 167/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0323 - accuracy: 0.9978 - val_loss: 0.1323 - val_accuracy: 0.9689\n",
      "Epoch 168/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0317 - accuracy: 0.9970 - val_loss: 0.1344 - val_accuracy: 0.9689\n",
      "Epoch 169/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0310 - accuracy: 0.9970 - val_loss: 0.1349 - val_accuracy: 0.9689\n",
      "Epoch 170/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0331 - accuracy: 0.9955 - val_loss: 0.1365 - val_accuracy: 0.9689\n",
      "Epoch 171/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0310 - accuracy: 0.9978 - val_loss: 0.1329 - val_accuracy: 0.9689\n",
      "Epoch 172/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0304 - accuracy: 0.9955 - val_loss: 0.1322 - val_accuracy: 0.9667\n",
      "Epoch 173/300\n",
      "1347/1347 [==============================] - 0s 40us/sample - loss: 0.0303 - accuracy: 0.9963 - val_loss: 0.1337 - val_accuracy: 0.9689\n",
      "Epoch 174/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0294 - accuracy: 0.9970 - val_loss: 0.1337 - val_accuracy: 0.9689\n",
      "Epoch 175/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0293 - accuracy: 0.9978 - val_loss: 0.1339 - val_accuracy: 0.9689\n",
      "Epoch 176/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0290 - accuracy: 0.9970 - val_loss: 0.1326 - val_accuracy: 0.9689\n",
      "Epoch 177/300\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.0287 - accuracy: 0.9978 - val_loss: 0.1327 - val_accuracy: 0.9667\n",
      "Epoch 178/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0282 - accuracy: 0.9970 - val_loss: 0.1328 - val_accuracy: 0.9667\n",
      "Epoch 179/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0282 - accuracy: 0.9985 - val_loss: 0.1333 - val_accuracy: 0.9689\n",
      "Epoch 180/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0280 - accuracy: 0.9970 - val_loss: 0.1343 - val_accuracy: 0.9689\n",
      "Epoch 181/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.0276 - accuracy: 0.9978 - val_loss: 0.1347 - val_accuracy: 0.9689\n",
      "Epoch 182/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0275 - accuracy: 0.9978 - val_loss: 0.1350 - val_accuracy: 0.9667\n",
      "Epoch 183/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0273 - accuracy: 0.9978 - val_loss: 0.1345 - val_accuracy: 0.9689\n",
      "Epoch 184/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0271 - accuracy: 0.9985 - val_loss: 0.1344 - val_accuracy: 0.9667\n",
      "Epoch 185/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0269 - accuracy: 0.9978 - val_loss: 0.1335 - val_accuracy: 0.9667\n",
      "Epoch 186/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0264 - accuracy: 0.9978 - val_loss: 0.1332 - val_accuracy: 0.9689\n",
      "Epoch 187/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0258 - accuracy: 0.9978 - val_loss: 0.1344 - val_accuracy: 0.9689\n",
      "Epoch 188/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0260 - accuracy: 0.9978 - val_loss: 0.1346 - val_accuracy: 0.9667\n",
      "Epoch 189/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0260 - accuracy: 0.9963 - val_loss: 0.1341 - val_accuracy: 0.9667\n",
      "Epoch 190/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0255 - accuracy: 0.9985 - val_loss: 0.1347 - val_accuracy: 0.9689\n",
      "Epoch 191/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0250 - accuracy: 0.9978 - val_loss: 0.1345 - val_accuracy: 0.9689\n",
      "Epoch 192/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0245 - accuracy: 0.9978 - val_loss: 0.1349 - val_accuracy: 0.9689\n",
      "Epoch 193/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0249 - accuracy: 0.9985 - val_loss: 0.1350 - val_accuracy: 0.9689\n",
      "Epoch 194/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0240 - accuracy: 0.9978 - val_loss: 0.1350 - val_accuracy: 0.9711\n",
      "Epoch 195/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0241 - accuracy: 0.9978 - val_loss: 0.1349 - val_accuracy: 0.9689\n",
      "Epoch 196/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0240 - accuracy: 0.9978 - val_loss: 0.1341 - val_accuracy: 0.9711\n",
      "Epoch 197/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0236 - accuracy: 0.9985 - val_loss: 0.1341 - val_accuracy: 0.9667\n",
      "Epoch 198/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0233 - accuracy: 0.9978 - val_loss: 0.1361 - val_accuracy: 0.9711\n",
      "Epoch 199/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0231 - accuracy: 0.9985 - val_loss: 0.1357 - val_accuracy: 0.9689\n",
      "Epoch 200/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0226 - accuracy: 0.9978 - val_loss: 0.1348 - val_accuracy: 0.9711\n",
      "Epoch 201/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0228 - accuracy: 0.9985 - val_loss: 0.1357 - val_accuracy: 0.9667\n",
      "Epoch 202/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0228 - accuracy: 0.9978 - val_loss: 0.1334 - val_accuracy: 0.9689\n",
      "Epoch 203/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0223 - accuracy: 0.9978 - val_loss: 0.1362 - val_accuracy: 0.9689\n",
      "Epoch 204/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0217 - accuracy: 0.9985 - val_loss: 0.1349 - val_accuracy: 0.9689\n",
      "Epoch 205/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0218 - accuracy: 0.9978 - val_loss: 0.1337 - val_accuracy: 0.9711\n",
      "Epoch 206/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0223 - accuracy: 0.9985 - val_loss: 0.1364 - val_accuracy: 0.9711\n",
      "Epoch 207/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0213 - accuracy: 0.9978 - val_loss: 0.1355 - val_accuracy: 0.9689\n",
      "Epoch 208/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0217 - accuracy: 0.9978 - val_loss: 0.1351 - val_accuracy: 0.9689\n",
      "Epoch 209/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0212 - accuracy: 0.9978 - val_loss: 0.1338 - val_accuracy: 0.9711\n",
      "Epoch 210/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0205 - accuracy: 0.9985 - val_loss: 0.1356 - val_accuracy: 0.9689\n",
      "Epoch 211/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0202 - accuracy: 0.9985 - val_loss: 0.1360 - val_accuracy: 0.9667\n",
      "Epoch 212/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0204 - accuracy: 0.9978 - val_loss: 0.1351 - val_accuracy: 0.9711\n",
      "Epoch 213/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0199 - accuracy: 0.9978 - val_loss: 0.1375 - val_accuracy: 0.9711\n",
      "Epoch 214/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0200 - accuracy: 0.9985 - val_loss: 0.1353 - val_accuracy: 0.9689\n",
      "Epoch 215/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0211 - accuracy: 0.9985 - val_loss: 0.1423 - val_accuracy: 0.9622\n",
      "Epoch 216/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0196 - accuracy: 0.9978 - val_loss: 0.1380 - val_accuracy: 0.9689\n",
      "Epoch 217/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0196 - accuracy: 0.9985 - val_loss: 0.1380 - val_accuracy: 0.9689\n",
      "Epoch 218/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0189 - accuracy: 0.9985 - val_loss: 0.1378 - val_accuracy: 0.9689\n",
      "Epoch 219/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0186 - accuracy: 0.9985 - val_loss: 0.1383 - val_accuracy: 0.9711\n",
      "Epoch 220/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0185 - accuracy: 0.9985 - val_loss: 0.1370 - val_accuracy: 0.9711\n",
      "Epoch 221/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0186 - accuracy: 0.9985 - val_loss: 0.1351 - val_accuracy: 0.9711\n",
      "Epoch 222/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0181 - accuracy: 0.9985 - val_loss: 0.1386 - val_accuracy: 0.9689\n",
      "Epoch 223/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0180 - accuracy: 0.9985 - val_loss: 0.1400 - val_accuracy: 0.9667\n",
      "Epoch 224/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0179 - accuracy: 0.9978 - val_loss: 0.1405 - val_accuracy: 0.9689\n",
      "Epoch 225/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0176 - accuracy: 0.9985 - val_loss: 0.1379 - val_accuracy: 0.9689\n",
      "Epoch 226/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0174 - accuracy: 0.9985 - val_loss: 0.1382 - val_accuracy: 0.9711\n",
      "Epoch 227/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0171 - accuracy: 0.9985 - val_loss: 0.1416 - val_accuracy: 0.9644\n",
      "Epoch 228/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0195 - accuracy: 0.9985 - val_loss: 0.1408 - val_accuracy: 0.9667\n",
      "Epoch 229/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0176 - accuracy: 0.9993 - val_loss: 0.1410 - val_accuracy: 0.9622\n",
      "Epoch 230/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0169 - accuracy: 0.9985 - val_loss: 0.1379 - val_accuracy: 0.9711\n",
      "Epoch 231/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0167 - accuracy: 0.9985 - val_loss: 0.1400 - val_accuracy: 0.9689\n",
      "Epoch 232/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0166 - accuracy: 0.9985 - val_loss: 0.1393 - val_accuracy: 0.9689\n",
      "Epoch 233/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0162 - accuracy: 0.9985 - val_loss: 0.1408 - val_accuracy: 0.9667\n",
      "Epoch 234/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0166 - accuracy: 0.9978 - val_loss: 0.1387 - val_accuracy: 0.9711\n",
      "Epoch 235/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0161 - accuracy: 0.9978 - val_loss: 0.1395 - val_accuracy: 0.9667\n",
      "Epoch 236/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0158 - accuracy: 0.9985 - val_loss: 0.1398 - val_accuracy: 0.9644\n",
      "Epoch 237/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0156 - accuracy: 0.9985 - val_loss: 0.1405 - val_accuracy: 0.9644\n",
      "Epoch 238/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0154 - accuracy: 0.9985 - val_loss: 0.1403 - val_accuracy: 0.9622\n",
      "Epoch 239/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0155 - accuracy: 0.9985 - val_loss: 0.1392 - val_accuracy: 0.9644\n",
      "Epoch 240/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0153 - accuracy: 0.9985 - val_loss: 0.1405 - val_accuracy: 0.9644\n",
      "Epoch 241/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0150 - accuracy: 0.9985 - val_loss: 0.1394 - val_accuracy: 0.9644\n",
      "Epoch 242/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0149 - accuracy: 0.9985 - val_loss: 0.1387 - val_accuracy: 0.9644\n",
      "Epoch 243/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0146 - accuracy: 0.9985 - val_loss: 0.1414 - val_accuracy: 0.9644\n",
      "Epoch 244/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0147 - accuracy: 0.9985 - val_loss: 0.1400 - val_accuracy: 0.9644\n",
      "Epoch 245/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0150 - accuracy: 0.9985 - val_loss: 0.1440 - val_accuracy: 0.9644\n",
      "Epoch 246/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0147 - accuracy: 0.9985 - val_loss: 0.1413 - val_accuracy: 0.9667\n",
      "Epoch 247/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.1381 - val_accuracy: 0.9711\n",
      "Epoch 248/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.1405 - val_accuracy: 0.9667\n",
      "Epoch 249/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0140 - accuracy: 0.9993 - val_loss: 0.1413 - val_accuracy: 0.9644\n",
      "Epoch 250/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0137 - accuracy: 0.9985 - val_loss: 0.1391 - val_accuracy: 0.9689\n",
      "Epoch 251/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0137 - accuracy: 0.9985 - val_loss: 0.1423 - val_accuracy: 0.9667\n",
      "Epoch 252/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0137 - accuracy: 0.9993 - val_loss: 0.1441 - val_accuracy: 0.9622\n",
      "Epoch 253/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0138 - accuracy: 0.9985 - val_loss: 0.1443 - val_accuracy: 0.9689\n",
      "Epoch 254/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0133 - accuracy: 0.9985 - val_loss: 0.1412 - val_accuracy: 0.9622\n",
      "Epoch 255/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0140 - accuracy: 0.9993 - val_loss: 0.1426 - val_accuracy: 0.9622\n",
      "Epoch 256/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0142 - accuracy: 0.9993 - val_loss: 0.1439 - val_accuracy: 0.9644\n",
      "Epoch 257/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0129 - accuracy: 0.9985 - val_loss: 0.1418 - val_accuracy: 0.9667\n",
      "Epoch 258/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0125 - accuracy: 0.9993 - val_loss: 0.1421 - val_accuracy: 0.9622\n",
      "Epoch 259/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0129 - accuracy: 0.9985 - val_loss: 0.1438 - val_accuracy: 0.9622\n",
      "Epoch 260/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0127 - accuracy: 0.9993 - val_loss: 0.1444 - val_accuracy: 0.9644\n",
      "Epoch 261/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0124 - accuracy: 0.9985 - val_loss: 0.1441 - val_accuracy: 0.9644\n",
      "Epoch 262/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0123 - accuracy: 0.9993 - val_loss: 0.1436 - val_accuracy: 0.9689\n",
      "Epoch 263/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0123 - accuracy: 0.9985 - val_loss: 0.1450 - val_accuracy: 0.9622\n",
      "Epoch 264/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0119 - accuracy: 0.9993 - val_loss: 0.1431 - val_accuracy: 0.9644\n",
      "Epoch 265/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0118 - accuracy: 0.9993 - val_loss: 0.1439 - val_accuracy: 0.9622\n",
      "Epoch 266/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0117 - accuracy: 0.9993 - val_loss: 0.1418 - val_accuracy: 0.9644\n",
      "Epoch 267/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0118 - accuracy: 0.9993 - val_loss: 0.1461 - val_accuracy: 0.9644\n",
      "Epoch 268/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0115 - accuracy: 0.9993 - val_loss: 0.1447 - val_accuracy: 0.9622\n",
      "Epoch 269/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0115 - accuracy: 0.9993 - val_loss: 0.1456 - val_accuracy: 0.9644\n",
      "Epoch 270/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0112 - accuracy: 0.9993 - val_loss: 0.1455 - val_accuracy: 0.9644\n",
      "Epoch 271/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0116 - accuracy: 0.9993 - val_loss: 0.1452 - val_accuracy: 0.9644\n",
      "Epoch 272/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0112 - accuracy: 0.9993 - val_loss: 0.1461 - val_accuracy: 0.9622\n",
      "Epoch 273/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0116 - accuracy: 0.9993 - val_loss: 0.1479 - val_accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0110 - accuracy: 0.9993 - val_loss: 0.1472 - val_accuracy: 0.9644\n",
      "Epoch 275/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.0108 - accuracy: 0.9993 - val_loss: 0.1481 - val_accuracy: 0.9644\n",
      "Epoch 276/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.1468 - val_accuracy: 0.9644\n",
      "Epoch 277/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0105 - accuracy: 0.9993 - val_loss: 0.1483 - val_accuracy: 0.9622\n",
      "Epoch 278/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0105 - accuracy: 0.9993 - val_loss: 0.1467 - val_accuracy: 0.9711\n",
      "Epoch 279/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0133 - accuracy: 0.9985 - val_loss: 0.1510 - val_accuracy: 0.9622\n",
      "Epoch 280/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.0119 - accuracy: 0.9993 - val_loss: 0.1445 - val_accuracy: 0.9644\n",
      "Epoch 281/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.0106 - accuracy: 0.9993 - val_loss: 0.1557 - val_accuracy: 0.9644\n",
      "Epoch 282/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0100 - accuracy: 0.9993 - val_loss: 0.1519 - val_accuracy: 0.9600\n",
      "Epoch 283/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0098 - accuracy: 0.9993 - val_loss: 0.1497 - val_accuracy: 0.9622\n",
      "Epoch 284/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0098 - accuracy: 0.9993 - val_loss: 0.1493 - val_accuracy: 0.9622\n",
      "Epoch 285/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.9644\n",
      "Epoch 286/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0096 - accuracy: 0.9993 - val_loss: 0.1484 - val_accuracy: 0.9600\n",
      "Epoch 287/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0098 - accuracy: 0.9993 - val_loss: 0.1498 - val_accuracy: 0.9622\n",
      "Epoch 288/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0094 - accuracy: 0.9993 - val_loss: 0.1506 - val_accuracy: 0.9644\n",
      "Epoch 289/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.0094 - accuracy: 0.9993 - val_loss: 0.1497 - val_accuracy: 0.9622\n",
      "Epoch 290/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.0094 - accuracy: 0.9993 - val_loss: 0.1481 - val_accuracy: 0.9644\n",
      "Epoch 291/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0092 - accuracy: 0.9993 - val_loss: 0.1491 - val_accuracy: 0.9622\n",
      "Epoch 292/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0091 - accuracy: 0.9993 - val_loss: 0.1530 - val_accuracy: 0.9622\n",
      "Epoch 293/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0095 - accuracy: 0.9993 - val_loss: 0.1512 - val_accuracy: 0.9622\n",
      "Epoch 294/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.9600\n",
      "Epoch 295/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.0089 - accuracy: 0.9993 - val_loss: 0.1521 - val_accuracy: 0.9622\n",
      "Epoch 296/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0088 - accuracy: 0.9993 - val_loss: 0.1508 - val_accuracy: 0.9622\n",
      "Epoch 297/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0087 - accuracy: 0.9993 - val_loss: 0.1499 - val_accuracy: 0.9622\n",
      "Epoch 298/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.1551 - val_accuracy: 0.9622\n",
      "Epoch 299/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.1500 - val_accuracy: 0.9600\n",
      "Epoch 300/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1505 - val_accuracy: 0.9622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a37c651d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "\n",
    "inputs = Input(shape=(n_features,))\n",
    "h_layer = Dense(n_hidden, activation=activation)(inputs)\n",
    "y_layer = Dense(n_classes, activation='softmax')(h_layer)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=y_layer)\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(\n",
    "    X_tr, to_categorical(Y_tr, num_classes=n_classes), \n",
    "    epochs=300, verbose=1, batch_size=32,\n",
    "    validation_data=(X_val, to_categorical(Y_val, num_classes=n_classes))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that you know if the model is underfitting or overfitting:\n",
    "<span style=\"color:green\">\n",
    "Underfitting case: model not complex enough (not enough parameters and depth) to learn dependencies between $X$ and $Y$<br/>\n",
    "Overfitting case: model too complex, learns statistically pointless dependencies between $X_{tr}$ and $Y_{tr}$\n",
    "</span>\n",
    "\n",
    "#### - In case of underfitting, can you explain why ? Also change the structure of the 2 previous networks to cancell underfitting\n",
    "#### - In case of overfitting, explain why and change the structure of the 2 previous networks to cancell the overfitting\n",
    "<span style=\"color:green\">\n",
    "We can observe a slight case of overfitting here, with a gap of about 5% between the training and the validation accuracies.<br/>Adding a Dropout with a small $p$ rate can be thus considered.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential without overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with relu activation\n",
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/300\n",
      "1347/1347 [==============================] - 1s 397us/sample - loss: 2.4092 - accuracy: 0.1062 - val_loss: 2.3101 - val_accuracy: 0.1311\n",
      "Epoch 2/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 2.2243 - accuracy: 0.2042 - val_loss: 2.1640 - val_accuracy: 0.2844\n",
      "Epoch 3/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 2.0774 - accuracy: 0.3385 - val_loss: 2.0213 - val_accuracy: 0.4244\n",
      "Epoch 4/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.9483 - accuracy: 0.4143 - val_loss: 1.8795 - val_accuracy: 0.4933\n",
      "Epoch 5/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.7945 - accuracy: 0.4714 - val_loss: 1.7110 - val_accuracy: 0.5244\n",
      "Epoch 6/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.6434 - accuracy: 0.5286 - val_loss: 1.5586 - val_accuracy: 0.6356\n",
      "Epoch 7/300\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 1.5280 - accuracy: 0.5731 - val_loss: 1.4241 - val_accuracy: 0.6800\n",
      "Epoch 8/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.4019 - accuracy: 0.6050 - val_loss: 1.3123 - val_accuracy: 0.7044\n",
      "Epoch 9/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 1.3142 - accuracy: 0.6251 - val_loss: 1.2179 - val_accuracy: 0.7089\n",
      "Epoch 10/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.2518 - accuracy: 0.6392 - val_loss: 1.1378 - val_accuracy: 0.7400\n",
      "Epoch 11/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.1943 - accuracy: 0.6451 - val_loss: 1.0649 - val_accuracy: 0.7489\n",
      "Epoch 12/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.1348 - accuracy: 0.6659 - val_loss: 1.0035 - val_accuracy: 0.7600\n",
      "Epoch 13/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.0974 - accuracy: 0.6659 - val_loss: 0.9550 - val_accuracy: 0.7667\n",
      "Epoch 14/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.0320 - accuracy: 0.6733 - val_loss: 0.9008 - val_accuracy: 0.7844\n",
      "Epoch 15/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.9830 - accuracy: 0.6993 - val_loss: 0.8583 - val_accuracy: 0.7933\n",
      "Epoch 16/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.9557 - accuracy: 0.6971 - val_loss: 0.8196 - val_accuracy: 0.7978\n",
      "Epoch 17/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.9106 - accuracy: 0.7134 - val_loss: 0.7821 - val_accuracy: 0.8111\n",
      "Epoch 18/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.9077 - accuracy: 0.7120 - val_loss: 0.7472 - val_accuracy: 0.8267\n",
      "Epoch 19/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.8541 - accuracy: 0.7238 - val_loss: 0.7084 - val_accuracy: 0.8311\n",
      "Epoch 20/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.8438 - accuracy: 0.7313 - val_loss: 0.6746 - val_accuracy: 0.8400\n",
      "Epoch 21/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.7909 - accuracy: 0.7550 - val_loss: 0.6474 - val_accuracy: 0.8444\n",
      "Epoch 22/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.7605 - accuracy: 0.7610 - val_loss: 0.6129 - val_accuracy: 0.8667\n",
      "Epoch 23/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.7412 - accuracy: 0.7647 - val_loss: 0.5853 - val_accuracy: 0.8711\n",
      "Epoch 24/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.7494 - accuracy: 0.7617 - val_loss: 0.5660 - val_accuracy: 0.8822\n",
      "Epoch 25/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.6790 - accuracy: 0.7758 - val_loss: 0.5439 - val_accuracy: 0.8822\n",
      "Epoch 26/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.6957 - accuracy: 0.7884 - val_loss: 0.5259 - val_accuracy: 0.8844\n",
      "Epoch 27/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.6415 - accuracy: 0.8040 - val_loss: 0.5077 - val_accuracy: 0.8911\n",
      "Epoch 28/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.6529 - accuracy: 0.7936 - val_loss: 0.4966 - val_accuracy: 0.8889\n",
      "Epoch 29/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.6278 - accuracy: 0.7944 - val_loss: 0.4748 - val_accuracy: 0.9000\n",
      "Epoch 30/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.6116 - accuracy: 0.7921 - val_loss: 0.4571 - val_accuracy: 0.9022\n",
      "Epoch 31/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.5855 - accuracy: 0.8129 - val_loss: 0.4455 - val_accuracy: 0.8978\n",
      "Epoch 32/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.5817 - accuracy: 0.8077 - val_loss: 0.4313 - val_accuracy: 0.9067\n",
      "Epoch 33/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.5936 - accuracy: 0.8085 - val_loss: 0.4192 - val_accuracy: 0.9044\n",
      "Epoch 34/300\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 0.5321 - accuracy: 0.8307 - val_loss: 0.4107 - val_accuracy: 0.9044\n",
      "Epoch 35/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.5620 - accuracy: 0.8137 - val_loss: 0.4031 - val_accuracy: 0.9111\n",
      "Epoch 36/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.5585 - accuracy: 0.8241 - val_loss: 0.3956 - val_accuracy: 0.9133\n",
      "Epoch 37/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.5437 - accuracy: 0.8241 - val_loss: 0.3839 - val_accuracy: 0.9133\n",
      "Epoch 38/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.5010 - accuracy: 0.8367 - val_loss: 0.3723 - val_accuracy: 0.9156\n",
      "Epoch 39/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.5040 - accuracy: 0.8411 - val_loss: 0.3655 - val_accuracy: 0.9222\n",
      "Epoch 40/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.5284 - accuracy: 0.8330 - val_loss: 0.3585 - val_accuracy: 0.9178\n",
      "Epoch 41/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.4899 - accuracy: 0.8382 - val_loss: 0.3524 - val_accuracy: 0.9267\n",
      "Epoch 42/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.4771 - accuracy: 0.8441 - val_loss: 0.3418 - val_accuracy: 0.9267\n",
      "Epoch 43/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.4948 - accuracy: 0.8426 - val_loss: 0.3349 - val_accuracy: 0.9222\n",
      "Epoch 44/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.4799 - accuracy: 0.8396 - val_loss: 0.3327 - val_accuracy: 0.9244\n",
      "Epoch 45/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.4668 - accuracy: 0.8552 - val_loss: 0.3289 - val_accuracy: 0.9244\n",
      "Epoch 46/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.4716 - accuracy: 0.8463 - val_loss: 0.3205 - val_accuracy: 0.9311\n",
      "Epoch 47/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.4748 - accuracy: 0.8471 - val_loss: 0.3152 - val_accuracy: 0.9333\n",
      "Epoch 48/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.4578 - accuracy: 0.8434 - val_loss: 0.3095 - val_accuracy: 0.9333\n",
      "Epoch 49/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.4624 - accuracy: 0.8441 - val_loss: 0.3093 - val_accuracy: 0.9356\n",
      "Epoch 50/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.4487 - accuracy: 0.8575 - val_loss: 0.3032 - val_accuracy: 0.9311\n",
      "Epoch 51/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.4404 - accuracy: 0.8560 - val_loss: 0.2987 - val_accuracy: 0.9311\n",
      "Epoch 52/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.4329 - accuracy: 0.8597 - val_loss: 0.2934 - val_accuracy: 0.9311\n",
      "Epoch 53/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.4038 - accuracy: 0.8745 - val_loss: 0.2894 - val_accuracy: 0.9356\n",
      "Epoch 54/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.4165 - accuracy: 0.8686 - val_loss: 0.2787 - val_accuracy: 0.9356\n",
      "Epoch 55/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.4366 - accuracy: 0.8582 - val_loss: 0.2780 - val_accuracy: 0.9333\n",
      "Epoch 56/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.4180 - accuracy: 0.8656 - val_loss: 0.2782 - val_accuracy: 0.9311\n",
      "Epoch 57/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.4139 - accuracy: 0.8597 - val_loss: 0.2729 - val_accuracy: 0.9356\n",
      "Epoch 58/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.4176 - accuracy: 0.8656 - val_loss: 0.2699 - val_accuracy: 0.9378\n",
      "Epoch 59/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.3979 - accuracy: 0.8797 - val_loss: 0.2679 - val_accuracy: 0.9356\n",
      "Epoch 60/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.4201 - accuracy: 0.8612 - val_loss: 0.2657 - val_accuracy: 0.9378\n",
      "Epoch 61/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.4265 - accuracy: 0.8619 - val_loss: 0.2595 - val_accuracy: 0.9422\n",
      "Epoch 62/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3605 - accuracy: 0.8894 - val_loss: 0.2570 - val_accuracy: 0.9378\n",
      "Epoch 63/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3867 - accuracy: 0.8634 - val_loss: 0.2531 - val_accuracy: 0.9356\n",
      "Epoch 64/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.3806 - accuracy: 0.8834 - val_loss: 0.2512 - val_accuracy: 0.9400\n",
      "Epoch 65/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.3970 - accuracy: 0.8619 - val_loss: 0.2508 - val_accuracy: 0.9422\n",
      "Epoch 66/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3648 - accuracy: 0.8961 - val_loss: 0.2512 - val_accuracy: 0.9422\n",
      "Epoch 67/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.3751 - accuracy: 0.8827 - val_loss: 0.2453 - val_accuracy: 0.9400\n",
      "Epoch 68/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.3661 - accuracy: 0.8842 - val_loss: 0.2458 - val_accuracy: 0.9356\n",
      "Epoch 69/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3757 - accuracy: 0.8731 - val_loss: 0.2409 - val_accuracy: 0.9444\n",
      "Epoch 70/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.3690 - accuracy: 0.8864 - val_loss: 0.2407 - val_accuracy: 0.9356\n",
      "Epoch 71/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3909 - accuracy: 0.8656 - val_loss: 0.2383 - val_accuracy: 0.9378\n",
      "Epoch 72/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.3585 - accuracy: 0.8820 - val_loss: 0.2358 - val_accuracy: 0.9400\n",
      "Epoch 73/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3824 - accuracy: 0.8701 - val_loss: 0.2364 - val_accuracy: 0.9422\n",
      "Epoch 74/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3624 - accuracy: 0.8753 - val_loss: 0.2327 - val_accuracy: 0.9422\n",
      "Epoch 75/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.3711 - accuracy: 0.8679 - val_loss: 0.2324 - val_accuracy: 0.9467\n",
      "Epoch 76/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3634 - accuracy: 0.8834 - val_loss: 0.2317 - val_accuracy: 0.9400\n",
      "Epoch 77/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3435 - accuracy: 0.8820 - val_loss: 0.2300 - val_accuracy: 0.9422\n",
      "Epoch 78/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.3737 - accuracy: 0.8745 - val_loss: 0.2272 - val_accuracy: 0.9400\n",
      "Epoch 79/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3517 - accuracy: 0.8805 - val_loss: 0.2272 - val_accuracy: 0.9422\n",
      "Epoch 80/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.3984 - accuracy: 0.8634 - val_loss: 0.2278 - val_accuracy: 0.9378\n",
      "Epoch 81/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.3427 - accuracy: 0.8938 - val_loss: 0.2241 - val_accuracy: 0.9400\n",
      "Epoch 82/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3182 - accuracy: 0.8983 - val_loss: 0.2216 - val_accuracy: 0.9444\n",
      "Epoch 83/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3433 - accuracy: 0.8812 - val_loss: 0.2215 - val_accuracy: 0.9400\n",
      "Epoch 84/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3274 - accuracy: 0.8909 - val_loss: 0.2179 - val_accuracy: 0.9422\n",
      "Epoch 85/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.3175 - accuracy: 0.8879 - val_loss: 0.2167 - val_accuracy: 0.9422\n",
      "Epoch 86/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.3386 - accuracy: 0.8924 - val_loss: 0.2158 - val_accuracy: 0.9422\n",
      "Epoch 87/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.3439 - accuracy: 0.8842 - val_loss: 0.2174 - val_accuracy: 0.9422\n",
      "Epoch 88/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.3325 - accuracy: 0.8842 - val_loss: 0.2129 - val_accuracy: 0.9467\n",
      "Epoch 89/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3144 - accuracy: 0.8901 - val_loss: 0.2117 - val_accuracy: 0.9422\n",
      "Epoch 90/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3178 - accuracy: 0.8886 - val_loss: 0.2120 - val_accuracy: 0.9422\n",
      "Epoch 91/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.3182 - accuracy: 0.8968 - val_loss: 0.2101 - val_accuracy: 0.9489\n",
      "Epoch 92/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3464 - accuracy: 0.8768 - val_loss: 0.2084 - val_accuracy: 0.9467\n",
      "Epoch 93/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3314 - accuracy: 0.8834 - val_loss: 0.2097 - val_accuracy: 0.9444\n",
      "Epoch 94/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3134 - accuracy: 0.8998 - val_loss: 0.2085 - val_accuracy: 0.9467\n",
      "Epoch 95/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3187 - accuracy: 0.8916 - val_loss: 0.2072 - val_accuracy: 0.9489\n",
      "Epoch 96/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3185 - accuracy: 0.8909 - val_loss: 0.2037 - val_accuracy: 0.9444\n",
      "Epoch 97/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.3244 - accuracy: 0.8842 - val_loss: 0.2052 - val_accuracy: 0.9400\n",
      "Epoch 98/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2914 - accuracy: 0.8983 - val_loss: 0.2064 - val_accuracy: 0.9467\n",
      "Epoch 99/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.3559 - accuracy: 0.8693 - val_loss: 0.2089 - val_accuracy: 0.9422\n",
      "Epoch 100/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2975 - accuracy: 0.9035 - val_loss: 0.2093 - val_accuracy: 0.9378\n",
      "Epoch 101/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.3263 - accuracy: 0.8857 - val_loss: 0.2077 - val_accuracy: 0.9444\n",
      "Epoch 102/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.3223 - accuracy: 0.8834 - val_loss: 0.2064 - val_accuracy: 0.9422\n",
      "Epoch 103/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3301 - accuracy: 0.8842 - val_loss: 0.2076 - val_accuracy: 0.9400\n",
      "Epoch 104/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.3164 - accuracy: 0.8909 - val_loss: 0.2041 - val_accuracy: 0.9444\n",
      "Epoch 105/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3112 - accuracy: 0.8879 - val_loss: 0.2007 - val_accuracy: 0.9444\n",
      "Epoch 106/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.3121 - accuracy: 0.8909 - val_loss: 0.2011 - val_accuracy: 0.9422\n",
      "Epoch 107/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2747 - accuracy: 0.9102 - val_loss: 0.2004 - val_accuracy: 0.9444\n",
      "Epoch 108/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2842 - accuracy: 0.9050 - val_loss: 0.2018 - val_accuracy: 0.9489\n",
      "Epoch 109/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2663 - accuracy: 0.9057 - val_loss: 0.1989 - val_accuracy: 0.9489\n",
      "Epoch 110/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.3028 - accuracy: 0.8909 - val_loss: 0.1986 - val_accuracy: 0.9467\n",
      "Epoch 111/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2900 - accuracy: 0.9035 - val_loss: 0.1995 - val_accuracy: 0.9444\n",
      "Epoch 112/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2908 - accuracy: 0.9020 - val_loss: 0.1983 - val_accuracy: 0.9422\n",
      "Epoch 113/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2953 - accuracy: 0.8998 - val_loss: 0.1970 - val_accuracy: 0.9444\n",
      "Epoch 114/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2859 - accuracy: 0.9065 - val_loss: 0.1967 - val_accuracy: 0.9444\n",
      "Epoch 115/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2930 - accuracy: 0.9013 - val_loss: 0.1981 - val_accuracy: 0.9511\n",
      "Epoch 116/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2841 - accuracy: 0.8990 - val_loss: 0.1965 - val_accuracy: 0.9444\n",
      "Epoch 117/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2847 - accuracy: 0.8953 - val_loss: 0.1968 - val_accuracy: 0.9444\n",
      "Epoch 118/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2793 - accuracy: 0.9109 - val_loss: 0.1963 - val_accuracy: 0.9422\n",
      "Epoch 119/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2924 - accuracy: 0.8909 - val_loss: 0.1958 - val_accuracy: 0.9467\n",
      "Epoch 120/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2839 - accuracy: 0.9072 - val_loss: 0.1919 - val_accuracy: 0.9467\n",
      "Epoch 121/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3064 - accuracy: 0.8879 - val_loss: 0.1939 - val_accuracy: 0.9400\n",
      "Epoch 122/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2843 - accuracy: 0.9027 - val_loss: 0.1948 - val_accuracy: 0.9444\n",
      "Epoch 123/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2992 - accuracy: 0.8998 - val_loss: 0.1943 - val_accuracy: 0.9444\n",
      "Epoch 124/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2957 - accuracy: 0.8953 - val_loss: 0.1948 - val_accuracy: 0.9444\n",
      "Epoch 125/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2881 - accuracy: 0.8961 - val_loss: 0.1934 - val_accuracy: 0.9400\n",
      "Epoch 126/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3032 - accuracy: 0.8976 - val_loss: 0.1913 - val_accuracy: 0.9444\n",
      "Epoch 127/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2815 - accuracy: 0.9035 - val_loss: 0.1884 - val_accuracy: 0.9422\n",
      "Epoch 128/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2679 - accuracy: 0.9072 - val_loss: 0.1886 - val_accuracy: 0.9467\n",
      "Epoch 129/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2720 - accuracy: 0.9050 - val_loss: 0.1888 - val_accuracy: 0.9444\n",
      "Epoch 130/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2767 - accuracy: 0.9042 - val_loss: 0.1928 - val_accuracy: 0.9489\n",
      "Epoch 131/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2718 - accuracy: 0.8998 - val_loss: 0.1926 - val_accuracy: 0.9444\n",
      "Epoch 132/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2699 - accuracy: 0.9094 - val_loss: 0.1903 - val_accuracy: 0.9422\n",
      "Epoch 133/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2819 - accuracy: 0.8916 - val_loss: 0.1922 - val_accuracy: 0.9467\n",
      "Epoch 134/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.3013 - accuracy: 0.8946 - val_loss: 0.1902 - val_accuracy: 0.9467\n",
      "Epoch 135/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2694 - accuracy: 0.9094 - val_loss: 0.1916 - val_accuracy: 0.9467\n",
      "Epoch 136/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2642 - accuracy: 0.9124 - val_loss: 0.1903 - val_accuracy: 0.9467\n",
      "Epoch 137/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2552 - accuracy: 0.9065 - val_loss: 0.1891 - val_accuracy: 0.9444\n",
      "Epoch 138/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2927 - accuracy: 0.9057 - val_loss: 0.1886 - val_accuracy: 0.9489\n",
      "Epoch 139/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2782 - accuracy: 0.8961 - val_loss: 0.1897 - val_accuracy: 0.9444\n",
      "Epoch 140/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2692 - accuracy: 0.9065 - val_loss: 0.1879 - val_accuracy: 0.9422\n",
      "Epoch 141/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2771 - accuracy: 0.9042 - val_loss: 0.1893 - val_accuracy: 0.9467\n",
      "Epoch 142/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.2817 - accuracy: 0.9050 - val_loss: 0.1898 - val_accuracy: 0.9467\n",
      "Epoch 143/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.2739 - accuracy: 0.8990 - val_loss: 0.1897 - val_accuracy: 0.9467\n",
      "Epoch 144/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2792 - accuracy: 0.9027 - val_loss: 0.1900 - val_accuracy: 0.9489\n",
      "Epoch 145/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2468 - accuracy: 0.9124 - val_loss: 0.1874 - val_accuracy: 0.9467\n",
      "Epoch 146/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2542 - accuracy: 0.9072 - val_loss: 0.1874 - val_accuracy: 0.9467\n",
      "Epoch 147/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2647 - accuracy: 0.9050 - val_loss: 0.1886 - val_accuracy: 0.9444\n",
      "Epoch 148/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.2674 - accuracy: 0.9042 - val_loss: 0.1889 - val_accuracy: 0.9444\n",
      "Epoch 149/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2706 - accuracy: 0.9079 - val_loss: 0.1869 - val_accuracy: 0.9422\n",
      "Epoch 150/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2715 - accuracy: 0.9050 - val_loss: 0.1862 - val_accuracy: 0.9467\n",
      "Epoch 151/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2708 - accuracy: 0.9079 - val_loss: 0.1877 - val_accuracy: 0.9467\n",
      "Epoch 152/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2532 - accuracy: 0.9131 - val_loss: 0.1904 - val_accuracy: 0.9489\n",
      "Epoch 153/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2599 - accuracy: 0.9161 - val_loss: 0.1898 - val_accuracy: 0.9444\n",
      "Epoch 154/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.2727 - accuracy: 0.9124 - val_loss: 0.1890 - val_accuracy: 0.9467\n",
      "Epoch 155/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2589 - accuracy: 0.9087 - val_loss: 0.1853 - val_accuracy: 0.9489\n",
      "Epoch 156/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2483 - accuracy: 0.9206 - val_loss: 0.1869 - val_accuracy: 0.9444\n",
      "Epoch 157/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2668 - accuracy: 0.9094 - val_loss: 0.1880 - val_accuracy: 0.9489\n",
      "Epoch 158/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2542 - accuracy: 0.9094 - val_loss: 0.1860 - val_accuracy: 0.9444\n",
      "Epoch 159/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2574 - accuracy: 0.9139 - val_loss: 0.1886 - val_accuracy: 0.9489\n",
      "Epoch 160/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2556 - accuracy: 0.9094 - val_loss: 0.1907 - val_accuracy: 0.9444\n",
      "Epoch 161/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2267 - accuracy: 0.9243 - val_loss: 0.1862 - val_accuracy: 0.9444\n",
      "Epoch 162/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2495 - accuracy: 0.9102 - val_loss: 0.1868 - val_accuracy: 0.9467\n",
      "Epoch 163/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2302 - accuracy: 0.9198 - val_loss: 0.1853 - val_accuracy: 0.9489\n",
      "Epoch 164/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2275 - accuracy: 0.9220 - val_loss: 0.1905 - val_accuracy: 0.9489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2625 - accuracy: 0.9057 - val_loss: 0.1910 - val_accuracy: 0.9467\n",
      "Epoch 166/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2576 - accuracy: 0.9050 - val_loss: 0.1888 - val_accuracy: 0.9467\n",
      "Epoch 167/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2484 - accuracy: 0.9117 - val_loss: 0.1867 - val_accuracy: 0.9489\n",
      "Epoch 168/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2345 - accuracy: 0.9198 - val_loss: 0.1883 - val_accuracy: 0.9467\n",
      "Epoch 169/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2479 - accuracy: 0.9146 - val_loss: 0.1911 - val_accuracy: 0.9489\n",
      "Epoch 170/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2430 - accuracy: 0.9131 - val_loss: 0.1868 - val_accuracy: 0.9444\n",
      "Epoch 171/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2467 - accuracy: 0.9161 - val_loss: 0.1843 - val_accuracy: 0.9467\n",
      "Epoch 172/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2365 - accuracy: 0.9154 - val_loss: 0.1837 - val_accuracy: 0.9489\n",
      "Epoch 173/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.2371 - accuracy: 0.9213 - val_loss: 0.1857 - val_accuracy: 0.9489\n",
      "Epoch 174/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2666 - accuracy: 0.9065 - val_loss: 0.1851 - val_accuracy: 0.9489\n",
      "Epoch 175/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2683 - accuracy: 0.9079 - val_loss: 0.1900 - val_accuracy: 0.9489\n",
      "Epoch 176/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2350 - accuracy: 0.9213 - val_loss: 0.1864 - val_accuracy: 0.9467\n",
      "Epoch 177/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2530 - accuracy: 0.9072 - val_loss: 0.1895 - val_accuracy: 0.9444\n",
      "Epoch 178/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2439 - accuracy: 0.9139 - val_loss: 0.1903 - val_accuracy: 0.9489\n",
      "Epoch 179/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2613 - accuracy: 0.9124 - val_loss: 0.1924 - val_accuracy: 0.9467\n",
      "Epoch 180/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2757 - accuracy: 0.9005 - val_loss: 0.1923 - val_accuracy: 0.9489\n",
      "Epoch 181/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2560 - accuracy: 0.9109 - val_loss: 0.1917 - val_accuracy: 0.9422\n",
      "Epoch 182/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2350 - accuracy: 0.9191 - val_loss: 0.1910 - val_accuracy: 0.9444\n",
      "Epoch 183/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2382 - accuracy: 0.9198 - val_loss: 0.1877 - val_accuracy: 0.9489\n",
      "Epoch 184/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2467 - accuracy: 0.9087 - val_loss: 0.1901 - val_accuracy: 0.9444\n",
      "Epoch 185/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2293 - accuracy: 0.9250 - val_loss: 0.1913 - val_accuracy: 0.9444\n",
      "Epoch 186/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2384 - accuracy: 0.9198 - val_loss: 0.1912 - val_accuracy: 0.9467\n",
      "Epoch 187/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2437 - accuracy: 0.9191 - val_loss: 0.1924 - val_accuracy: 0.9444\n",
      "Epoch 188/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2363 - accuracy: 0.9169 - val_loss: 0.1898 - val_accuracy: 0.9467\n",
      "Epoch 189/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2302 - accuracy: 0.9169 - val_loss: 0.1937 - val_accuracy: 0.9467\n",
      "Epoch 190/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2346 - accuracy: 0.9206 - val_loss: 0.1926 - val_accuracy: 0.9444\n",
      "Epoch 191/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2309 - accuracy: 0.9117 - val_loss: 0.1934 - val_accuracy: 0.9467\n",
      "Epoch 192/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2280 - accuracy: 0.9183 - val_loss: 0.1904 - val_accuracy: 0.9467\n",
      "Epoch 193/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2247 - accuracy: 0.9191 - val_loss: 0.1881 - val_accuracy: 0.9467\n",
      "Epoch 194/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2521 - accuracy: 0.9139 - val_loss: 0.1907 - val_accuracy: 0.9444\n",
      "Epoch 195/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2346 - accuracy: 0.9117 - val_loss: 0.1916 - val_accuracy: 0.9444\n",
      "Epoch 196/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2186 - accuracy: 0.9228 - val_loss: 0.1919 - val_accuracy: 0.9444\n",
      "Epoch 197/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2235 - accuracy: 0.9250 - val_loss: 0.1935 - val_accuracy: 0.9467\n",
      "Epoch 198/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2390 - accuracy: 0.9102 - val_loss: 0.1901 - val_accuracy: 0.9467\n",
      "Epoch 199/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2315 - accuracy: 0.9198 - val_loss: 0.1905 - val_accuracy: 0.9444\n",
      "Epoch 200/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2289 - accuracy: 0.9228 - val_loss: 0.1877 - val_accuracy: 0.9467\n",
      "Epoch 201/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2250 - accuracy: 0.9228 - val_loss: 0.1909 - val_accuracy: 0.9444\n",
      "Epoch 202/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2157 - accuracy: 0.9250 - val_loss: 0.1926 - val_accuracy: 0.9467\n",
      "Epoch 203/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2548 - accuracy: 0.9109 - val_loss: 0.1897 - val_accuracy: 0.9444\n",
      "Epoch 204/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2415 - accuracy: 0.9213 - val_loss: 0.1899 - val_accuracy: 0.9467\n",
      "Epoch 205/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2206 - accuracy: 0.9280 - val_loss: 0.1919 - val_accuracy: 0.9467\n",
      "Epoch 206/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2303 - accuracy: 0.9235 - val_loss: 0.1915 - val_accuracy: 0.9467\n",
      "Epoch 207/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2139 - accuracy: 0.9228 - val_loss: 0.1915 - val_accuracy: 0.9444\n",
      "Epoch 208/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2151 - accuracy: 0.9265 - val_loss: 0.1918 - val_accuracy: 0.9467\n",
      "Epoch 209/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2386 - accuracy: 0.9131 - val_loss: 0.1913 - val_accuracy: 0.9444\n",
      "Epoch 210/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2251 - accuracy: 0.9191 - val_loss: 0.1900 - val_accuracy: 0.9467\n",
      "Epoch 211/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2166 - accuracy: 0.9272 - val_loss: 0.1901 - val_accuracy: 0.9467\n",
      "Epoch 212/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2212 - accuracy: 0.9235 - val_loss: 0.1892 - val_accuracy: 0.9467\n",
      "Epoch 213/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2425 - accuracy: 0.9102 - val_loss: 0.1904 - val_accuracy: 0.9444\n",
      "Epoch 214/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2344 - accuracy: 0.9213 - val_loss: 0.1925 - val_accuracy: 0.9467\n",
      "Epoch 215/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2113 - accuracy: 0.9339 - val_loss: 0.1917 - val_accuracy: 0.9467\n",
      "Epoch 216/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2349 - accuracy: 0.9206 - val_loss: 0.1938 - val_accuracy: 0.9467\n",
      "Epoch 217/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2297 - accuracy: 0.9183 - val_loss: 0.1942 - val_accuracy: 0.9467\n",
      "Epoch 218/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2252 - accuracy: 0.9176 - val_loss: 0.1918 - val_accuracy: 0.9467\n",
      "Epoch 219/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.2093 - accuracy: 0.9317 - val_loss: 0.1929 - val_accuracy: 0.9489\n",
      "Epoch 220/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1969 - accuracy: 0.9324 - val_loss: 0.1978 - val_accuracy: 0.9400\n",
      "Epoch 221/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2050 - accuracy: 0.9272 - val_loss: 0.1950 - val_accuracy: 0.9467\n",
      "Epoch 222/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2163 - accuracy: 0.9287 - val_loss: 0.1962 - val_accuracy: 0.9400\n",
      "Epoch 223/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2355 - accuracy: 0.9109 - val_loss: 0.1921 - val_accuracy: 0.9467\n",
      "Epoch 224/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2276 - accuracy: 0.9206 - val_loss: 0.1945 - val_accuracy: 0.9467\n",
      "Epoch 225/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2499 - accuracy: 0.9169 - val_loss: 0.1942 - val_accuracy: 0.9467\n",
      "Epoch 226/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2094 - accuracy: 0.9376 - val_loss: 0.1962 - val_accuracy: 0.9467\n",
      "Epoch 227/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2105 - accuracy: 0.9265 - val_loss: 0.1935 - val_accuracy: 0.9467\n",
      "Epoch 228/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2068 - accuracy: 0.9339 - val_loss: 0.1978 - val_accuracy: 0.9422\n",
      "Epoch 229/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2369 - accuracy: 0.9161 - val_loss: 0.1924 - val_accuracy: 0.9467\n",
      "Epoch 230/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2386 - accuracy: 0.9272 - val_loss: 0.1910 - val_accuracy: 0.9489\n",
      "Epoch 231/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2056 - accuracy: 0.9235 - val_loss: 0.1911 - val_accuracy: 0.9467\n",
      "Epoch 232/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2097 - accuracy: 0.9287 - val_loss: 0.1913 - val_accuracy: 0.9444\n",
      "Epoch 233/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2224 - accuracy: 0.9206 - val_loss: 0.1943 - val_accuracy: 0.9444\n",
      "Epoch 234/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2068 - accuracy: 0.9250 - val_loss: 0.1942 - val_accuracy: 0.9467\n",
      "Epoch 235/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.2476 - accuracy: 0.9131 - val_loss: 0.1926 - val_accuracy: 0.9467\n",
      "Epoch 236/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2265 - accuracy: 0.9206 - val_loss: 0.1948 - val_accuracy: 0.9467\n",
      "Epoch 237/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.2143 - accuracy: 0.9280 - val_loss: 0.1974 - val_accuracy: 0.9467\n",
      "Epoch 238/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2180 - accuracy: 0.9206 - val_loss: 0.1978 - val_accuracy: 0.9444\n",
      "Epoch 239/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.2180 - accuracy: 0.9272 - val_loss: 0.1962 - val_accuracy: 0.9467\n",
      "Epoch 240/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2396 - accuracy: 0.9169 - val_loss: 0.1947 - val_accuracy: 0.9467\n",
      "Epoch 241/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2402 - accuracy: 0.9206 - val_loss: 0.1936 - val_accuracy: 0.9467\n",
      "Epoch 242/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2027 - accuracy: 0.9295 - val_loss: 0.1935 - val_accuracy: 0.9444\n",
      "Epoch 243/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2177 - accuracy: 0.9287 - val_loss: 0.1904 - val_accuracy: 0.9489\n",
      "Epoch 244/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2039 - accuracy: 0.9280 - val_loss: 0.1967 - val_accuracy: 0.9444\n",
      "Epoch 245/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2219 - accuracy: 0.9206 - val_loss: 0.1963 - val_accuracy: 0.9467\n",
      "Epoch 246/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.2075 - accuracy: 0.9302 - val_loss: 0.1907 - val_accuracy: 0.9444\n",
      "Epoch 247/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1946 - accuracy: 0.9324 - val_loss: 0.1943 - val_accuracy: 0.9378\n",
      "Epoch 248/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.2217 - accuracy: 0.9220 - val_loss: 0.1971 - val_accuracy: 0.9422\n",
      "Epoch 249/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2272 - accuracy: 0.9146 - val_loss: 0.1922 - val_accuracy: 0.9467\n",
      "Epoch 250/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2050 - accuracy: 0.9280 - val_loss: 0.1942 - val_accuracy: 0.9467\n",
      "Epoch 251/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1975 - accuracy: 0.9310 - val_loss: 0.1950 - val_accuracy: 0.9422\n",
      "Epoch 252/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2257 - accuracy: 0.9161 - val_loss: 0.1947 - val_accuracy: 0.9444\n",
      "Epoch 253/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2245 - accuracy: 0.9213 - val_loss: 0.1969 - val_accuracy: 0.9422\n",
      "Epoch 254/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2121 - accuracy: 0.9265 - val_loss: 0.1964 - val_accuracy: 0.9444\n",
      "Epoch 255/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2124 - accuracy: 0.9280 - val_loss: 0.1963 - val_accuracy: 0.9422\n",
      "Epoch 256/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2044 - accuracy: 0.9362 - val_loss: 0.1942 - val_accuracy: 0.9444\n",
      "Epoch 257/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2040 - accuracy: 0.9272 - val_loss: 0.1966 - val_accuracy: 0.9444\n",
      "Epoch 258/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2111 - accuracy: 0.9228 - val_loss: 0.1936 - val_accuracy: 0.9422\n",
      "Epoch 259/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1997 - accuracy: 0.9287 - val_loss: 0.1889 - val_accuracy: 0.9467\n",
      "Epoch 260/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2222 - accuracy: 0.9272 - val_loss: 0.1921 - val_accuracy: 0.9467\n",
      "Epoch 261/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1997 - accuracy: 0.9376 - val_loss: 0.1929 - val_accuracy: 0.9422\n",
      "Epoch 262/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2138 - accuracy: 0.9272 - val_loss: 0.1898 - val_accuracy: 0.9489\n",
      "Epoch 263/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1926 - accuracy: 0.9384 - val_loss: 0.1952 - val_accuracy: 0.9444\n",
      "Epoch 264/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2297 - accuracy: 0.9117 - val_loss: 0.1972 - val_accuracy: 0.9444\n",
      "Epoch 265/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1975 - accuracy: 0.9332 - val_loss: 0.1983 - val_accuracy: 0.9422\n",
      "Epoch 266/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2059 - accuracy: 0.9354 - val_loss: 0.1947 - val_accuracy: 0.9467\n",
      "Epoch 267/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2037 - accuracy: 0.9384 - val_loss: 0.1917 - val_accuracy: 0.9489\n",
      "Epoch 268/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2031 - accuracy: 0.9310 - val_loss: 0.1919 - val_accuracy: 0.9467\n",
      "Epoch 269/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2193 - accuracy: 0.9139 - val_loss: 0.1934 - val_accuracy: 0.9444\n",
      "Epoch 270/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2102 - accuracy: 0.9228 - val_loss: 0.1909 - val_accuracy: 0.9467\n",
      "Epoch 271/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2092 - accuracy: 0.9332 - val_loss: 0.1948 - val_accuracy: 0.9489\n",
      "Epoch 272/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2257 - accuracy: 0.9228 - val_loss: 0.1994 - val_accuracy: 0.9467\n",
      "Epoch 273/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2062 - accuracy: 0.9228 - val_loss: 0.1945 - val_accuracy: 0.9489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2017 - accuracy: 0.9369 - val_loss: 0.1943 - val_accuracy: 0.9467\n",
      "Epoch 275/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1919 - accuracy: 0.9406 - val_loss: 0.1997 - val_accuracy: 0.9444\n",
      "Epoch 276/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2110 - accuracy: 0.9310 - val_loss: 0.1958 - val_accuracy: 0.9467\n",
      "Epoch 277/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2163 - accuracy: 0.9280 - val_loss: 0.1997 - val_accuracy: 0.9378\n",
      "Epoch 278/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2092 - accuracy: 0.9295 - val_loss: 0.1974 - val_accuracy: 0.9467\n",
      "Epoch 279/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1927 - accuracy: 0.9362 - val_loss: 0.1979 - val_accuracy: 0.9422\n",
      "Epoch 280/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2073 - accuracy: 0.9272 - val_loss: 0.1945 - val_accuracy: 0.9444\n",
      "Epoch 281/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2153 - accuracy: 0.9272 - val_loss: 0.1923 - val_accuracy: 0.9467\n",
      "Epoch 282/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1990 - accuracy: 0.9295 - val_loss: 0.1923 - val_accuracy: 0.9444\n",
      "Epoch 283/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2110 - accuracy: 0.9280 - val_loss: 0.1977 - val_accuracy: 0.9378\n",
      "Epoch 284/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2119 - accuracy: 0.9206 - val_loss: 0.1957 - val_accuracy: 0.9444\n",
      "Epoch 285/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2320 - accuracy: 0.9079 - val_loss: 0.1988 - val_accuracy: 0.9467\n",
      "Epoch 286/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2071 - accuracy: 0.9339 - val_loss: 0.1986 - val_accuracy: 0.9422\n",
      "Epoch 287/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2031 - accuracy: 0.9354 - val_loss: 0.1935 - val_accuracy: 0.9467\n",
      "Epoch 288/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2299 - accuracy: 0.9198 - val_loss: 0.1952 - val_accuracy: 0.9467\n",
      "Epoch 289/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2333 - accuracy: 0.9117 - val_loss: 0.1970 - val_accuracy: 0.9467\n",
      "Epoch 290/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1948 - accuracy: 0.9310 - val_loss: 0.2003 - val_accuracy: 0.9444\n",
      "Epoch 291/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2070 - accuracy: 0.9295 - val_loss: 0.1967 - val_accuracy: 0.9467\n",
      "Epoch 292/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2034 - accuracy: 0.9302 - val_loss: 0.1961 - val_accuracy: 0.9467\n",
      "Epoch 293/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1955 - accuracy: 0.9324 - val_loss: 0.2029 - val_accuracy: 0.9422\n",
      "Epoch 294/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1787 - accuracy: 0.9406 - val_loss: 0.2026 - val_accuracy: 0.9422\n",
      "Epoch 295/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2182 - accuracy: 0.9176 - val_loss: 0.2026 - val_accuracy: 0.9422\n",
      "Epoch 296/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2113 - accuracy: 0.9354 - val_loss: 0.2014 - val_accuracy: 0.9422\n",
      "Epoch 297/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1697 - accuracy: 0.9436 - val_loss: 0.2018 - val_accuracy: 0.9422\n",
      "Epoch 298/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2027 - accuracy: 0.9295 - val_loss: 0.1992 - val_accuracy: 0.9467\n",
      "Epoch 299/300\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.2094 - accuracy: 0.9310 - val_loss: 0.2033 - val_accuracy: 0.9422\n",
      "Epoch 300/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1883 - accuracy: 0.9317 - val_loss: 0.2028 - val_accuracy: 0.9467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a383c9650>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(n_hidden, input_dim=n_features, activation=activation))\n",
    "keras_model.add(Dropout(0.1))\n",
    "keras_model.add(Dense(n_classes, activation='softmax'))\n",
    "keras_model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "keras_model.fit(\n",
    "    X_tr, to_categorical(Y_tr, num_classes=n_classes),\n",
    "    epochs=300, batch_size=32, verbose=1,\n",
    "    validation_data=(X_val, to_categorical(Y_val, num_classes=n_classes))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional without overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with relu activation\n",
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/300\n",
      "1347/1347 [==============================] - 0s 313us/sample - loss: 2.4111 - accuracy: 0.1143 - val_loss: 2.2598 - val_accuracy: 0.1444\n",
      "Epoch 2/300\n",
      "1347/1347 [==============================] - 0s 53us/sample - loss: 2.2041 - accuracy: 0.1811 - val_loss: 2.1270 - val_accuracy: 0.2911\n",
      "Epoch 3/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 2.0762 - accuracy: 0.3133 - val_loss: 2.0054 - val_accuracy: 0.3822\n",
      "Epoch 4/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.9528 - accuracy: 0.3898 - val_loss: 1.8733 - val_accuracy: 0.4356\n",
      "Epoch 5/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 1.8270 - accuracy: 0.4321 - val_loss: 1.7341 - val_accuracy: 0.5022\n",
      "Epoch 6/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.6912 - accuracy: 0.4892 - val_loss: 1.5871 - val_accuracy: 0.5867\n",
      "Epoch 7/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 1.5525 - accuracy: 0.5531 - val_loss: 1.4435 - val_accuracy: 0.6489\n",
      "Epoch 8/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 1.4200 - accuracy: 0.6117 - val_loss: 1.2958 - val_accuracy: 0.7533\n",
      "Epoch 9/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.2905 - accuracy: 0.6667 - val_loss: 1.1680 - val_accuracy: 0.7778\n",
      "Epoch 10/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 1.1730 - accuracy: 0.6986 - val_loss: 1.0511 - val_accuracy: 0.7911\n",
      "Epoch 11/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 1.1023 - accuracy: 0.6964 - val_loss: 0.9594 - val_accuracy: 0.8067\n",
      "Epoch 12/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 1.0116 - accuracy: 0.7246 - val_loss: 0.8787 - val_accuracy: 0.8222\n",
      "Epoch 13/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.9352 - accuracy: 0.7506 - val_loss: 0.8090 - val_accuracy: 0.8311\n",
      "Epoch 14/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.8697 - accuracy: 0.7661 - val_loss: 0.7473 - val_accuracy: 0.8333\n",
      "Epoch 15/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.8407 - accuracy: 0.7684 - val_loss: 0.6963 - val_accuracy: 0.8444\n",
      "Epoch 16/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.7948 - accuracy: 0.7773 - val_loss: 0.6499 - val_accuracy: 0.8578\n",
      "Epoch 17/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.7571 - accuracy: 0.7832 - val_loss: 0.6092 - val_accuracy: 0.8667\n",
      "Epoch 18/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.7251 - accuracy: 0.7840 - val_loss: 0.5760 - val_accuracy: 0.8733\n",
      "Epoch 19/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.6719 - accuracy: 0.8174 - val_loss: 0.5417 - val_accuracy: 0.8822\n",
      "Epoch 20/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.6596 - accuracy: 0.8040 - val_loss: 0.5115 - val_accuracy: 0.8956\n",
      "Epoch 21/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.6255 - accuracy: 0.8278 - val_loss: 0.4854 - val_accuracy: 0.9089\n",
      "Epoch 22/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.6020 - accuracy: 0.8307 - val_loss: 0.4594 - val_accuracy: 0.9178\n",
      "Epoch 23/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.5915 - accuracy: 0.8226 - val_loss: 0.4384 - val_accuracy: 0.9222\n",
      "Epoch 24/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.5812 - accuracy: 0.8174 - val_loss: 0.4224 - val_accuracy: 0.9311\n",
      "Epoch 25/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.5422 - accuracy: 0.8330 - val_loss: 0.4041 - val_accuracy: 0.9289\n",
      "Epoch 26/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.5253 - accuracy: 0.8486 - val_loss: 0.3856 - val_accuracy: 0.9333\n",
      "Epoch 27/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.5217 - accuracy: 0.8448 - val_loss: 0.3717 - val_accuracy: 0.9356\n",
      "Epoch 28/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.4836 - accuracy: 0.8582 - val_loss: 0.3576 - val_accuracy: 0.9356\n",
      "Epoch 29/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.4933 - accuracy: 0.8486 - val_loss: 0.3458 - val_accuracy: 0.9378\n",
      "Epoch 30/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.4758 - accuracy: 0.8597 - val_loss: 0.3343 - val_accuracy: 0.9400\n",
      "Epoch 31/300\n",
      "1347/1347 [==============================] - 0s 54us/sample - loss: 0.4614 - accuracy: 0.8604 - val_loss: 0.3244 - val_accuracy: 0.9378\n",
      "Epoch 32/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.4470 - accuracy: 0.8634 - val_loss: 0.3146 - val_accuracy: 0.9400\n",
      "Epoch 33/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.4408 - accuracy: 0.8686 - val_loss: 0.3051 - val_accuracy: 0.9378\n",
      "Epoch 34/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.4146 - accuracy: 0.8886 - val_loss: 0.2946 - val_accuracy: 0.9422\n",
      "Epoch 35/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.4171 - accuracy: 0.8731 - val_loss: 0.2854 - val_accuracy: 0.9400\n",
      "Epoch 36/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.4159 - accuracy: 0.8693 - val_loss: 0.2805 - val_accuracy: 0.9400\n",
      "Epoch 37/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.4087 - accuracy: 0.8805 - val_loss: 0.2755 - val_accuracy: 0.9422\n",
      "Epoch 38/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.4026 - accuracy: 0.8768 - val_loss: 0.2689 - val_accuracy: 0.9444\n",
      "Epoch 39/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3828 - accuracy: 0.8805 - val_loss: 0.2612 - val_accuracy: 0.9444\n",
      "Epoch 40/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.4011 - accuracy: 0.8768 - val_loss: 0.2590 - val_accuracy: 0.9422\n",
      "Epoch 41/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3518 - accuracy: 0.8961 - val_loss: 0.2496 - val_accuracy: 0.9422\n",
      "Epoch 42/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.3737 - accuracy: 0.8879 - val_loss: 0.2494 - val_accuracy: 0.9422\n",
      "Epoch 43/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3682 - accuracy: 0.8924 - val_loss: 0.2423 - val_accuracy: 0.9444\n",
      "Epoch 44/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3809 - accuracy: 0.8834 - val_loss: 0.2392 - val_accuracy: 0.9444\n",
      "Epoch 45/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.3488 - accuracy: 0.8946 - val_loss: 0.2356 - val_accuracy: 0.9444\n",
      "Epoch 46/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3544 - accuracy: 0.8946 - val_loss: 0.2312 - val_accuracy: 0.9489\n",
      "Epoch 47/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.3606 - accuracy: 0.8864 - val_loss: 0.2301 - val_accuracy: 0.9444\n",
      "Epoch 48/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3496 - accuracy: 0.8983 - val_loss: 0.2262 - val_accuracy: 0.9489\n",
      "Epoch 49/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3262 - accuracy: 0.8998 - val_loss: 0.2202 - val_accuracy: 0.9467\n",
      "Epoch 50/300\n",
      "1347/1347 [==============================] - 0s 59us/sample - loss: 0.3505 - accuracy: 0.8924 - val_loss: 0.2169 - val_accuracy: 0.9467\n",
      "Epoch 51/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3103 - accuracy: 0.9065 - val_loss: 0.2127 - val_accuracy: 0.9489\n",
      "Epoch 52/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.3026 - accuracy: 0.9102 - val_loss: 0.2102 - val_accuracy: 0.9489\n",
      "Epoch 53/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.3181 - accuracy: 0.9013 - val_loss: 0.2081 - val_accuracy: 0.9489\n",
      "Epoch 54/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.3299 - accuracy: 0.8909 - val_loss: 0.2078 - val_accuracy: 0.9489\n",
      "Epoch 55/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.3185 - accuracy: 0.8931 - val_loss: 0.2049 - val_accuracy: 0.9489\n",
      "Epoch 56/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2861 - accuracy: 0.9117 - val_loss: 0.2005 - val_accuracy: 0.9511\n",
      "Epoch 57/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.3029 - accuracy: 0.9057 - val_loss: 0.1987 - val_accuracy: 0.9511\n",
      "Epoch 58/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3023 - accuracy: 0.9050 - val_loss: 0.1959 - val_accuracy: 0.9489\n",
      "Epoch 59/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2872 - accuracy: 0.9124 - val_loss: 0.1958 - val_accuracy: 0.9511\n",
      "Epoch 60/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3028 - accuracy: 0.9020 - val_loss: 0.1946 - val_accuracy: 0.9489\n",
      "Epoch 61/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.3028 - accuracy: 0.9020 - val_loss: 0.1932 - val_accuracy: 0.9511\n",
      "Epoch 62/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.3004 - accuracy: 0.9057 - val_loss: 0.1929 - val_accuracy: 0.9533\n",
      "Epoch 63/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2970 - accuracy: 0.8990 - val_loss: 0.1925 - val_accuracy: 0.9533\n",
      "Epoch 64/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.3019 - accuracy: 0.9035 - val_loss: 0.1884 - val_accuracy: 0.9533\n",
      "Epoch 65/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2669 - accuracy: 0.9131 - val_loss: 0.1885 - val_accuracy: 0.9511\n",
      "Epoch 66/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2751 - accuracy: 0.9213 - val_loss: 0.1833 - val_accuracy: 0.9556\n",
      "Epoch 67/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2785 - accuracy: 0.9169 - val_loss: 0.1819 - val_accuracy: 0.9556\n",
      "Epoch 68/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2591 - accuracy: 0.9191 - val_loss: 0.1810 - val_accuracy: 0.9511\n",
      "Epoch 69/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2926 - accuracy: 0.9020 - val_loss: 0.1798 - val_accuracy: 0.9556\n",
      "Epoch 70/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2784 - accuracy: 0.9102 - val_loss: 0.1805 - val_accuracy: 0.9556\n",
      "Epoch 71/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2650 - accuracy: 0.9169 - val_loss: 0.1783 - val_accuracy: 0.9533\n",
      "Epoch 72/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2610 - accuracy: 0.9109 - val_loss: 0.1760 - val_accuracy: 0.9556\n",
      "Epoch 73/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2758 - accuracy: 0.9109 - val_loss: 0.1766 - val_accuracy: 0.9511\n",
      "Epoch 74/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2456 - accuracy: 0.9206 - val_loss: 0.1725 - val_accuracy: 0.9533\n",
      "Epoch 75/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2584 - accuracy: 0.9206 - val_loss: 0.1740 - val_accuracy: 0.9511\n",
      "Epoch 76/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2555 - accuracy: 0.9139 - val_loss: 0.1749 - val_accuracy: 0.9533\n",
      "Epoch 77/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2582 - accuracy: 0.9161 - val_loss: 0.1711 - val_accuracy: 0.9556\n",
      "Epoch 78/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2453 - accuracy: 0.9213 - val_loss: 0.1708 - val_accuracy: 0.9578\n",
      "Epoch 79/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2440 - accuracy: 0.9243 - val_loss: 0.1665 - val_accuracy: 0.9600\n",
      "Epoch 80/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.2397 - accuracy: 0.9280 - val_loss: 0.1674 - val_accuracy: 0.9578\n",
      "Epoch 81/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2337 - accuracy: 0.9324 - val_loss: 0.1663 - val_accuracy: 0.9600\n",
      "Epoch 82/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2428 - accuracy: 0.9198 - val_loss: 0.1661 - val_accuracy: 0.9578\n",
      "Epoch 83/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2494 - accuracy: 0.9154 - val_loss: 0.1663 - val_accuracy: 0.9600\n",
      "Epoch 84/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2312 - accuracy: 0.9228 - val_loss: 0.1651 - val_accuracy: 0.9600\n",
      "Epoch 85/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2432 - accuracy: 0.9213 - val_loss: 0.1641 - val_accuracy: 0.9556\n",
      "Epoch 86/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2337 - accuracy: 0.9206 - val_loss: 0.1622 - val_accuracy: 0.9556\n",
      "Epoch 87/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2178 - accuracy: 0.9339 - val_loss: 0.1623 - val_accuracy: 0.9556\n",
      "Epoch 88/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2065 - accuracy: 0.9436 - val_loss: 0.1598 - val_accuracy: 0.9556\n",
      "Epoch 89/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2321 - accuracy: 0.9176 - val_loss: 0.1600 - val_accuracy: 0.9578\n",
      "Epoch 90/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.2208 - accuracy: 0.9272 - val_loss: 0.1582 - val_accuracy: 0.9556\n",
      "Epoch 91/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2268 - accuracy: 0.9213 - val_loss: 0.1575 - val_accuracy: 0.9556\n",
      "Epoch 92/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2161 - accuracy: 0.9339 - val_loss: 0.1588 - val_accuracy: 0.9556\n",
      "Epoch 93/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2375 - accuracy: 0.9191 - val_loss: 0.1603 - val_accuracy: 0.9578\n",
      "Epoch 94/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2115 - accuracy: 0.9295 - val_loss: 0.1573 - val_accuracy: 0.9556\n",
      "Epoch 95/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2302 - accuracy: 0.9235 - val_loss: 0.1569 - val_accuracy: 0.9622\n",
      "Epoch 96/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2181 - accuracy: 0.9287 - val_loss: 0.1598 - val_accuracy: 0.9578\n",
      "Epoch 97/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2163 - accuracy: 0.9362 - val_loss: 0.1564 - val_accuracy: 0.9600\n",
      "Epoch 98/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2351 - accuracy: 0.9198 - val_loss: 0.1536 - val_accuracy: 0.9556\n",
      "Epoch 99/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2048 - accuracy: 0.9391 - val_loss: 0.1547 - val_accuracy: 0.9622\n",
      "Epoch 100/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2151 - accuracy: 0.9295 - val_loss: 0.1540 - val_accuracy: 0.9578\n",
      "Epoch 101/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.2226 - accuracy: 0.9258 - val_loss: 0.1528 - val_accuracy: 0.9578\n",
      "Epoch 102/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1959 - accuracy: 0.9339 - val_loss: 0.1524 - val_accuracy: 0.9578\n",
      "Epoch 103/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2190 - accuracy: 0.9287 - val_loss: 0.1525 - val_accuracy: 0.9622\n",
      "Epoch 104/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1967 - accuracy: 0.9399 - val_loss: 0.1516 - val_accuracy: 0.9578\n",
      "Epoch 105/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.2080 - accuracy: 0.9258 - val_loss: 0.1531 - val_accuracy: 0.9600\n",
      "Epoch 106/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2090 - accuracy: 0.9317 - val_loss: 0.1545 - val_accuracy: 0.9578\n",
      "Epoch 107/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1955 - accuracy: 0.9428 - val_loss: 0.1561 - val_accuracy: 0.9556\n",
      "Epoch 108/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2092 - accuracy: 0.9265 - val_loss: 0.1534 - val_accuracy: 0.9578\n",
      "Epoch 109/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2075 - accuracy: 0.9317 - val_loss: 0.1521 - val_accuracy: 0.9578\n",
      "Epoch 110/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.1910 - accuracy: 0.9421 - val_loss: 0.1522 - val_accuracy: 0.9600\n",
      "Epoch 111/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1926 - accuracy: 0.9391 - val_loss: 0.1528 - val_accuracy: 0.9556\n",
      "Epoch 112/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.2040 - accuracy: 0.9347 - val_loss: 0.1532 - val_accuracy: 0.9533\n",
      "Epoch 113/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.2094 - accuracy: 0.9362 - val_loss: 0.1536 - val_accuracy: 0.9533\n",
      "Epoch 114/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1998 - accuracy: 0.9347 - val_loss: 0.1513 - val_accuracy: 0.9556\n",
      "Epoch 115/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.2002 - accuracy: 0.9391 - val_loss: 0.1513 - val_accuracy: 0.9578\n",
      "Epoch 116/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1918 - accuracy: 0.9347 - val_loss: 0.1524 - val_accuracy: 0.9556\n",
      "Epoch 117/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.2015 - accuracy: 0.9347 - val_loss: 0.1520 - val_accuracy: 0.9578\n",
      "Epoch 118/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1979 - accuracy: 0.9332 - val_loss: 0.1549 - val_accuracy: 0.9556\n",
      "Epoch 119/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1786 - accuracy: 0.9421 - val_loss: 0.1526 - val_accuracy: 0.9600\n",
      "Epoch 120/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1964 - accuracy: 0.9421 - val_loss: 0.1508 - val_accuracy: 0.9600\n",
      "Epoch 121/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1721 - accuracy: 0.9495 - val_loss: 0.1499 - val_accuracy: 0.9600\n",
      "Epoch 122/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1822 - accuracy: 0.9451 - val_loss: 0.1501 - val_accuracy: 0.9556\n",
      "Epoch 123/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1807 - accuracy: 0.9421 - val_loss: 0.1498 - val_accuracy: 0.9578\n",
      "Epoch 124/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1979 - accuracy: 0.9339 - val_loss: 0.1537 - val_accuracy: 0.9489\n",
      "Epoch 125/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1906 - accuracy: 0.9332 - val_loss: 0.1546 - val_accuracy: 0.9511\n",
      "Epoch 126/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1849 - accuracy: 0.9347 - val_loss: 0.1581 - val_accuracy: 0.9533\n",
      "Epoch 127/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1715 - accuracy: 0.9458 - val_loss: 0.1503 - val_accuracy: 0.9578\n",
      "Epoch 128/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1859 - accuracy: 0.9354 - val_loss: 0.1506 - val_accuracy: 0.9622\n",
      "Epoch 129/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1695 - accuracy: 0.9480 - val_loss: 0.1500 - val_accuracy: 0.9600\n",
      "Epoch 130/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1934 - accuracy: 0.9324 - val_loss: 0.1508 - val_accuracy: 0.9556\n",
      "Epoch 131/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1761 - accuracy: 0.9458 - val_loss: 0.1540 - val_accuracy: 0.9533\n",
      "Epoch 132/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1867 - accuracy: 0.9354 - val_loss: 0.1508 - val_accuracy: 0.9578\n",
      "Epoch 133/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1938 - accuracy: 0.9369 - val_loss: 0.1536 - val_accuracy: 0.9533\n",
      "Epoch 134/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1674 - accuracy: 0.9517 - val_loss: 0.1523 - val_accuracy: 0.9511\n",
      "Epoch 135/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1866 - accuracy: 0.9399 - val_loss: 0.1514 - val_accuracy: 0.9511\n",
      "Epoch 136/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1819 - accuracy: 0.9421 - val_loss: 0.1508 - val_accuracy: 0.9511\n",
      "Epoch 137/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1712 - accuracy: 0.9458 - val_loss: 0.1501 - val_accuracy: 0.9511\n",
      "Epoch 138/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1831 - accuracy: 0.9399 - val_loss: 0.1473 - val_accuracy: 0.9600\n",
      "Epoch 139/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1623 - accuracy: 0.9495 - val_loss: 0.1506 - val_accuracy: 0.9511\n",
      "Epoch 140/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1693 - accuracy: 0.9414 - val_loss: 0.1498 - val_accuracy: 0.9533\n",
      "Epoch 141/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1530 - accuracy: 0.9555 - val_loss: 0.1479 - val_accuracy: 0.9556\n",
      "Epoch 142/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1802 - accuracy: 0.9362 - val_loss: 0.1507 - val_accuracy: 0.9533\n",
      "Epoch 143/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1651 - accuracy: 0.9525 - val_loss: 0.1471 - val_accuracy: 0.9600\n",
      "Epoch 144/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1523 - accuracy: 0.9569 - val_loss: 0.1486 - val_accuracy: 0.9556\n",
      "Epoch 145/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.1819 - accuracy: 0.9421 - val_loss: 0.1471 - val_accuracy: 0.9600\n",
      "Epoch 146/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1759 - accuracy: 0.9295 - val_loss: 0.1487 - val_accuracy: 0.9600\n",
      "Epoch 147/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1578 - accuracy: 0.9480 - val_loss: 0.1499 - val_accuracy: 0.9600\n",
      "Epoch 148/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1707 - accuracy: 0.9525 - val_loss: 0.1488 - val_accuracy: 0.9578\n",
      "Epoch 149/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1574 - accuracy: 0.9495 - val_loss: 0.1486 - val_accuracy: 0.9578\n",
      "Epoch 150/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1701 - accuracy: 0.9465 - val_loss: 0.1506 - val_accuracy: 0.9533\n",
      "Epoch 151/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1837 - accuracy: 0.9369 - val_loss: 0.1504 - val_accuracy: 0.9556\n",
      "Epoch 152/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1589 - accuracy: 0.9517 - val_loss: 0.1491 - val_accuracy: 0.9600\n",
      "Epoch 153/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1600 - accuracy: 0.9414 - val_loss: 0.1525 - val_accuracy: 0.9578\n",
      "Epoch 154/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1563 - accuracy: 0.9488 - val_loss: 0.1519 - val_accuracy: 0.9578\n",
      "Epoch 155/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1666 - accuracy: 0.9465 - val_loss: 0.1512 - val_accuracy: 0.9578\n",
      "Epoch 156/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1641 - accuracy: 0.9488 - val_loss: 0.1500 - val_accuracy: 0.9600\n",
      "Epoch 157/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1806 - accuracy: 0.9406 - val_loss: 0.1502 - val_accuracy: 0.9556\n",
      "Epoch 158/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1587 - accuracy: 0.9525 - val_loss: 0.1483 - val_accuracy: 0.9622\n",
      "Epoch 159/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1730 - accuracy: 0.9421 - val_loss: 0.1519 - val_accuracy: 0.9556\n",
      "Epoch 160/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1624 - accuracy: 0.9488 - val_loss: 0.1513 - val_accuracy: 0.9578\n",
      "Epoch 161/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1450 - accuracy: 0.9540 - val_loss: 0.1515 - val_accuracy: 0.9556\n",
      "Epoch 162/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1764 - accuracy: 0.9421 - val_loss: 0.1509 - val_accuracy: 0.9578\n",
      "Epoch 163/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1555 - accuracy: 0.9547 - val_loss: 0.1508 - val_accuracy: 0.9556\n",
      "Epoch 164/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1566 - accuracy: 0.9517 - val_loss: 0.1514 - val_accuracy: 0.9556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1540 - accuracy: 0.9517 - val_loss: 0.1494 - val_accuracy: 0.9556\n",
      "Epoch 166/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1572 - accuracy: 0.9532 - val_loss: 0.1503 - val_accuracy: 0.9578\n",
      "Epoch 167/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1680 - accuracy: 0.9495 - val_loss: 0.1504 - val_accuracy: 0.9578\n",
      "Epoch 168/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1448 - accuracy: 0.9525 - val_loss: 0.1506 - val_accuracy: 0.9578\n",
      "Epoch 169/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1509 - accuracy: 0.9577 - val_loss: 0.1510 - val_accuracy: 0.9600\n",
      "Epoch 170/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1576 - accuracy: 0.9540 - val_loss: 0.1516 - val_accuracy: 0.9556\n",
      "Epoch 171/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1592 - accuracy: 0.9495 - val_loss: 0.1515 - val_accuracy: 0.9556\n",
      "Epoch 172/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1599 - accuracy: 0.9414 - val_loss: 0.1513 - val_accuracy: 0.9600\n",
      "Epoch 173/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1552 - accuracy: 0.9488 - val_loss: 0.1523 - val_accuracy: 0.9578\n",
      "Epoch 174/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1403 - accuracy: 0.9540 - val_loss: 0.1534 - val_accuracy: 0.9578\n",
      "Epoch 175/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1500 - accuracy: 0.9510 - val_loss: 0.1513 - val_accuracy: 0.9578\n",
      "Epoch 176/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1592 - accuracy: 0.9443 - val_loss: 0.1509 - val_accuracy: 0.9578\n",
      "Epoch 177/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1619 - accuracy: 0.9465 - val_loss: 0.1515 - val_accuracy: 0.9578\n",
      "Epoch 178/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1534 - accuracy: 0.9436 - val_loss: 0.1515 - val_accuracy: 0.9556\n",
      "Epoch 179/300\n",
      "1347/1347 [==============================] - 0s 48us/sample - loss: 0.1543 - accuracy: 0.9532 - val_loss: 0.1525 - val_accuracy: 0.9578\n",
      "Epoch 180/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1407 - accuracy: 0.9547 - val_loss: 0.1509 - val_accuracy: 0.9600\n",
      "Epoch 181/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1476 - accuracy: 0.9510 - val_loss: 0.1503 - val_accuracy: 0.9600\n",
      "Epoch 182/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1440 - accuracy: 0.9532 - val_loss: 0.1501 - val_accuracy: 0.9533\n",
      "Epoch 183/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1392 - accuracy: 0.9503 - val_loss: 0.1514 - val_accuracy: 0.9600\n",
      "Epoch 184/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1361 - accuracy: 0.9621 - val_loss: 0.1501 - val_accuracy: 0.9578\n",
      "Epoch 185/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1424 - accuracy: 0.9517 - val_loss: 0.1507 - val_accuracy: 0.9578\n",
      "Epoch 186/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1488 - accuracy: 0.9443 - val_loss: 0.1503 - val_accuracy: 0.9578\n",
      "Epoch 187/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1577 - accuracy: 0.9451 - val_loss: 0.1523 - val_accuracy: 0.9622\n",
      "Epoch 188/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1612 - accuracy: 0.9384 - val_loss: 0.1524 - val_accuracy: 0.9600\n",
      "Epoch 189/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1295 - accuracy: 0.9614 - val_loss: 0.1538 - val_accuracy: 0.9578\n",
      "Epoch 190/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1283 - accuracy: 0.9607 - val_loss: 0.1525 - val_accuracy: 0.9578\n",
      "Epoch 191/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1398 - accuracy: 0.9510 - val_loss: 0.1523 - val_accuracy: 0.9578\n",
      "Epoch 192/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1522 - accuracy: 0.9517 - val_loss: 0.1521 - val_accuracy: 0.9644\n",
      "Epoch 193/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1451 - accuracy: 0.9480 - val_loss: 0.1529 - val_accuracy: 0.9600\n",
      "Epoch 194/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1523 - accuracy: 0.9503 - val_loss: 0.1526 - val_accuracy: 0.9600\n",
      "Epoch 195/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1512 - accuracy: 0.9473 - val_loss: 0.1515 - val_accuracy: 0.9578\n",
      "Epoch 196/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1470 - accuracy: 0.9480 - val_loss: 0.1529 - val_accuracy: 0.9622\n",
      "Epoch 197/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1205 - accuracy: 0.9681 - val_loss: 0.1514 - val_accuracy: 0.9622\n",
      "Epoch 198/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1541 - accuracy: 0.9495 - val_loss: 0.1523 - val_accuracy: 0.9578\n",
      "Epoch 199/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1498 - accuracy: 0.9458 - val_loss: 0.1537 - val_accuracy: 0.9556\n",
      "Epoch 200/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1584 - accuracy: 0.9451 - val_loss: 0.1530 - val_accuracy: 0.9578\n",
      "Epoch 201/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1474 - accuracy: 0.9495 - val_loss: 0.1526 - val_accuracy: 0.9578\n",
      "Epoch 202/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1463 - accuracy: 0.9510 - val_loss: 0.1531 - val_accuracy: 0.9622\n",
      "Epoch 203/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1321 - accuracy: 0.9569 - val_loss: 0.1532 - val_accuracy: 0.9600\n",
      "Epoch 204/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1265 - accuracy: 0.9584 - val_loss: 0.1514 - val_accuracy: 0.9556\n",
      "Epoch 205/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1398 - accuracy: 0.9510 - val_loss: 0.1508 - val_accuracy: 0.9556\n",
      "Epoch 206/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1377 - accuracy: 0.9532 - val_loss: 0.1516 - val_accuracy: 0.9600\n",
      "Epoch 207/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1595 - accuracy: 0.9503 - val_loss: 0.1525 - val_accuracy: 0.9533\n",
      "Epoch 208/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1233 - accuracy: 0.9651 - val_loss: 0.1551 - val_accuracy: 0.9622\n",
      "Epoch 209/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1263 - accuracy: 0.9569 - val_loss: 0.1519 - val_accuracy: 0.9600\n",
      "Epoch 210/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1280 - accuracy: 0.9540 - val_loss: 0.1531 - val_accuracy: 0.9556\n",
      "Epoch 211/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1360 - accuracy: 0.9562 - val_loss: 0.1518 - val_accuracy: 0.9578\n",
      "Epoch 212/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1426 - accuracy: 0.9547 - val_loss: 0.1540 - val_accuracy: 0.9578\n",
      "Epoch 213/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1353 - accuracy: 0.9569 - val_loss: 0.1543 - val_accuracy: 0.9533\n",
      "Epoch 214/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1448 - accuracy: 0.9503 - val_loss: 0.1554 - val_accuracy: 0.9533\n",
      "Epoch 215/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1335 - accuracy: 0.9555 - val_loss: 0.1567 - val_accuracy: 0.9489\n",
      "Epoch 216/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1464 - accuracy: 0.9503 - val_loss: 0.1531 - val_accuracy: 0.9600\n",
      "Epoch 217/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1522 - accuracy: 0.9510 - val_loss: 0.1550 - val_accuracy: 0.9578\n",
      "Epoch 218/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1391 - accuracy: 0.9525 - val_loss: 0.1544 - val_accuracy: 0.9622\n",
      "Epoch 219/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1329 - accuracy: 0.9614 - val_loss: 0.1575 - val_accuracy: 0.9556\n",
      "Epoch 220/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1365 - accuracy: 0.9540 - val_loss: 0.1542 - val_accuracy: 0.9556\n",
      "Epoch 221/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1495 - accuracy: 0.9517 - val_loss: 0.1536 - val_accuracy: 0.9600\n",
      "Epoch 222/300\n",
      "1347/1347 [==============================] - 0s 52us/sample - loss: 0.1353 - accuracy: 0.9525 - val_loss: 0.1536 - val_accuracy: 0.9600\n",
      "Epoch 223/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1183 - accuracy: 0.9607 - val_loss: 0.1548 - val_accuracy: 0.9556\n",
      "Epoch 224/300\n",
      "1347/1347 [==============================] - 0s 51us/sample - loss: 0.1518 - accuracy: 0.9525 - val_loss: 0.1576 - val_accuracy: 0.9622\n",
      "Epoch 225/300\n",
      "1347/1347 [==============================] - 0s 56us/sample - loss: 0.1252 - accuracy: 0.9599 - val_loss: 0.1574 - val_accuracy: 0.9578\n",
      "Epoch 226/300\n",
      "1347/1347 [==============================] - 0s 57us/sample - loss: 0.1372 - accuracy: 0.9569 - val_loss: 0.1597 - val_accuracy: 0.9578\n",
      "Epoch 227/300\n",
      "1347/1347 [==============================] - 0s 49us/sample - loss: 0.1250 - accuracy: 0.9614 - val_loss: 0.1600 - val_accuracy: 0.9533\n",
      "Epoch 228/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1164 - accuracy: 0.9621 - val_loss: 0.1581 - val_accuracy: 0.9556\n",
      "Epoch 229/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1347 - accuracy: 0.9555 - val_loss: 0.1583 - val_accuracy: 0.9533\n",
      "Epoch 230/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1345 - accuracy: 0.9562 - val_loss: 0.1559 - val_accuracy: 0.9578\n",
      "Epoch 231/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1394 - accuracy: 0.9510 - val_loss: 0.1606 - val_accuracy: 0.9489\n",
      "Epoch 232/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1190 - accuracy: 0.9651 - val_loss: 0.1607 - val_accuracy: 0.9578\n",
      "Epoch 233/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1142 - accuracy: 0.9644 - val_loss: 0.1604 - val_accuracy: 0.9578\n",
      "Epoch 234/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1351 - accuracy: 0.9599 - val_loss: 0.1605 - val_accuracy: 0.9578\n",
      "Epoch 235/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1363 - accuracy: 0.9510 - val_loss: 0.1594 - val_accuracy: 0.9600\n",
      "Epoch 236/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1290 - accuracy: 0.9607 - val_loss: 0.1618 - val_accuracy: 0.9578\n",
      "Epoch 237/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1220 - accuracy: 0.9621 - val_loss: 0.1603 - val_accuracy: 0.9556\n",
      "Epoch 238/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1294 - accuracy: 0.9569 - val_loss: 0.1611 - val_accuracy: 0.9578\n",
      "Epoch 239/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1216 - accuracy: 0.9540 - val_loss: 0.1621 - val_accuracy: 0.9578\n",
      "Epoch 240/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1191 - accuracy: 0.9577 - val_loss: 0.1622 - val_accuracy: 0.9556\n",
      "Epoch 241/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1376 - accuracy: 0.9577 - val_loss: 0.1670 - val_accuracy: 0.9578\n",
      "Epoch 242/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1251 - accuracy: 0.9629 - val_loss: 0.1606 - val_accuracy: 0.9578\n",
      "Epoch 243/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1205 - accuracy: 0.9592 - val_loss: 0.1616 - val_accuracy: 0.9578\n",
      "Epoch 244/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1275 - accuracy: 0.9599 - val_loss: 0.1622 - val_accuracy: 0.9578\n",
      "Epoch 245/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1214 - accuracy: 0.9577 - val_loss: 0.1612 - val_accuracy: 0.9600\n",
      "Epoch 246/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1349 - accuracy: 0.9540 - val_loss: 0.1628 - val_accuracy: 0.9622\n",
      "Epoch 247/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1295 - accuracy: 0.9584 - val_loss: 0.1638 - val_accuracy: 0.9600\n",
      "Epoch 248/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1504 - accuracy: 0.9465 - val_loss: 0.1619 - val_accuracy: 0.9556\n",
      "Epoch 249/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1325 - accuracy: 0.9532 - val_loss: 0.1628 - val_accuracy: 0.9578\n",
      "Epoch 250/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.1174 - accuracy: 0.9636 - val_loss: 0.1610 - val_accuracy: 0.9622\n",
      "Epoch 251/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1189 - accuracy: 0.9577 - val_loss: 0.1608 - val_accuracy: 0.9622\n",
      "Epoch 252/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1205 - accuracy: 0.9562 - val_loss: 0.1621 - val_accuracy: 0.9556\n",
      "Epoch 253/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1291 - accuracy: 0.9562 - val_loss: 0.1620 - val_accuracy: 0.9578\n",
      "Epoch 254/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1225 - accuracy: 0.9614 - val_loss: 0.1650 - val_accuracy: 0.9556\n",
      "Epoch 255/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1191 - accuracy: 0.9629 - val_loss: 0.1640 - val_accuracy: 0.9578\n",
      "Epoch 256/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.1322 - accuracy: 0.9629 - val_loss: 0.1629 - val_accuracy: 0.9578\n",
      "Epoch 257/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1263 - accuracy: 0.9607 - val_loss: 0.1642 - val_accuracy: 0.9556\n",
      "Epoch 258/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1156 - accuracy: 0.9636 - val_loss: 0.1643 - val_accuracy: 0.9622\n",
      "Epoch 259/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1175 - accuracy: 0.9614 - val_loss: 0.1637 - val_accuracy: 0.9600\n",
      "Epoch 260/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1238 - accuracy: 0.9592 - val_loss: 0.1621 - val_accuracy: 0.9533\n",
      "Epoch 261/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1351 - accuracy: 0.9547 - val_loss: 0.1648 - val_accuracy: 0.9578\n",
      "Epoch 262/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1182 - accuracy: 0.9621 - val_loss: 0.1622 - val_accuracy: 0.9622\n",
      "Epoch 263/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1247 - accuracy: 0.9577 - val_loss: 0.1632 - val_accuracy: 0.9622\n",
      "Epoch 264/300\n",
      "1347/1347 [==============================] - 0s 41us/sample - loss: 0.1260 - accuracy: 0.9569 - val_loss: 0.1635 - val_accuracy: 0.9622\n",
      "Epoch 265/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1347 - accuracy: 0.9525 - val_loss: 0.1642 - val_accuracy: 0.9556\n",
      "Epoch 266/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1046 - accuracy: 0.9666 - val_loss: 0.1646 - val_accuracy: 0.9600\n",
      "Epoch 267/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1110 - accuracy: 0.9607 - val_loss: 0.1633 - val_accuracy: 0.9622\n",
      "Epoch 268/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1140 - accuracy: 0.9644 - val_loss: 0.1636 - val_accuracy: 0.9533\n",
      "Epoch 269/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1273 - accuracy: 0.9562 - val_loss: 0.1659 - val_accuracy: 0.9556\n",
      "Epoch 270/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1280 - accuracy: 0.9599 - val_loss: 0.1641 - val_accuracy: 0.9622\n",
      "Epoch 271/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1084 - accuracy: 0.9607 - val_loss: 0.1639 - val_accuracy: 0.9556\n",
      "Epoch 272/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1235 - accuracy: 0.9540 - val_loss: 0.1646 - val_accuracy: 0.9622\n",
      "Epoch 273/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1119 - accuracy: 0.9673 - val_loss: 0.1634 - val_accuracy: 0.9533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1226 - accuracy: 0.9577 - val_loss: 0.1620 - val_accuracy: 0.9578\n",
      "Epoch 275/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1337 - accuracy: 0.9503 - val_loss: 0.1667 - val_accuracy: 0.9578\n",
      "Epoch 276/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1186 - accuracy: 0.9621 - val_loss: 0.1678 - val_accuracy: 0.9511\n",
      "Epoch 277/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1207 - accuracy: 0.9607 - val_loss: 0.1641 - val_accuracy: 0.9578\n",
      "Epoch 278/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1232 - accuracy: 0.9547 - val_loss: 0.1650 - val_accuracy: 0.9556\n",
      "Epoch 279/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1291 - accuracy: 0.9562 - val_loss: 0.1663 - val_accuracy: 0.9622\n",
      "Epoch 280/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1293 - accuracy: 0.9517 - val_loss: 0.1652 - val_accuracy: 0.9622\n",
      "Epoch 281/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1198 - accuracy: 0.9651 - val_loss: 0.1650 - val_accuracy: 0.9644\n",
      "Epoch 282/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1259 - accuracy: 0.9577 - val_loss: 0.1690 - val_accuracy: 0.9578\n",
      "Epoch 283/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1042 - accuracy: 0.9703 - val_loss: 0.1656 - val_accuracy: 0.9600\n",
      "Epoch 284/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1080 - accuracy: 0.9644 - val_loss: 0.1685 - val_accuracy: 0.9622\n",
      "Epoch 285/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1216 - accuracy: 0.9577 - val_loss: 0.1681 - val_accuracy: 0.9600\n",
      "Epoch 286/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1177 - accuracy: 0.9607 - val_loss: 0.1668 - val_accuracy: 0.9600\n",
      "Epoch 287/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1036 - accuracy: 0.9666 - val_loss: 0.1669 - val_accuracy: 0.9556\n",
      "Epoch 288/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1227 - accuracy: 0.9547 - val_loss: 0.1664 - val_accuracy: 0.9600\n",
      "Epoch 289/300\n",
      "1347/1347 [==============================] - 0s 47us/sample - loss: 0.1115 - accuracy: 0.9584 - val_loss: 0.1686 - val_accuracy: 0.9600\n",
      "Epoch 290/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1196 - accuracy: 0.9599 - val_loss: 0.1698 - val_accuracy: 0.9556\n",
      "Epoch 291/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1063 - accuracy: 0.9673 - val_loss: 0.1694 - val_accuracy: 0.9578\n",
      "Epoch 292/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1082 - accuracy: 0.9651 - val_loss: 0.1669 - val_accuracy: 0.9556\n",
      "Epoch 293/300\n",
      "1347/1347 [==============================] - 0s 42us/sample - loss: 0.1155 - accuracy: 0.9569 - val_loss: 0.1688 - val_accuracy: 0.9600\n",
      "Epoch 294/300\n",
      "1347/1347 [==============================] - 0s 50us/sample - loss: 0.1181 - accuracy: 0.9547 - val_loss: 0.1651 - val_accuracy: 0.9622\n",
      "Epoch 295/300\n",
      "1347/1347 [==============================] - 0s 45us/sample - loss: 0.1321 - accuracy: 0.9517 - val_loss: 0.1664 - val_accuracy: 0.9556\n",
      "Epoch 296/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1171 - accuracy: 0.9644 - val_loss: 0.1660 - val_accuracy: 0.9622\n",
      "Epoch 297/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1167 - accuracy: 0.9629 - val_loss: 0.1680 - val_accuracy: 0.9600\n",
      "Epoch 298/300\n",
      "1347/1347 [==============================] - 0s 46us/sample - loss: 0.1222 - accuracy: 0.9614 - val_loss: 0.1693 - val_accuracy: 0.9578\n",
      "Epoch 299/300\n",
      "1347/1347 [==============================] - 0s 44us/sample - loss: 0.1132 - accuracy: 0.9644 - val_loss: 0.1711 - val_accuracy: 0.9578\n",
      "Epoch 300/300\n",
      "1347/1347 [==============================] - 0s 43us/sample - loss: 0.1288 - accuracy: 0.9540 - val_loss: 0.1714 - val_accuracy: 0.9578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a38bd2250>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "\n",
    "inputs = Input(shape=(n_features,))\n",
    "h_layer = Dense(n_hidden, activation=activation)(inputs)\n",
    "h_layer_dropout = Dropout(0.1)(h_layer)\n",
    "y_layer = Dense(n_classes, activation='softmax')(h_layer_dropout)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=y_layer)\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(\n",
    "    X_tr, to_categorical(Y_tr, num_classes=n_classes), \n",
    "    epochs=300, verbose=1, batch_size=32,\n",
    "    validation_data=(X_val, to_categorical(Y_val, num_classes=n_classes))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
